{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90bf045f",
   "metadata": {},
   "source": [
    "# NLTK Complete Guide - Section 1: Introduction & Setup\n",
    "\n",
    "This notebook covers:\n",
    "- What is NLTK?\n",
    "- Installation\n",
    "- Downloading NLTK Data\n",
    "- Verifying Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed66cb5",
   "metadata": {},
   "source": [
    "## 1.1 What is NLTK?\n",
    "\n",
    "**NLTK (Natural Language Toolkit)** is a leading platform for building Python programs to work with human language data.\n",
    "\n",
    "It provides:\n",
    "- Easy-to-use interfaces to **50+ corpora** and lexical resources\n",
    "- Text processing libraries for **classification, tokenization, stemming, tagging, parsing**\n",
    "- Wrappers for industrial-strength NLP libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b43ddaf",
   "metadata": {},
   "source": [
    "## 1.2 Installation\n",
    "\n",
    "Run these commands in your terminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11e0150b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install NLTK (uncomment to run)\n",
    "# !pip install nltk\n",
    "\n",
    "# Install with all dependencies\n",
    "# !pip install nltk numpy scipy matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9d55fe",
   "metadata": {},
   "source": [
    "## 1.3 Downloading NLTK Data\n",
    "\n",
    "NLTK requires additional data packages for various functionalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1c4caaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Version: 3.9.2\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Check NLTK version\n",
    "print(f\"NLTK Version: {nltk.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c563c440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading punkt...\n",
      "Downloading stopwords...\n",
      "Downloading wordnet...\n",
      "Downloading averaged_perceptron_tagger...\n",
      "Downloading maxent_ne_chunker...\n",
      "Downloading words...\n",
      "Downloading vader_lexicon...\n",
      "Downloading omw-1.4...\n",
      "\n",
      "✅ All essential packages downloaded!\n"
     ]
    }
   ],
   "source": [
    "# Download essential packages (run this once)\n",
    "essential_packages = [\n",
    "    'punkt',                        # Tokenizers\n",
    "    'stopwords',                    # Stop words\n",
    "    'wordnet',                      # WordNet lexical database\n",
    "    'averaged_perceptron_tagger',   # POS tagger\n",
    "    'maxent_ne_chunker',            # Named Entity chunker\n",
    "    'words',                        # Word list\n",
    "    'vader_lexicon',                # Sentiment analysis\n",
    "    'omw-1.4',                      # Open Multilingual WordNet\n",
    "]\n",
    "\n",
    "for package in essential_packages:\n",
    "    print(f\"Downloading {package}...\")\n",
    "    nltk.download(package, quiet=True)\n",
    "\n",
    "print(\"\\n✅ All essential packages downloaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "864f025a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Download ALL data (takes longer, ~3GB)\n",
    "# nltk.download('all')\n",
    "\n",
    "# Alternative: Open interactive GUI downloader\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108b7d22",
   "metadata": {},
   "source": [
    "## 1.4 Verifying Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "648e43a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Hello, NLTK is working!\n",
      "Tokens: ['Hello', ',', 'NLTK', 'is', 'working', '!']\n",
      "\n",
      "✅ NLTK is working correctly!\n"
     ]
    }
   ],
   "source": [
    "# Test basic tokenization\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Hello, NLTK is working!\"\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(\"\\n✅ NLTK is working correctly!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "902800bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Data Packages Status:\n",
      "----------------------------------------\n",
      "✅ punkt\n",
      "✅ stopwords\n",
      "❌ wordnet - run: nltk.download('wordnet')\n",
      "✅ averaged_perceptron_tagger\n",
      "✅ maxent_ne_chunker\n",
      "❌ vader_lexicon - run: nltk.download('vader_lexicon')\n"
     ]
    }
   ],
   "source": [
    "# Check which data packages are available\n",
    "from nltk.data import find\n",
    "\n",
    "packages_to_check = [\n",
    "    ('tokenizers/punkt', 'punkt'),\n",
    "    ('corpora/stopwords', 'stopwords'),\n",
    "    ('corpora/wordnet', 'wordnet'),\n",
    "    ('taggers/averaged_perceptron_tagger', 'averaged_perceptron_tagger'),\n",
    "    ('chunkers/maxent_ne_chunker', 'maxent_ne_chunker'),\n",
    "    ('sentiment/vader_lexicon', 'vader_lexicon'),\n",
    "]\n",
    "\n",
    "print(\"NLTK Data Packages Status:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for path, name in packages_to_check:\n",
    "    try:\n",
    "        find(path)\n",
    "        print(f\"✅ {name}\")\n",
    "    except LookupError:\n",
    "        print(f\"❌ {name} - run: nltk.download('{name}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abf3984",
   "metadata": {},
   "source": [
    "## 1.5 Quick Test - All Core Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af74b57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences: ['NLTK is a powerful library for natural language processing.', 'It helps developers analyze text easily.']\n",
      "\n",
      "Tokens: ['NLTK', 'is', 'a', 'powerful', 'library', 'for', 'natural', 'language', 'processing', '.', 'It', 'helps', 'developers', 'analyze', 'text', 'easily', '.']\n",
      "\n",
      "Without stopwords: ['NLTK', 'powerful', 'library', 'natural', 'language', 'processing', 'helps', 'developers', 'analyze', 'text', 'easily']\n",
      "\n",
      "Stemmed: ['nltk', 'power', 'librari', 'natur', 'languag', 'process', 'help', 'develop', 'analyz', 'text', 'easili']\n",
      "\n",
      "Lemmatized: ['nltk', 'powerful', 'library', 'natural', 'language', 'processing', 'help', 'developer', 'analyze', 'text', 'easily']\n",
      "\n",
      "POS Tagged: [('NLTK', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('powerful', 'JJ'), ('library', 'NN'), ('for', 'IN'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('.', '.'), ('It', 'PRP'), ('helps', 'VBZ'), ('developers', 'NNS'), ('analyze', 'VBP'), ('text', 'JJ'), ('easily', 'RB'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Quick test of core NLTK features\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "text = \"NLTK is a powerful library for natural language processing. It helps developers analyze text easily.\"\n",
    "\n",
    "# Sentence tokenization\n",
    "sentences = sent_tokenize(text)\n",
    "print(f\"Sentences: {sentences}\\n\")\n",
    "\n",
    "# Word tokenization\n",
    "tokens = word_tokenize(text)\n",
    "print(f\"Tokens: {tokens}\\n\")\n",
    "\n",
    "# Stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered = [w for w in tokens if w.lower() not in stop_words and w.isalpha()]\n",
    "print(f\"Without stopwords: {filtered}\\n\")\n",
    "\n",
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed = [stemmer.stem(w) for w in filtered]\n",
    "print(f\"Stemmed: {stemmed}\\n\")\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized = [lemmatizer.lemmatize(w.lower()) for w in filtered]\n",
    "print(f\"Lemmatized: {lemmatized}\\n\")\n",
    "\n",
    "# POS Tagging\n",
    "tagged = pos_tag(tokens)\n",
    "print(f\"POS Tagged: {tagged}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887bf2f6",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Task | Code |\n",
    "|------|------|\n",
    "| Install NLTK | `pip install nltk` |\n",
    "| Download data | `nltk.download('package_name')` |\n",
    "| Download all | `nltk.download('all')` |\n",
    "| Check version | `nltk.__version__` |\n",
    "\n",
    "### Essential Packages\n",
    "- `punkt` - Tokenizers\n",
    "- `stopwords` - Stop words\n",
    "- `wordnet` - WordNet lexical database\n",
    "- `averaged_perceptron_tagger` - POS tagger\n",
    "- `maxent_ne_chunker` - Named Entity Recognition\n",
    "- `vader_lexicon` - Sentiment Analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
