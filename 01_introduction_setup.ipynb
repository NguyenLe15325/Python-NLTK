{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90bf045f",
   "metadata": {},
   "source": [
    "# NLTK Complete Guide - Section 1: Introduction & Setup\n",
    "\n",
    "This notebook covers:\n",
    "- What is NLTK?\n",
    "- Installation\n",
    "- Downloading NLTK Data\n",
    "- Verifying Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed66cb5",
   "metadata": {},
   "source": [
    "## 1.1 What is NLTK?\n",
    "\n",
    "**NLTK (Natural Language Toolkit)** is a leading platform for building Python programs to work with human language data.\n",
    "\n",
    "It provides:\n",
    "- Easy-to-use interfaces to **50+ corpora** and lexical resources\n",
    "- Text processing libraries for **classification, tokenization, stemming, tagging, parsing**\n",
    "- Wrappers for industrial-strength NLP libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b43ddaf",
   "metadata": {},
   "source": [
    "## 1.2 Installation\n",
    "\n",
    "Run these commands in your terminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e0150b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install NLTK (uncomment to run)\n",
    "# !pip install nltk\n",
    "\n",
    "# Install with all dependencies\n",
    "# !pip install nltk numpy scipy matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9d55fe",
   "metadata": {},
   "source": [
    "## 1.3 Downloading NLTK Data\n",
    "\n",
    "NLTK requires additional data packages for various functionalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c4caaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Check NLTK version\n",
    "print(f\"NLTK Version: {nltk.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c563c440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download essential packages (run this once)\n",
    "essential_packages = [\n",
    "    'punkt',                        # Tokenizers\n",
    "    'stopwords',                    # Stop words\n",
    "    'wordnet',                      # WordNet lexical database\n",
    "    'averaged_perceptron_tagger',   # POS tagger\n",
    "    'maxent_ne_chunker',            # Named Entity chunker\n",
    "    'words',                        # Word list\n",
    "    'vader_lexicon',                # Sentiment analysis\n",
    "    'omw-1.4',                      # Open Multilingual WordNet\n",
    "]\n",
    "\n",
    "for package in essential_packages:\n",
    "    print(f\"Downloading {package}...\")\n",
    "    nltk.download(package, quiet=True)\n",
    "\n",
    "print(\"\\n✅ All essential packages downloaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864f025a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Download ALL data (takes longer, ~3GB)\n",
    "# nltk.download('all')\n",
    "\n",
    "# Alternative: Open interactive GUI downloader\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108b7d22",
   "metadata": {},
   "source": [
    "## 1.4 Verifying Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648e43a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test basic tokenization\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Hello, NLTK is working!\"\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(\"\\n✅ NLTK is working correctly!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902800bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which data packages are available\n",
    "from nltk.data import find\n",
    "\n",
    "packages_to_check = [\n",
    "    ('tokenizers/punkt', 'punkt'),\n",
    "    ('corpora/stopwords', 'stopwords'),\n",
    "    ('corpora/wordnet', 'wordnet'),\n",
    "    ('taggers/averaged_perceptron_tagger', 'averaged_perceptron_tagger'),\n",
    "    ('chunkers/maxent_ne_chunker', 'maxent_ne_chunker'),\n",
    "    ('sentiment/vader_lexicon', 'vader_lexicon'),\n",
    "]\n",
    "\n",
    "print(\"NLTK Data Packages Status:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for path, name in packages_to_check:\n",
    "    try:\n",
    "        find(path)\n",
    "        print(f\"✅ {name}\")\n",
    "    except LookupError:\n",
    "        print(f\"❌ {name} - run: nltk.download('{name}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abf3984",
   "metadata": {},
   "source": [
    "## 1.5 Quick Test - All Core Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af74b57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test of core NLTK features\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "text = \"NLTK is a powerful library for natural language processing. It helps developers analyze text easily.\"\n",
    "\n",
    "# Sentence tokenization\n",
    "sentences = sent_tokenize(text)\n",
    "print(f\"Sentences: {sentences}\\n\")\n",
    "\n",
    "# Word tokenization\n",
    "tokens = word_tokenize(text)\n",
    "print(f\"Tokens: {tokens}\\n\")\n",
    "\n",
    "# Stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered = [w for w in tokens if w.lower() not in stop_words and w.isalpha()]\n",
    "print(f\"Without stopwords: {filtered}\\n\")\n",
    "\n",
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed = [stemmer.stem(w) for w in filtered]\n",
    "print(f\"Stemmed: {stemmed}\\n\")\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized = [lemmatizer.lemmatize(w.lower()) for w in filtered]\n",
    "print(f\"Lemmatized: {lemmatized}\\n\")\n",
    "\n",
    "# POS Tagging\n",
    "tagged = pos_tag(tokens)\n",
    "print(f\"POS Tagged: {tagged}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887bf2f6",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Task | Code |\n",
    "|------|------|\n",
    "| Install NLTK | `pip install nltk` |\n",
    "| Download data | `nltk.download('package_name')` |\n",
    "| Download all | `nltk.download('all')` |\n",
    "| Check version | `nltk.__version__` |\n",
    "\n",
    "### Essential Packages\n",
    "- `punkt` - Tokenizers\n",
    "- `stopwords` - Stop words\n",
    "- `wordnet` - WordNet lexical database\n",
    "- `averaged_perceptron_tagger` - POS tagger\n",
    "- `maxent_ne_chunker` - Named Entity Recognition\n",
    "- `vader_lexicon` - Sentiment Analysis"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
