{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a07f2fb",
   "metadata": {},
   "source": [
    "# NLTK Complete Guide - Section 3: Tokenization\n",
    "\n",
    "This notebook covers:\n",
    "- Word Tokenization\n",
    "- Sentence Tokenization\n",
    "- Regular Expression Tokenization\n",
    "- Tweet Tokenization\n",
    "- Multi-Word Expression Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72d4841",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "from nltk.tokenize import (\n",
    "    word_tokenize,\n",
    "    wordpunct_tokenize,\n",
    "    WhitespaceTokenizer,\n",
    "    sent_tokenize,\n",
    "    PunktSentenceTokenizer,\n",
    "    RegexpTokenizer,\n",
    "    regexp_tokenize,\n",
    "    TweetTokenizer,\n",
    "    MWETokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38efda0c",
   "metadata": {},
   "source": [
    "## 3.1 Word Tokenization\n",
    "\n",
    "Splitting text into individual words/tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c866da",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello! How are you? I'm doing fine, thanks.\"\n",
    "print(f\"Original text: {text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e695053",
   "metadata": {},
   "source": [
    "### word_tokenize (TreeBank Tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798e8c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard word tokenizer\n",
    "tokens = word_tokenize(text)\n",
    "print(f\"word_tokenize():\")\n",
    "print(f\"  Tokens: {tokens}\")\n",
    "print(f\"  Count: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73d75cd",
   "metadata": {},
   "source": [
    "### wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed24977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punctuation-based tokenizer (splits on all punctuation)\n",
    "punct_tokens = wordpunct_tokenize(text)\n",
    "print(f\"wordpunct_tokenize():\")\n",
    "print(f\"  Tokens: {punct_tokens}\")\n",
    "print(f\"  Count: {len(punct_tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddd908f",
   "metadata": {},
   "source": [
    "### WhitespaceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8837f794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whitespace tokenizer (splits only on whitespace)\n",
    "ws_tokenizer = WhitespaceTokenizer()\n",
    "ws_tokens = ws_tokenizer.tokenize(text)\n",
    "print(f\"WhitespaceTokenizer():\")\n",
    "print(f\"  Tokens: {ws_tokens}\")\n",
    "print(f\"  Count: {len(ws_tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecfe63f",
   "metadata": {},
   "source": [
    "### Comparison on Tricky Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b92fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tricky_texts = [\n",
    "    \"I can't believe it's not butter!\",\n",
    "    \"The price is $19.99 (20% off).\",\n",
    "    \"Visit https://nltk.org for info.\",\n",
    "    \"Dr. Smith works at St. Mary's.\",\n",
    "    \"The U.S.A. won 3-2 in overtime.\",\n",
    "]\n",
    "\n",
    "print(\"Tokenization Comparison\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for text in tricky_texts:\n",
    "    print(f\"\\nText: {text}\")\n",
    "    print(f\"  word_tokenize:      {word_tokenize(text)}\")\n",
    "    print(f\"  wordpunct_tokenize: {wordpunct_tokenize(text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c23bce",
   "metadata": {},
   "source": [
    "## 3.2 Sentence Tokenization\n",
    "\n",
    "Splitting text into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da16379",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Dr. Smith went to the store. He bought milk. \n",
    "The price was $3.50! Can you believe it? \n",
    "Mrs. Johnson said, \"That's expensive.\" \n",
    "I agree... it's too much.\"\"\"\n",
    "\n",
    "print(\"Original text:\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bb89c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence tokenization\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "print(f\"\\nFound {len(sentences)} sentences:\")\n",
    "print(\"-\" * 40)\n",
    "for i, sent in enumerate(sentences, 1):\n",
    "    print(f\"{i}. {sent}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba40deb0",
   "metadata": {},
   "source": [
    "### Custom Sentence Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd3df7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on domain-specific text\n",
    "training_text = \"\"\"\n",
    "This is sentence one. This is sentence two. \n",
    "Dr. Smith went home. Mr. Jones stayed.\n",
    "The U.S.A. is large. Canada is also large.\n",
    "\"\"\"\n",
    "\n",
    "# Create custom tokenizer\n",
    "custom_tokenizer = PunktSentenceTokenizer(training_text)\n",
    "\n",
    "# Test text\n",
    "test_text = \"Dr. Brown said hello. Mrs. Green replied. They discussed the U.S.A. economy.\"\n",
    "custom_sentences = custom_tokenizer.tokenize(test_text)\n",
    "\n",
    "print(f\"Test text: {test_text}\\n\")\n",
    "print(\"Custom tokenizer results:\")\n",
    "for i, sent in enumerate(custom_sentences, 1):\n",
    "    print(f\"  {i}. {sent}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96c6aa7",
   "metadata": {},
   "source": [
    "## 3.3 Regular Expression Tokenization\n",
    "\n",
    "Custom tokenization using regex patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c30f314",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello! Email: user@example.com, Phone: 123-456-7890, Price: $99.99\"\n",
    "print(f\"Original text: {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24b5ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize only words (no punctuation)\n",
    "word_only = RegexpTokenizer(r'\\w+')\n",
    "print(f\"\\nWords only (\\\\w+):\")\n",
    "print(f\"  {word_only.tokenize(text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b94e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize alphabetic words only\n",
    "alpha_only = RegexpTokenizer(r'[a-zA-Z]+')\n",
    "print(f\"Alphabetic only:\")\n",
    "print(f\"  {alpha_only.tokenize(text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796edaed",
   "metadata": {},
   "source": [
    "### Extracting Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32447f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Contact us at support@company.com or sales@company.org.\n",
    "Call 123-456-7890 or 098-765-4321 for assistance.\n",
    "Visit https://www.example.com or http://test.org for info.\n",
    "Prices: $10.99, $25.50, and $100.\"\"\"\n",
    "\n",
    "print(\"Text:\")\n",
    "print(text)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e15a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract different patterns\n",
    "patterns = {\n",
    "    'Emails': r'[\\w\\.-]+@[\\w\\.-]+\\.\\w+',\n",
    "    'Phone numbers': r'\\d{3}-\\d{3}-\\d{4}',\n",
    "    'URLs': r'https?://[\\w\\./]+',\n",
    "    'Prices': r'\\$\\d+\\.?\\d*',\n",
    "}\n",
    "\n",
    "for name, pattern in patterns.items():\n",
    "    matches = regexp_tokenize(text, pattern)\n",
    "    print(f\"{name}: {matches}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad04071f",
   "metadata": {},
   "source": [
    "### Gap-based Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff2063b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split on pattern (gaps=True)\n",
    "text = \"word1---word2===word3...word4\"\n",
    "\n",
    "# Split on non-word characters\n",
    "gap_tokenizer = RegexpTokenizer(r'\\W+', gaps=True)\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Gap-based tokenization: {gap_tokenizer.tokenize(text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e1344c",
   "metadata": {},
   "source": [
    "## 3.4 Tweet Tokenization\n",
    "\n",
    "Specialized tokenizer for social media text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335f8518",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [\n",
    "    \"This is a cooool #NLP tweet! @user :-) ðŸ˜Š https://example.com\",\n",
    "    \"OMG!!! I loooove this!!! ðŸ˜ðŸ˜ðŸ˜ #amazing #wow\",\n",
    "    \"@scientist Check out this AMAZING discovery! ðŸ”¬\",\n",
    "    \"Can't believe it's not butter! ðŸ˜‚ðŸ˜‚ #funny\",\n",
    "]\n",
    "\n",
    "print(\"Sample tweets:\")\n",
    "for i, t in enumerate(tweets, 1):\n",
    "    print(f\"  {i}. {t}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d995cb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default tweet tokenizer\n",
    "tknzr_default = TweetTokenizer()\n",
    "\n",
    "print(\"Default TweetTokenizer:\")\n",
    "print(\"=\" * 60)\n",
    "for tweet in tweets:\n",
    "    print(f\"Tweet: {tweet}\")\n",
    "    print(f\"Tokens: {tknzr_default.tokenize(tweet)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a3b80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customized tweet tokenizer\n",
    "tknzr_custom = TweetTokenizer(\n",
    "    preserve_case=False,      # Lowercase\n",
    "    strip_handles=True,       # Remove @mentions\n",
    "    reduce_len=True           # Reduce repeated chars (coooool -> coool)\n",
    ")\n",
    "\n",
    "print(\"Customized TweetTokenizer (lowercase, no handles, reduced length):\")\n",
    "print(\"=\" * 60)\n",
    "for tweet in tweets:\n",
    "    print(f\"Tweet: {tweet}\")\n",
    "    print(f\"Tokens: {tknzr_custom.tokenize(tweet)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6425b09e",
   "metadata": {},
   "source": [
    "### TweetTokenizer Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b2cc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = \"AMAZING!!! @user This is sooooo cool!!! ðŸ˜ŠðŸ˜Š #NLP\"\n",
    "\n",
    "options = [\n",
    "    {\"preserve_case\": True, \"strip_handles\": False, \"reduce_len\": False},\n",
    "    {\"preserve_case\": False, \"strip_handles\": False, \"reduce_len\": False},\n",
    "    {\"preserve_case\": False, \"strip_handles\": True, \"reduce_len\": False},\n",
    "    {\"preserve_case\": False, \"strip_handles\": True, \"reduce_len\": True},\n",
    "]\n",
    "\n",
    "print(f\"Tweet: {tweet}\\n\")\n",
    "print(f\"{'Options':<60} Result\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for opts in options:\n",
    "    tknzr = TweetTokenizer(**opts)\n",
    "    tokens = tknzr.tokenize(tweet)\n",
    "    print(f\"{str(opts):<60} {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb59fa9e",
   "metadata": {},
   "source": [
    "## 3.5 Multi-Word Expression (MWE) Tokenization\n",
    "\n",
    "Keep multi-word expressions together as single tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f448f313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MWE tokenizer\n",
    "mwe_tokenizer = MWETokenizer()\n",
    "\n",
    "# Add multi-word expressions\n",
    "mwe_tokenizer.add_mwe(('New', 'York'))\n",
    "mwe_tokenizer.add_mwe(('New', 'York', 'City'))\n",
    "mwe_tokenizer.add_mwe(('United', 'States'))\n",
    "mwe_tokenizer.add_mwe(('machine', 'learning'))\n",
    "mwe_tokenizer.add_mwe(('natural', 'language', 'processing'))\n",
    "mwe_tokenizer.add_mwe(('San', 'Francisco'))\n",
    "\n",
    "print(\"MWEs added: New York, New York City, United States,\")\n",
    "print(\"            machine learning, natural language processing, San Francisco\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc728f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sentences\n",
    "sentences = [\n",
    "    \"I love machine learning in New York\",\n",
    "    \"Natural language processing is used in the United States\",\n",
    "    \"New York City and San Francisco are great cities\",\n",
    "]\n",
    "\n",
    "print(\"MWE Tokenization Results:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for sent in sentences:\n",
    "    # First tokenize with standard tokenizer\n",
    "    tokens = word_tokenize(sent)\n",
    "    # Then apply MWE tokenizer\n",
    "    mwe_tokens = mwe_tokenizer.tokenize(tokens)\n",
    "    \n",
    "    print(f\"\\nSentence: {sent}\")\n",
    "    print(f\"  Standard: {tokens}\")\n",
    "    print(f\"  MWE:      {mwe_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e7861f",
   "metadata": {},
   "source": [
    "### Technical Domain MWE Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45d0fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Technical/NLP multi-word expressions\n",
    "technical_mwes = [\n",
    "    ('deep', 'learning'),\n",
    "    ('neural', 'network'),\n",
    "    ('artificial', 'intelligence'),\n",
    "    ('named', 'entity', 'recognition'),\n",
    "    ('part', 'of', 'speech'),\n",
    "    ('support', 'vector', 'machine'),\n",
    "    ('random', 'forest'),\n",
    "]\n",
    "\n",
    "tech_tokenizer = MWETokenizer(technical_mwes, separator='_')\n",
    "\n",
    "text = \"Deep learning and neural network are part of artificial intelligence\"\n",
    "tokens = word_tokenize(text.lower())\n",
    "mwe_tokens = tech_tokenizer.tokenize(tokens)\n",
    "\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"\\nStandard tokens: {tokens}\")\n",
    "print(f\"MWE tokens:      {mwe_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155e3af9",
   "metadata": {},
   "source": [
    "## 3.6 Tokenization with Spans\n",
    "\n",
    "Get token positions in the original text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af4a655",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "text = \"Hello world! How are you?\"\n",
    "print(f\"Text: '{text}'\")\n",
    "print(f\"       0123456789...\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b452666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get spans with WhitespaceTokenizer\n",
    "ws_tokenizer = WhitespaceTokenizer()\n",
    "spans = list(ws_tokenizer.span_tokenize(text))\n",
    "\n",
    "print(\"WhitespaceTokenizer spans:\")\n",
    "print(f\"{'Start':>6} {'End':>6}  Token\")\n",
    "print(\"-\" * 25)\n",
    "for start, end in spans:\n",
    "    token = text[start:end]\n",
    "    print(f\"{start:>6} {end:>6}  '{token}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a23f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get spans with TreebankWordTokenizer\n",
    "tb_tokenizer = TreebankWordTokenizer()\n",
    "spans = list(tb_tokenizer.span_tokenize(text))\n",
    "\n",
    "print(\"TreebankWordTokenizer spans:\")\n",
    "print(f\"{'Start':>6} {'End':>6}  Token\")\n",
    "print(\"-\" * 25)\n",
    "for start, end in spans:\n",
    "    token = text[start:end]\n",
    "    print(f\"{start:>6} {end:>6}  '{token}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c450a45",
   "metadata": {},
   "source": [
    "## 3.7 Practical Tokenization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91635259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "def clean_tokenize(text, lowercase=True, remove_punct=True, min_length=1):\n",
    "    \"\"\"Clean and tokenize text with options\"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    if lowercase:\n",
    "        tokens = [t.lower() for t in tokens]\n",
    "    \n",
    "    if remove_punct:\n",
    "        tokens = [t for t in tokens if t not in string.punctuation]\n",
    "    \n",
    "    tokens = [t for t in tokens if len(t) >= min_length]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def tokenize_for_nlp(text):\n",
    "    \"\"\"Tokenize text suitable for NLP tasks (removes stopwords)\"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [t for t in tokens if t.isalnum() and t not in stop_words]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1705c29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The quick brown fox jumps over the lazy dog! It's amazing, isn't it?\"\n",
    "\n",
    "print(f\"Text: {text}\\n\")\n",
    "print(f\"clean_tokenize (default):      {clean_tokenize(text)}\")\n",
    "print(f\"clean_tokenize (keep case):    {clean_tokenize(text, lowercase=False)}\")\n",
    "print(f\"clean_tokenize (min_length=3): {clean_tokenize(text, min_length=3)}\")\n",
    "print(f\"tokenize_for_nlp:              {tokenize_for_nlp(text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37e7aa8",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Tokenizer | Best For | Example |\n",
    "|-----------|----------|----------|\n",
    "| `word_tokenize` | General text | \"don't\" â†’ [\"do\", \"n't\"] |\n",
    "| `wordpunct_tokenize` | Punctuation-sensitive | \"don't\" â†’ [\"don\", \"'\", \"t\"] |\n",
    "| `WhitespaceTokenizer` | Simple splitting | Splits only on spaces |\n",
    "| `sent_tokenize` | Sentence splitting | Handles Dr., Mrs., etc. |\n",
    "| `RegexpTokenizer` | Custom patterns | Extract emails, phones |\n",
    "| `TweetTokenizer` | Social media | Handles emojis, hashtags |\n",
    "| `MWETokenizer` | Multi-word terms | \"New York\" â†’ \"New_York\" |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
