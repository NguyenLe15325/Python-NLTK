{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14a0b588",
   "metadata": {},
   "source": [
    "# NLTK Complete Guide - Section 4: Stopwords & Text Cleaning\n",
    "\n",
    "This notebook covers:\n",
    "- Stopwords Basics\n",
    "- Custom Stopwords\n",
    "- Text Cleaning Functions\n",
    "- Complete Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20c878d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72c7fe2",
   "metadata": {},
   "source": [
    "## 4.1 Stopwords Basics\n",
    "\n",
    "**Stopwords** are common words that usually don't carry much meaning (the, is, at, which, on, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa71b8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "print(f\"Number of English stopwords: {len(stop_words)}\")\n",
    "print(f\"\\nFirst 30 stopwords (alphabetically):\")\n",
    "print(sorted(stop_words)[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a61200a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All available languages\n",
    "print(\"Available languages:\")\n",
    "print(stopwords.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f4fe09",
   "metadata": {},
   "source": [
    "### Stopwords by Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba83eb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = ['english', 'spanish', 'french', 'german', 'italian', 'portuguese']\n",
    "\n",
    "print(f\"{'Language':<12} {'Count':>6}  Sample words\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for lang in languages:\n",
    "    words = stopwords.words(lang)\n",
    "    sample = words[:5]\n",
    "    print(f\"{lang.capitalize():<12} {len(words):>6}  {sample}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fde019d",
   "metadata": {},
   "source": [
    "### Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724b5a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is a sample sentence showing off the stop words filtration.\"\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Tokenize\n",
    "tokens = word_tokenize(text.lower())\n",
    "\n",
    "# Remove stopwords\n",
    "filtered = [w for w in tokens if w not in stop_words]\n",
    "\n",
    "# Remove stopwords AND punctuation\n",
    "filtered_clean = [w for w in tokens if w not in stop_words and w.isalnum()]\n",
    "\n",
    "print(f\"Original: {text}\")\n",
    "print(f\"\\nTokens ({len(tokens)}): {tokens}\")\n",
    "print(f\"\\nWithout stopwords ({len(filtered)}): {filtered}\")\n",
    "print(f\"\\nWithout stopwords + punct ({len(filtered_clean)}): {filtered_clean}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bea76fd",
   "metadata": {},
   "source": [
    "## 4.2 Custom Stopwords\n",
    "\n",
    "Modify stopwords for your specific use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d82e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_stopwords = set(stopwords.words('english'))\n",
    "print(f\"Base stopwords: {len(base_stopwords)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3bdf6d",
   "metadata": {},
   "source": [
    "### Extend Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa038fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add domain-specific or common words\n",
    "extended_stopwords = base_stopwords.union({'also', 'however', 'therefore', 'thus', 'hence', 'would', 'could'})\n",
    "\n",
    "print(f\"Extended stopwords: {len(extended_stopwords)}\")\n",
    "print(f\"Added words: {extended_stopwords - base_stopwords}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3084a211",
   "metadata": {},
   "source": [
    "### Keep Negations (for Sentiment Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb4e9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove negations from stopwords (important for sentiment!)\n",
    "negations = {'not', 'no', 'nor', 'neither', 'never', \"don't\", \"won't\", \"can't\", \"isn't\", \"aren't\"}\n",
    "sentiment_stopwords = base_stopwords - negations\n",
    "\n",
    "print(f\"Sentiment stopwords: {len(sentiment_stopwords)}\")\n",
    "print(f\"Kept words: {base_stopwords - sentiment_stopwords}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe815f4",
   "metadata": {},
   "source": [
    "### Comparison: Standard vs Sentiment-Aware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88207fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This product is not good and I would never recommend it to anyone.\"\n",
    "tokens = word_tokenize(text.lower())\n",
    "\n",
    "# Standard removal\n",
    "standard = [w for w in tokens if w not in base_stopwords and w.isalnum()]\n",
    "\n",
    "# Sentiment-aware removal\n",
    "sentiment = [w for w in tokens if w not in sentiment_stopwords and w.isalnum()]\n",
    "\n",
    "print(f\"Original: {text}\")\n",
    "print(f\"\\nStandard stopword removal: {standard}\")\n",
    "print(\"  ‚ö†Ô∏è  'not' and 'never' removed - loses negative sentiment!\")\n",
    "print(f\"\\nSentiment-aware removal: {sentiment}\")\n",
    "print(\"  ‚úÖ 'not' and 'never' preserved - keeps negative sentiment!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b0cc50",
   "metadata": {},
   "source": [
    "### Domain-Specific Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643a6d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Medical domain\n",
    "medical_stopwords = base_stopwords.union({\n",
    "    'patient', 'patients', 'doctor', 'hospital', 'treatment', \n",
    "    'medical', 'clinical', 'symptoms', 'condition', 'diagnosis'\n",
    "})\n",
    "\n",
    "# Legal domain\n",
    "legal_stopwords = base_stopwords.union({\n",
    "    'court', 'plaintiff', 'defendant', 'hereby', 'whereas',\n",
    "    'shall', 'pursuant', 'herein', 'thereof', 'aforementioned'\n",
    "})\n",
    "\n",
    "print(f\"Medical stopwords: {len(medical_stopwords)}\")\n",
    "print(f\"Legal stopwords: {len(legal_stopwords)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f327cd",
   "metadata": {},
   "source": [
    "## 4.3 Text Cleaning Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27897560",
   "metadata": {},
   "source": [
    "### Basic Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f9b45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_basic(text):\n",
    "    \"\"\"Basic text cleaning\"\"\"\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Test\n",
    "dirty = \"  Hello,   WORLD!!!   How are   you???  \"\n",
    "print(f\"Before: '{dirty}'\")\n",
    "print(f\"After:  '{clean_text_basic(dirty)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9ffaeb",
   "metadata": {},
   "source": [
    "### Advanced Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f231ab14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_advanced(text):\n",
    "    \"\"\"Advanced text cleaning with regex\"\"\"\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    \n",
    "    # Remove mentions (@username)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    \n",
    "    # Remove hashtags\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    \n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7912539",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_text = \"\"\"\n",
    "Check out https://example.com!!! \n",
    "<b>AMAZING</b> deal @store #sale \n",
    "Contact: user@email.com\n",
    "Price: $99.99 (50% OFF!!!)\n",
    "\"\"\"\n",
    "\n",
    "print(\"Original:\")\n",
    "print(dirty_text)\n",
    "print(\"\\nBasic cleaning:\")\n",
    "print(clean_text_basic(dirty_text))\n",
    "print(\"\\nAdvanced cleaning:\")\n",
    "print(clean_text_advanced(dirty_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc02eda",
   "metadata": {},
   "source": [
    "## 4.4 Complete Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89194ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, \n",
    "                    lowercase=True,\n",
    "                    remove_urls=True,\n",
    "                    remove_html=True,\n",
    "                    remove_emails=True,\n",
    "                    remove_mentions=True,\n",
    "                    remove_hashtags=False,  # Keep hashtag text\n",
    "                    remove_numbers=True,\n",
    "                    remove_punctuation=True,\n",
    "                    remove_stopwords_flag=True,\n",
    "                    min_word_length=2,\n",
    "                    custom_stopwords=None):\n",
    "    \"\"\"\n",
    "    Complete text preprocessing pipeline.\n",
    "    \n",
    "    Returns: List of cleaned tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    \n",
    "    if remove_urls:\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    \n",
    "    if remove_html:\n",
    "        text = re.sub(r'<[^>]+>', '', text)\n",
    "    \n",
    "    if remove_emails:\n",
    "        text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    \n",
    "    if remove_mentions:\n",
    "        text = re.sub(r'@\\w+', '', text)\n",
    "    \n",
    "    if remove_hashtags:\n",
    "        text = re.sub(r'#\\w+', '', text)\n",
    "    else:\n",
    "        # Keep hashtag text, just remove #\n",
    "        text = re.sub(r'#(\\w+)', r'\\1', text)\n",
    "    \n",
    "    if remove_numbers:\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    if remove_punctuation:\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    if remove_stopwords_flag:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        if custom_stopwords:\n",
    "            stop_words = stop_words.union(custom_stopwords)\n",
    "        tokens = [t for t in tokens if t not in stop_words]\n",
    "    \n",
    "    # Filter by length\n",
    "    tokens = [t for t in tokens if len(t) >= min_word_length]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f212ce0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "messy_text = \"\"\"\n",
    "üéâ Check out our AMAZING new product at https://shop.example.com! \n",
    "<p>Contact support@company.com for help.</p>\n",
    "@customer said: \"This is the BEST thing I've bought in 2024!!!\"\n",
    "#happy #satisfied\n",
    "Price: only $49.99 (was $99.99) - 50% OFF!!! \n",
    "Limited time offer... Don't miss out!!!\n",
    "\"\"\"\n",
    "\n",
    "print(\"Original text:\")\n",
    "print(messy_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8d2260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default preprocessing\n",
    "tokens = preprocess_text(messy_text)\n",
    "print(\"Default preprocessing:\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e4c45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep hashtags (as text)\n",
    "tokens = preprocess_text(messy_text, remove_hashtags=False)\n",
    "print(\"Keep hashtags:\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf5da38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For sentiment analysis (keep negations)\n",
    "sentiment_stops = set(stopwords.words('english')) - {'not', 'no', 'never', \"don't\", \"won't\"}\n",
    "tokens = preprocess_text(\n",
    "    \"This is not good and I don't like it at all!\",\n",
    "    remove_stopwords_flag=True,\n",
    ")\n",
    "print(\"For sentiment:\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d4c428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal preprocessing (keep more)\n",
    "tokens = preprocess_text(\n",
    "    messy_text,\n",
    "    remove_stopwords_flag=False,\n",
    "    remove_numbers=False,\n",
    "    min_word_length=1\n",
    ")\n",
    "print(\"Minimal preprocessing:\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5451280f",
   "metadata": {},
   "source": [
    "## 4.5 Text Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21277603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text):\n",
    "    \"\"\"Expand common contractions\"\"\"\n",
    "    contractions = {\n",
    "        \"won't\": \"will not\",\n",
    "        \"can't\": \"cannot\",\n",
    "        \"n't\": \" not\",\n",
    "        \"'re\": \" are\",\n",
    "        \"'s\": \" is\",\n",
    "        \"'d\": \" would\",\n",
    "        \"'ll\": \" will\",\n",
    "        \"'ve\": \" have\",\n",
    "        \"'m\": \" am\",\n",
    "    }\n",
    "    \n",
    "    for contraction, expansion in contractions.items():\n",
    "        text = text.replace(contraction, expansion)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Test\n",
    "text = \"I can't believe it's working! We're so happy!\"\n",
    "print(f\"Original:  {text}\")\n",
    "print(f\"Expanded:  {expand_contractions(text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a5f038",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_repeated_chars(text):\n",
    "    \"\"\"Reduce repeated characters (coooool -> cool)\"\"\"\n",
    "    # Reduce 3+ repeated chars to 2\n",
    "    return re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n",
    "\n",
    "# Test\n",
    "text = \"This is sooooo coooool! I loooove it!!!\"\n",
    "print(f\"Original:   {text}\")\n",
    "print(f\"Normalized: {normalize_repeated_chars(text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66d344b",
   "metadata": {},
   "source": [
    "## 4.6 Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff3dfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_corpus(documents, **kwargs):\n",
    "    \"\"\"Preprocess a list of documents\"\"\"\n",
    "    return [preprocess_text(doc, **kwargs) for doc in documents]\n",
    "\n",
    "# Example documents\n",
    "documents = [\n",
    "    \"Natural language processing is fascinating!\",\n",
    "    \"Machine learning enables NLP applications.\",\n",
    "    \"Deep learning has transformed NLP research.\",\n",
    "    \"Text preprocessing is essential for NLP.\",\n",
    "    \"NLP combines linguistics and computer science.\",\n",
    "]\n",
    "\n",
    "# Preprocess all\n",
    "processed = preprocess_corpus(documents)\n",
    "\n",
    "print(\"Original ‚Üí Processed\")\n",
    "print(\"=\" * 60)\n",
    "for orig, proc in zip(documents, processed):\n",
    "    print(f\"{orig}\")\n",
    "    print(f\"  ‚Üí {proc}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c8a1b4",
   "metadata": {},
   "source": [
    "### Create Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d231eecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def create_vocabulary(tokenized_docs, min_freq=1):\n",
    "    \"\"\"Create vocabulary from tokenized documents\"\"\"\n",
    "    word_counts = Counter()\n",
    "    for doc in tokenized_docs:\n",
    "        word_counts.update(doc)\n",
    "    \n",
    "    # Filter by frequency\n",
    "    vocab = {word for word, count in word_counts.items() if count >= min_freq}\n",
    "    \n",
    "    return vocab, word_counts\n",
    "\n",
    "vocab, counts = create_vocabulary(processed)\n",
    "\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "print(f\"\\nMost common words:\")\n",
    "for word, count in counts.most_common(10):\n",
    "    print(f\"  {word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66d45dd",
   "metadata": {},
   "source": [
    "## 4.7 TextCleaner Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f48c06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCleaner:\n",
    "    \"\"\"Reusable text cleaning utility\"\"\"\n",
    "    \n",
    "    def __init__(self, language='english'):\n",
    "        self.stop_words = set(stopwords.words(language))\n",
    "        self.custom_stopwords = set()\n",
    "    \n",
    "    def add_stopwords(self, words):\n",
    "        \"\"\"Add custom stopwords\"\"\"\n",
    "        self.custom_stopwords.update(words)\n",
    "        return self\n",
    "    \n",
    "    def keep_words(self, words):\n",
    "        \"\"\"Remove words from stopwords (keep them)\"\"\"\n",
    "        self.stop_words -= set(words)\n",
    "        return self\n",
    "    \n",
    "    def clean(self, text):\n",
    "        \"\"\"Clean text with configured options\"\"\"\n",
    "        # Lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+', '', text)\n",
    "        \n",
    "        # Remove HTML\n",
    "        text = re.sub(r'<[^>]+>', '', text)\n",
    "        \n",
    "        # Remove punctuation\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        \n",
    "        # Remove numbers\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # Remove stopwords\n",
    "        all_stopwords = self.stop_words.union(self.custom_stopwords)\n",
    "        tokens = [t for t in tokens if t not in all_stopwords and len(t) > 1]\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def clean_batch(self, texts):\n",
    "        \"\"\"Clean multiple texts\"\"\"\n",
    "        return [self.clean(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39a0aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cleaner\n",
    "cleaner = TextCleaner()\n",
    "\n",
    "# Configure for sentiment analysis\n",
    "cleaner.keep_words(['not', 'no', 'never'])  # Keep negations\n",
    "cleaner.add_stopwords(['said', 'also'])     # Remove common words\n",
    "\n",
    "# Test\n",
    "text = \"I said this product is not good. Also, it never works properly!\"\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Cleaned: {cleaner.clean(text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18419156",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Task | Code |\n",
    "|------|------|\n",
    "| Get stopwords | `stopwords.words('english')` |\n",
    "| Remove stopwords | `[w for w in tokens if w not in stop_words]` |\n",
    "| Add stopwords | `stop_words.union({'word1', 'word2'})` |\n",
    "| Remove from stopwords | `stop_words - {'not', 'no'}` |\n",
    "| Remove URLs | `re.sub(r'http\\S+', '', text)` |\n",
    "| Remove HTML | `re.sub(r'<[^>]+>', '', text)` |\n",
    "| Remove punctuation | `re.sub(r'[^\\w\\s]', '', text)` |\n",
    "\n",
    "### Best Practices\n",
    "1. **For sentiment analysis**: Keep negations (not, no, never)\n",
    "2. **For topic modeling**: Remove domain-specific common words\n",
    "3. **For search**: More aggressive cleaning is usually better\n",
    "4. **Always**: Remove URLs, HTML, and normalize whitespace"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
