{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e89c8955",
   "metadata": {},
   "source": [
    "# NLTK Complete Guide - Section 7: Part-of-Speech (POS) Tagging\n",
    "\n",
    "This notebook covers:\n",
    "- What is POS Tagging?\n",
    "- NLTK POS Taggers\n",
    "- Penn Treebank Tag Set\n",
    "- Custom Taggers\n",
    "- Practical Applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab9b81f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('tagsets', quiet=True)\n",
    "nltk.download('universal_tagset', quiet=True)\n",
    "nltk.download('brown', quiet=True)\n",
    "\n",
    "from nltk import pos_tag, pos_tag_sents\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879207b0",
   "metadata": {},
   "source": [
    "## 7.1 What is POS Tagging?\n",
    "\n",
    "**Part-of-Speech (POS) Tagging** assigns grammatical categories to words:\n",
    "- Noun, Verb, Adjective, Adverb\n",
    "- Pronoun, Preposition, Conjunction\n",
    "- And more specific subcategories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bff8eea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: The quick brown fox jumps over the lazy dog.\n",
      "\n",
      "POS Tags:\n",
      "[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Tokenize and tag\n",
    "tokens = word_tokenize(sentence)\n",
    "tagged = pos_tag(tokens)\n",
    "\n",
    "print(f\"Sentence: {sentence}\\n\")\n",
    "print(\"POS Tags:\")\n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85427d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word         Tag    Description\n",
      "--------------------------------------------------\n",
      "The          DT     Determiner\n",
      "quick        JJ     Adjective\n",
      "brown        NN     Noun (singular)\n",
      "fox          NN     Noun (singular)\n",
      "jumps        VBZ    Verb (3rd person singular)\n",
      "over         IN     Preposition\n",
      "the          DT     Determiner\n",
      "lazy         JJ     Adjective\n",
      "dog          NN     Noun (singular)\n",
      ".            .      Punctuation\n"
     ]
    }
   ],
   "source": [
    "# Pretty print\n",
    "print(f\"{'Word':<12} {'Tag':<6} {'Description'}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "tag_descriptions = {\n",
    "    'DT': 'Determiner',\n",
    "    'JJ': 'Adjective',\n",
    "    'NN': 'Noun (singular)',\n",
    "    'NNS': 'Noun (plural)',\n",
    "    'VBZ': 'Verb (3rd person singular)',\n",
    "    'IN': 'Preposition',\n",
    "    '.': 'Punctuation',\n",
    "}\n",
    "\n",
    "for word, tag in tagged:\n",
    "    desc = tag_descriptions.get(tag, 'Other')\n",
    "    print(f\"{word:<12} {tag:<6} {desc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a4c23b",
   "metadata": {},
   "source": [
    "## 7.2 Penn Treebank Tag Set\n",
    "\n",
    "NLTK uses the Penn Treebank tagset by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4602e41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common Penn Treebank POS Tags\n",
      "============================================================\n",
      "NN     Noun, singular (dog, city)\n",
      "NNS    Noun, plural (dogs, cities)\n",
      "NNP    Proper noun, singular (John, London)\n",
      "NNPS   Proper noun, plural (Americans)\n",
      "VB     Verb, base form (run, eat)\n",
      "VBD    Verb, past tense (ran, ate)\n",
      "VBG    Verb, gerund (running, eating)\n",
      "VBN    Verb, past participle (eaten, written)\n",
      "VBP    Verb, non-3rd person (run, eat)\n",
      "VBZ    Verb, 3rd person singular (runs, eats)\n",
      "JJ     Adjective (big, green)\n",
      "JJR    Adjective, comparative (bigger)\n",
      "JJS    Adjective, superlative (biggest)\n",
      "RB     Adverb (quickly, very)\n",
      "RBR    Adverb, comparative (faster)\n",
      "RBS    Adverb, superlative (fastest)\n",
      "PRP    Personal pronoun (I, you, he)\n",
      "PRP$   Possessive pronoun (my, your)\n",
      "DT     Determiner (the, a, an)\n",
      "IN     Preposition (in, on, at)\n",
      "CC     Coordinating conjunction (and, or)\n",
      "TO     to\n",
      "MD     Modal (can, will, should)\n"
     ]
    }
   ],
   "source": [
    "# Common POS tags\n",
    "common_tags = {\n",
    "    # Nouns\n",
    "    'NN': 'Noun, singular (dog, city)',\n",
    "    'NNS': 'Noun, plural (dogs, cities)',\n",
    "    'NNP': 'Proper noun, singular (John, London)',\n",
    "    'NNPS': 'Proper noun, plural (Americans)',\n",
    "    \n",
    "    # Verbs\n",
    "    'VB': 'Verb, base form (run, eat)',\n",
    "    'VBD': 'Verb, past tense (ran, ate)',\n",
    "    'VBG': 'Verb, gerund (running, eating)',\n",
    "    'VBN': 'Verb, past participle (eaten, written)',\n",
    "    'VBP': 'Verb, non-3rd person (run, eat)',\n",
    "    'VBZ': 'Verb, 3rd person singular (runs, eats)',\n",
    "    \n",
    "    # Adjectives\n",
    "    'JJ': 'Adjective (big, green)',\n",
    "    'JJR': 'Adjective, comparative (bigger)',\n",
    "    'JJS': 'Adjective, superlative (biggest)',\n",
    "    \n",
    "    # Adverbs\n",
    "    'RB': 'Adverb (quickly, very)',\n",
    "    'RBR': 'Adverb, comparative (faster)',\n",
    "    'RBS': 'Adverb, superlative (fastest)',\n",
    "    \n",
    "    # Others\n",
    "    'PRP': 'Personal pronoun (I, you, he)',\n",
    "    'PRP$': 'Possessive pronoun (my, your)',\n",
    "    'DT': 'Determiner (the, a, an)',\n",
    "    'IN': 'Preposition (in, on, at)',\n",
    "    'CC': 'Coordinating conjunction (and, or)',\n",
    "    'TO': 'to',\n",
    "    'MD': 'Modal (can, will, should)',\n",
    "}\n",
    "\n",
    "print(\"Common Penn Treebank POS Tags\")\n",
    "print(\"=\" * 60)\n",
    "for tag, description in common_tags.items():\n",
    "    print(f\"{tag:<6} {description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9694476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n"
     ]
    }
   ],
   "source": [
    "# Get help on a specific tag\n",
    "nltk.help.upenn_tagset('VBG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06acb7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n"
     ]
    }
   ],
   "source": [
    "# All noun tags\n",
    "nltk.help.upenn_tagset('NN.*')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc86843",
   "metadata": {},
   "source": [
    "## 7.3 Universal Tagset\n",
    "\n",
    "Simplified tagset that works across languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a5b7ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word         Penn     Universal \n",
      "-----------------------------------\n",
      "The          DT       DET       \n",
      "quick        JJ       ADJ       \n",
      "brown        NN       NOUN      \n",
      "fox          NN       NOUN      \n",
      "jumps        VBZ      VERB      \n",
      "over         IN       ADP       \n",
      "the          DT       DET       \n",
      "lazy         JJ       ADJ       \n",
      "dog          NN       NOUN      \n",
      ".            .        .         \n"
     ]
    }
   ],
   "source": [
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "tokens = word_tokenize(sentence)\n",
    "\n",
    "# Default (Penn Treebank)\n",
    "penn_tags = pos_tag(tokens)\n",
    "\n",
    "# Universal tagset\n",
    "universal_tags = pos_tag(tokens, tagset='universal')\n",
    "\n",
    "print(f\"{'Word':<12} {'Penn':<8} {'Universal':<10}\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "for (word, penn), (_, univ) in zip(penn_tags, universal_tags):\n",
    "    print(f\"{word:<12} {penn:<8} {univ:<10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4f5f710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Universal Tagset\n",
      "========================================\n",
      "NOUN     Nouns\n",
      "VERB     Verbs\n",
      "ADJ      Adjectives\n",
      "ADV      Adverbs\n",
      "PRON     Pronouns\n",
      "DET      Determiners\n",
      "ADP      Adpositions (prepositions)\n",
      "NUM      Numbers\n",
      "CONJ     Conjunctions\n",
      "PRT      Particles\n",
      ".        Punctuation\n",
      "X        Other\n"
     ]
    }
   ],
   "source": [
    "# Universal tags\n",
    "universal_tagset = {\n",
    "    'NOUN': 'Nouns',\n",
    "    'VERB': 'Verbs',\n",
    "    'ADJ': 'Adjectives',\n",
    "    'ADV': 'Adverbs',\n",
    "    'PRON': 'Pronouns',\n",
    "    'DET': 'Determiners',\n",
    "    'ADP': 'Adpositions (prepositions)',\n",
    "    'NUM': 'Numbers',\n",
    "    'CONJ': 'Conjunctions',\n",
    "    'PRT': 'Particles',\n",
    "    '.': 'Punctuation',\n",
    "    'X': 'Other',\n",
    "}\n",
    "\n",
    "print(\"Universal Tagset\")\n",
    "print(\"=\" * 40)\n",
    "for tag, desc in universal_tagset.items():\n",
    "    print(f\"{tag:<8} {desc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa792027",
   "metadata": {},
   "source": [
    "## 7.4 Tagging Multiple Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44e50080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method 1: Individual sentences\n",
      "==================================================\n",
      "\n",
      "Natural language processing is fascinating.\n",
      "Tags: [('Natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('is', 'VBZ'), ('fascinating', 'VBG'), ('.', '.')]\n",
      "\n",
      "It enables computers to understand human language.\n",
      "Tags: [('It', 'PRP'), ('enables', 'VBZ'), ('computers', 'NNS'), ('to', 'TO'), ('understand', 'VB'), ('human', 'JJ'), ('language', 'NN'), ('.', '.')]\n",
      "\n",
      "Many applications use NLP today.\n",
      "Tags: [('Many', 'JJ'), ('applications', 'NNS'), ('use', 'VBP'), ('NLP', 'NNP'), ('today', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Natural language processing is fascinating.\n",
    "It enables computers to understand human language.\n",
    "Many applications use NLP today.\"\"\"\n",
    "\n",
    "# Method 1: Tag sentence by sentence\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "print(\"Method 1: Individual sentences\")\n",
    "print(\"=\" * 50)\n",
    "for sent in sentences:\n",
    "    tokens = word_tokenize(sent)\n",
    "    tagged = pos_tag(tokens)\n",
    "    print(f\"\\n{sent}\")\n",
    "    print(f\"Tags: {tagged}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d478701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method 2: Batch tagging (pos_tag_sents)\n",
      "==================================================\n",
      "\n",
      "Sentence 1: [('Natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('is', 'VBZ'), ('fascinating', 'VBG'), ('.', '.')]\n",
      "\n",
      "Sentence 2: [('It', 'PRP'), ('enables', 'VBZ'), ('computers', 'NNS'), ('to', 'TO'), ('understand', 'VB'), ('human', 'JJ'), ('language', 'NN'), ('.', '.')]\n",
      "\n",
      "Sentence 3: [('Many', 'JJ'), ('applications', 'NNS'), ('use', 'VBP'), ('NLP', 'NNP'), ('today', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Method 2: Batch tagging (more efficient)\n",
    "tokenized_sents = [word_tokenize(s) for s in sentences]\n",
    "tagged_sents = pos_tag_sents(tokenized_sents)\n",
    "\n",
    "print(\"Method 2: Batch tagging (pos_tag_sents)\")\n",
    "print(\"=\" * 50)\n",
    "for i, tagged in enumerate(tagged_sents, 1):\n",
    "    print(f\"\\nSentence {i}: {tagged}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5e5eeb",
   "metadata": {},
   "source": [
    "## 7.5 Context-Dependent POS\n",
    "\n",
    "The same word can have different POS tags depending on context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "973df61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'book' in different contexts:\n",
      "----------------------------------------\n",
      "I read a book.                 book = NN\n",
      "Please book a table.           book = NN\n"
     ]
    }
   ],
   "source": [
    "# \"book\" as noun vs verb\n",
    "sentences = [\n",
    "    \"I read a book.\",           # book = noun\n",
    "    \"Please book a table.\",     # book = verb\n",
    "]\n",
    "\n",
    "print(\"'book' in different contexts:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for sent in sentences:\n",
    "    tokens = word_tokenize(sent)\n",
    "    tagged = pos_tag(tokens)\n",
    "    book_tag = [t for w, t in tagged if w.lower() == 'book'][0]\n",
    "    print(f\"{sent:<30} book = {book_tag}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea491b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context-Dependent POS Tags\n",
      "=======================================================\n",
      "Sentence                            Word     Tag\n",
      "-------------------------------------------------------\n",
      "I run every day.                    run      VBP\n",
      "The run was exhausting.             run      NN\n",
      "She can fish.                       fish     VB\n",
      "I caught a fish.                    fish     NN\n",
      "Light the candle.                   light    NNP\n",
      "The light is bright.                light    NN\n",
      "This box is light.                  light    JJ\n"
     ]
    }
   ],
   "source": [
    "# More examples of context-dependent tags\n",
    "ambiguous_examples = [\n",
    "    (\"I run every day.\", \"run\"),\n",
    "    (\"The run was exhausting.\", \"run\"),\n",
    "    (\"She can fish.\", \"fish\"),\n",
    "    (\"I caught a fish.\", \"fish\"),\n",
    "    (\"Light the candle.\", \"light\"),\n",
    "    (\"The light is bright.\", \"light\"),\n",
    "    (\"This box is light.\", \"light\"),\n",
    "]\n",
    "\n",
    "print(\"Context-Dependent POS Tags\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"{'Sentence':<35} {'Word':<8} {'Tag'}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for sent, target_word in ambiguous_examples:\n",
    "    tokens = word_tokenize(sent)\n",
    "    tagged = pos_tag(tokens)\n",
    "    word_tag = [t for w, t in tagged if w.lower() == target_word][0]\n",
    "    print(f\"{sent:<35} {target_word:<8} {word_tag}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f017e58",
   "metadata": {},
   "source": [
    "### âš ï¸ Note: Why Some Tags May Be Wrong\n",
    "\n",
    "You may notice that **\"Light the candle\"** tags `light` as `NNP` (proper noun) instead of `VB` (verb). This is a **tagger error** that reveals how POS tagging works under the hood.\n",
    "\n",
    "---\n",
    "\n",
    "## How POS Tagging Works Under the Hood\n",
    "\n",
    "### 1. **Statistical/Probabilistic Approach**\n",
    "\n",
    "NLTK's default tagger (`averaged_perceptron_tagger`) is a **machine learning model** trained on labeled data. It doesn't \"understand\" languageâ€”it makes **statistical predictions** based on patterns learned during training.\n",
    "\n",
    "The tagger calculates:\n",
    "```\n",
    "P(tag | word, context) = probability of a tag given the word and surrounding context\n",
    "```\n",
    "\n",
    "### 2. **Features Used by the Tagger**\n",
    "\n",
    "The perceptron tagger considers multiple **features**:\n",
    "\n",
    "| Feature | Example | Why It Matters |\n",
    "|---------|---------|----------------|\n",
    "| **Current word** | \"light\" | Some words strongly predict certain tags |\n",
    "| **Word suffix** | \"-ing\", \"-ed\", \"-ly\" | Suffixes indicate verb forms, adverbs |\n",
    "| **Word prefix** | \"un-\", \"pre-\" | Can indicate adjectives or verbs |\n",
    "| **Previous tag** | DT â†’ likely NN/JJ next | Tag sequences follow patterns |\n",
    "| **Previous word** | \"the\" before noun | Determiners precede nouns |\n",
    "| **Capitalization** | \"Light\" vs \"light\" | Capitals often indicate proper nouns |\n",
    "| **Word shape** | all caps, mixed case | Helps identify special tokens |\n",
    "\n",
    "### 3. **Why Errors Occur**\n",
    "\n",
    "**Problem 1: Capitalization Bias**\n",
    "- \"Light\" at sentence start is capitalized\n",
    "- Training data has many capitalized proper nouns (names, places)\n",
    "- Tagger may incorrectly associate capitalization with NNP\n",
    "\n",
    "**Problem 2: Ambiguity**\n",
    "- \"light\" can be: noun, verb, or adjective\n",
    "- Without strong contextual signals, the tagger guesses based on training statistics\n",
    "\n",
    "**Problem 3: Training Data Distribution**\n",
    "- If \"light\" appears more often as noun/adjective in training data\n",
    "- The model may be biased toward those tags\n",
    "\n",
    "### 4. **The Tagging Algorithm (Simplified)**\n",
    "\n",
    "```\n",
    "For each word in sentence:\n",
    "    1. Extract features (word, suffix, previous tag, etc.)\n",
    "    2. Calculate score for each possible tag\n",
    "    3. Select tag with highest score\n",
    "    4. Move to next word (using this tag as \"previous tag\")\n",
    "```\n",
    "\n",
    "This is a **greedy left-to-right** approachâ€”early mistakes can cascade!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6f71fe64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effect of Capitalization on POS Tagging\n",
      "=======================================================\n",
      "Sentence                       Word       Tag    Expected\n",
      "-------------------------------------------------------\n",
      "Light the candle.              Light      NNP    VB âœ—\n",
      "light the candle.              light      NN     VB âœ—\n",
      "Please light the candle.       light      VBD    VB âœ—\n"
     ]
    }
   ],
   "source": [
    "# Demonstrating why \"Light\" gets tagged as NNP\n",
    "\n",
    "# Case sensitivity matters!\n",
    "examples = [\n",
    "    \"Light the candle.\",      # Capitalized (sentence start)\n",
    "    \"light the candle.\",      # Lowercase\n",
    "    \"Please light the candle.\", # \"light\" not at start\n",
    "]\n",
    "\n",
    "print(\"Effect of Capitalization on POS Tagging\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"{'Sentence':<30} {'Word':<10} {'Tag':<6} {'Expected'}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for sent in examples:\n",
    "    tokens = word_tokenize(sent)\n",
    "    tagged = pos_tag(tokens)\n",
    "    \n",
    "    # Find 'light' tag\n",
    "    for word, tag in tagged:\n",
    "        if word.lower() == 'light':\n",
    "            expected = 'VB'\n",
    "            status = 'âœ“' if tag == 'VB' else 'âœ—'\n",
    "            print(f\"{sent:<30} {word:<10} {tag:<6} {expected} {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bfbe2c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How Context Influences POS Decisions\n",
      "=================================================================\n",
      "Sentence                       Target   Actual   Expected Match\n",
      "-----------------------------------------------------------------\n",
      "I will light the fire.         light    VB       VB       âœ“\n",
      "The light is dim.              light    NN       NN       âœ“\n",
      "It feels light.                light    NN       JJ       âœ—\n",
      "Turn on the light.             light    NN       NN       âœ“\n"
     ]
    }
   ],
   "source": [
    "# Looking at what features influence tagging decisions\n",
    "# Let's examine how context changes predictions\n",
    "\n",
    "print(\"How Context Influences POS Decisions\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "context_examples = [\n",
    "    # Different preceding words change the prediction\n",
    "    (\"I will light the fire.\", \"light\", \"VB\"),      # Modal 'will' â†’ verb expected\n",
    "    (\"The light is dim.\", \"light\", \"NN\"),           # Determiner 'the' â†’ noun expected  \n",
    "    (\"It feels light.\", \"light\", \"JJ\"),             # 'feels' + adj pattern\n",
    "    (\"Turn on the light.\", \"light\", \"NN\"),          # 'the' + noun pattern\n",
    "]\n",
    "\n",
    "print(f\"{'Sentence':<30} {'Target':<8} {'Actual':<8} {'Expected':<8} {'Match'}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for sent, target, expected in context_examples:\n",
    "    tokens = word_tokenize(sent)\n",
    "    tagged = pos_tag(tokens)\n",
    "    actual = [t for w, t in tagged if w.lower() == target.lower()][0]\n",
    "    match = 'âœ“' if actual == expected else 'âœ—'\n",
    "    print(f\"{sent:<30} {target:<8} {actual:<8} {expected:<8} {match}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17123c8b",
   "metadata": {},
   "source": [
    "### 5. **Understanding Tagger Limitations**\n",
    "\n",
    "| Limitation | Description | Example |\n",
    "|------------|-------------|---------|\n",
    "| **Sentence-initial capitalization** | Words at start look like proper nouns | \"Light the fire\" â†’ Light=NNP âœ— |\n",
    "| **Rare word forms** | Unseen words rely on suffix/shape | Neologisms may be mistagged |\n",
    "| **Garden path sentences** | Ambiguous structure confuses tagger | \"The horse raced past the barn fell\" |\n",
    "| **Domain mismatch** | Trained on news, tested on tweets | Technical jargon may fail |\n",
    "\n",
    "### 6. **Ways to Improve Accuracy**\n",
    "\n",
    "1. **Lowercase sentence-initial words** (if appropriate)\n",
    "2. **Use domain-specific models** for specialized text\n",
    "3. **Combine with other NLP** (NER, parsing) for disambiguation\n",
    "4. **Fine-tune on your data** for better domain accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "81b1edf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Dive: Why 'light' is Hard to Tag Correctly\n",
      "=================================================================\n",
      "Sentence                       Actual   Expected\n",
      "-----------------------------------------------------------------\n",
      "Light the candle.              NNP      VB (imperative verb)\n",
      "light the candle.              NN       VB (imperative verb)\n",
      "Please light the candle.       VBD      VB (verb)\n",
      "Can you light the candle?      VB       VB (verb)\n",
      "I will light the candle.       VB       VB (verb)\n",
      "\n",
      "=================================================================\n",
      "KEY INSIGHT: The tagger struggles with IMPERATIVE sentences!\n",
      "Without a clear subject, it can't identify 'light' as a verb.\n",
      "Adding modal verbs (will, can) provides context â†’ correct VB tag.\n"
     ]
    }
   ],
   "source": [
    "# Examining the tagger's behavior more closely\n",
    "# The perceptron tagger has specific biases we can observe\n",
    "\n",
    "print(\"Deep Dive: Why 'light' is Hard to Tag Correctly\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "problematic = [\n",
    "    (\"Light the candle.\", \"light\", \"VB (imperative verb)\"),\n",
    "    (\"light the candle.\", \"light\", \"VB (imperative verb)\"),\n",
    "    (\"Please light the candle.\", \"light\", \"VB (verb)\"),\n",
    "    (\"Can you light the candle?\", \"light\", \"VB (verb)\"),\n",
    "    (\"I will light the candle.\", \"light\", \"VB (verb)\"),\n",
    "]\n",
    "\n",
    "print(f\"{'Sentence':<30} {'Actual':<8} {'Expected'}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for sent, target, expected in problematic:\n",
    "    tokens = word_tokenize(sent)\n",
    "    tagged = pos_tag(tokens)\n",
    "    actual = [t for w, t in tagged if w.lower() == target][0]\n",
    "    print(f\"{sent:<30} {actual:<8} {expected}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 65)\n",
    "print(\"KEY INSIGHT: The tagger struggles with IMPERATIVE sentences!\")\n",
    "print(\"Without a clear subject, it can't identify 'light' as a verb.\")\n",
    "print(\"Adding modal verbs (will, can) provides context â†’ correct VB tag.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19253a0b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Deep Dive: What the Tagger Actually Looks At\n",
    "\n",
    "### The Chicken-and-Egg Problem\n",
    "\n",
    "You might wonder: *\"If the tagger uses neighboring POS tags, but those neighbors are also ambiguous, how does it decide?\"*\n",
    "\n",
    "**Answer: It processes LEFT-TO-RIGHT and only uses PREVIOUS tags (already decided), never FUTURE tags.**\n",
    "\n",
    "```\n",
    "Sentence:  \"The    light    is    bright\"\n",
    "            â†“       â†“       â†“      â†“\n",
    "Step 1:    DT      ???     ???    ???   (tag \"The\" first)\n",
    "Step 2:    DT  â†’   NN      ???    ???   (use DT to help tag \"light\")\n",
    "Step 3:    DT      NN  â†’   VBZ    ???   (use NN to help tag \"is\")\n",
    "Step 4:    DT      NN      VBZ â†’  JJ    (use VBZ to help tag \"bright\")\n",
    "```\n",
    "\n",
    "This is called **greedy left-to-right decoding**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "34890024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: 'Light the candle'\n",
      "================================================================================\n",
      "\n",
      "Step-by-step feature extraction (LEFT â†’ RIGHT):\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "STEP 1: Tagging word 'Light' at position 0\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸ“‹ FEATURES EXTRACTED:\n",
      "\n",
      "  Word-based features:\n",
      "    word: 'Light'\n",
      "    word.lower(): 'light'\n",
      "    suffix(-3): 'ght'\n",
      "    prefix(3): 'Lig'\n",
      "\n",
      "  Shape features:\n",
      "    is_capitalized: True\n",
      "    is_all_caps: False\n",
      "    is_first_word: True\n",
      "\n",
      "  Context features (ALREADY KNOWN):\n",
      "    prev_word: '<START>'\n",
      "    prev_tag:  '<START>'  â† Already decided!\n",
      "    next_word: 'the' (word known, tag unknown)\n",
      "\n",
      "  ğŸ¯ PREDICTION: 'Light' â†’ NNP\n",
      "     Tags so far: [('Light', 'NNP')]\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "STEP 2: Tagging word 'the' at position 1\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸ“‹ FEATURES EXTRACTED:\n",
      "\n",
      "  Word-based features:\n",
      "    word: 'the'\n",
      "    word.lower(): 'the'\n",
      "    suffix(-3): 'the'\n",
      "    prefix(3): 'the'\n",
      "\n",
      "  Shape features:\n",
      "    is_capitalized: False\n",
      "    is_all_caps: False\n",
      "    is_first_word: False\n",
      "\n",
      "  Context features (ALREADY KNOWN):\n",
      "    prev_word: 'Light'\n",
      "    prev_tag:  'NNP'  â† Already decided!\n",
      "    next_word: 'candle' (word known, tag unknown)\n",
      "\n",
      "  ğŸ¯ PREDICTION: 'the' â†’ DT\n",
      "     Tags so far: [('Light', 'NNP'), ('the', 'DT')]\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "STEP 3: Tagging word 'candle' at position 2\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸ“‹ FEATURES EXTRACTED:\n",
      "\n",
      "  Word-based features:\n",
      "    word: 'candle'\n",
      "    word.lower(): 'candle'\n",
      "    suffix(-3): 'dle'\n",
      "    prefix(3): 'can'\n",
      "\n",
      "  Shape features:\n",
      "    is_capitalized: False\n",
      "    is_all_caps: False\n",
      "    is_first_word: False\n",
      "\n",
      "  Context features (ALREADY KNOWN):\n",
      "    prev_word: 'the'\n",
      "    prev_tag:  'DT'  â† Already decided!\n",
      "    next_word: '<END>' (word known, tag unknown)\n",
      "\n",
      "  ğŸ¯ PREDICTION: 'candle' â†’ NN\n",
      "     Tags so far: [('Light', 'NNP'), ('the', 'DT'), ('candle', 'NN')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Light', 'NNP'), ('the', 'DT'), ('candle', 'NN')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's visualize exactly what features the tagger extracts for each word\n",
    "# This simulates the actual feature extraction process\n",
    "\n",
    "def show_tagger_features(sentence):\n",
    "    \"\"\"\n",
    "    Simulate the features that NLTK's averaged perceptron tagger extracts.\n",
    "    Based on the actual implementation in nltk/tag/perceptron.py\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(sentence)\n",
    "    \n",
    "    print(f\"Sentence: '{sentence}'\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\nStep-by-step feature extraction (LEFT â†’ RIGHT):\\n\")\n",
    "    \n",
    "    # Simulate left-to-right tagging\n",
    "    predicted_tags = []\n",
    "    \n",
    "    for i, word in enumerate(tokens):\n",
    "        print(f\"{'â”€' * 80}\")\n",
    "        print(f\"STEP {i+1}: Tagging word '{word}' at position {i}\")\n",
    "        print(f\"{'â”€' * 80}\")\n",
    "        \n",
    "        # Features the tagger actually uses:\n",
    "        features = {}\n",
    "        \n",
    "        # 1. Word features\n",
    "        features['word'] = word\n",
    "        features['word.lower()'] = word.lower()\n",
    "        features['suffix(-3)'] = word[-3:] if len(word) >= 3 else word\n",
    "        features['suffix(-2)'] = word[-2:] if len(word) >= 2 else word\n",
    "        features['suffix(-1)'] = word[-1:] if len(word) >= 1 else word\n",
    "        features['prefix(1)'] = word[0] if len(word) >= 1 else ''\n",
    "        features['prefix(2)'] = word[:2] if len(word) >= 2 else word\n",
    "        features['prefix(3)'] = word[:3] if len(word) >= 3 else word\n",
    "        \n",
    "        # 2. Word shape features\n",
    "        features['is_capitalized'] = word[0].isupper() if word else False\n",
    "        features['is_all_caps'] = word.isupper()\n",
    "        features['is_all_lower'] = word.islower()\n",
    "        features['has_digit'] = any(c.isdigit() for c in word)\n",
    "        features['has_hyphen'] = '-' in word\n",
    "        \n",
    "        # 3. Position features\n",
    "        features['is_first_word'] = (i == 0)\n",
    "        features['is_last_word'] = (i == len(tokens) - 1)\n",
    "        \n",
    "        # 4. Context features (PREVIOUS words - already known!)\n",
    "        features['prev_word'] = tokens[i-1] if i > 0 else '<START>'\n",
    "        features['prev_word(-2)'] = tokens[i-2] if i > 1 else '<START>'\n",
    "        \n",
    "        # 5. PREVIOUS TAG features (already decided in earlier steps!)\n",
    "        features['prev_tag'] = predicted_tags[i-1] if i > 0 else '<START>'\n",
    "        features['prev_tag(-2)'] = predicted_tags[i-2] if i > 1 else '<START>'\n",
    "        \n",
    "        # 6. Next word (lookahead - words are known, but NOT their tags!)\n",
    "        features['next_word'] = tokens[i+1] if i < len(tokens)-1 else '<END>'\n",
    "        \n",
    "        # Print features\n",
    "        print(\"\\nğŸ“‹ FEATURES EXTRACTED:\")\n",
    "        print(\"\\n  Word-based features:\")\n",
    "        for k in ['word', 'word.lower()', 'suffix(-3)', 'prefix(3)']:\n",
    "            print(f\"    {k}: '{features[k]}'\")\n",
    "        \n",
    "        print(\"\\n  Shape features:\")\n",
    "        for k in ['is_capitalized', 'is_all_caps', 'is_first_word']:\n",
    "            print(f\"    {k}: {features[k]}\")\n",
    "        \n",
    "        print(\"\\n  Context features (ALREADY KNOWN):\")\n",
    "        print(f\"    prev_word: '{features['prev_word']}'\")\n",
    "        print(f\"    prev_tag:  '{features['prev_tag']}'  â† Already decided!\")\n",
    "        print(f\"    next_word: '{features['next_word']}' (word known, tag unknown)\")\n",
    "        \n",
    "        # Get actual prediction\n",
    "        actual_tags = pos_tag(tokens)\n",
    "        current_tag = actual_tags[i][1]\n",
    "        predicted_tags.append(current_tag)\n",
    "        \n",
    "        print(f\"\\n  ğŸ¯ PREDICTION: '{word}' â†’ {current_tag}\")\n",
    "        print(f\"     Tags so far: {list(zip(tokens[:i+1], predicted_tags))}\")\n",
    "    \n",
    "    return list(zip(tokens, predicted_tags))\n",
    "\n",
    "# Run the visualization\n",
    "show_tagger_features(\"Light the candle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c93e87fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How Previous Tags Cascade Through the Sentence\n",
      "======================================================================\n",
      "\n",
      "'The light is bright'\n",
      "--------------------------------------------------\n",
      "Step   Word       Prev Tag     â†’ Current Tag\n",
      "--------------------------------------------------\n",
      "1      The        <START>      â†’ DT\n",
      "2      light      DT           â†’ NN\n",
      "3      is         NN           â†’ VBZ\n",
      "4      bright     VBZ          â†’ JJ\n",
      "\n",
      "'I will light the candle'\n",
      "--------------------------------------------------\n",
      "Step   Word       Prev Tag     â†’ Current Tag\n",
      "--------------------------------------------------\n",
      "1      I          <START>      â†’ PRP\n",
      "2      will       PRP          â†’ MD\n",
      "3      light      MD           â†’ VB\n",
      "4      the        VB           â†’ DT\n",
      "5      candle     DT           â†’ NN\n",
      "\n",
      "======================================================================\n",
      "NOTICE: Each tag decision uses the PREVIOUS tag as a feature!\n",
      "The tagger never looks at FUTURE tags (they don't exist yet).\n"
     ]
    }
   ],
   "source": [
    "# Compare how PREVIOUS TAG influences decisions\n",
    "print(\"How Previous Tags Cascade Through the Sentence\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "examples = [\n",
    "    \"The light is bright\",      # DT â†’ NN â†’ VBZ â†’ JJ\n",
    "    \"I will light the candle\",  # PRP â†’ MD â†’ VB â†’ DT â†’ NN\n",
    "]\n",
    "\n",
    "for sent in examples:\n",
    "    tokens = word_tokenize(sent)\n",
    "    tagged = pos_tag(tokens)\n",
    "    \n",
    "    print(f\"\\n'{sent}'\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'Step':<6} {'Word':<10} {'Prev Tag':<12} {'â†’ Current Tag'}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for i, (word, tag) in enumerate(tagged):\n",
    "        prev_tag = tagged[i-1][1] if i > 0 else '<START>'\n",
    "        print(f\"{i+1:<6} {word:<10} {prev_tag:<12} â†’ {tag}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"NOTICE: Each tag decision uses the PREVIOUS tag as a feature!\")\n",
    "print(\"The tagger never looks at FUTURE tags (they don't exist yet).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480c5a4a",
   "metadata": {},
   "source": [
    "### Complete Feature List Used by NLTK's Averaged Perceptron Tagger\n",
    "\n",
    "The tagger extracts these features for each word at position `i`:\n",
    "\n",
    "| Category | Feature | Description | Example for \"light\" |\n",
    "|----------|---------|-------------|---------------------|\n",
    "| **Word** | `word` | The exact word | \"light\" |\n",
    "| | `word.lower()` | Lowercased | \"light\" |\n",
    "| | `suffix(-1,-2,-3)` | Last 1/2/3 chars | \"t\", \"ht\", \"ght\" |\n",
    "| | `prefix(1,2,3)` | First 1/2/3 chars | \"l\", \"li\", \"lig\" |\n",
    "| **Shape** | `is_upper` | Starts uppercase? | True (if \"Light\") |\n",
    "| | `is_title` | Title case? | True |\n",
    "| | `is_digit` | Contains digits? | False |\n",
    "| | `has_hyphen` | Contains hyphen? | False |\n",
    "| **Position** | `i == 0` | First word? | Depends |\n",
    "| | `i == len-1` | Last word? | Depends |\n",
    "| **Context** | `word[i-1]` | Previous word | \"The\" or \"will\" |\n",
    "| | `word[i-2]` | Word 2 back | varies |\n",
    "| | `word[i+1]` | Next word | \"the\" |\n",
    "| **Tags** | `tag[i-1]` | Previous tag | DT or MD |\n",
    "| | `tag[i-2]` | Tag 2 back | varies |\n",
    "| | `tag[i-1] + tag[i-2]` | Tag bigram | \"MD+PRP\" |\n",
    "\n",
    "### âš ï¸ Key Insight: NO FUTURE TAGS!\n",
    "\n",
    "Notice there's **no `tag[i+1]`** (next tag) feature because:\n",
    "- Tags are assigned **left-to-right**\n",
    "- Future tags **don't exist** when deciding current tag\n",
    "- Only previous tags (already decided) can be used\n",
    "\n",
    "This is why **early mistakes cascade**â€”a wrong tag at position 0 affects all subsequent predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bb2b0fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Cascading Demo\n",
      "======================================================================\n",
      "\n",
      "When the first word is mistagged, it can affect the entire sentence.\n",
      "\n",
      "Common Tag Transition Patterns (from training data):\n",
      "--------------------------------------------------\n",
      "If prev_tag is...    Next tag likely...\n",
      "--------------------------------------------------\n",
      "<START>              ['DT', 'PRP', 'NNP', 'RB', 'VB']\n",
      "DT                   ['NN', 'JJ', 'NNP', 'RB']\n",
      "MD                   ['VB', 'RB']\n",
      "PRP                  ['VBP', 'VBD', 'MD', 'VBZ']\n",
      "NNP                  ['NNP', 'VBZ', 'VBD', ',']\n",
      "JJ                   ['NN', 'NNS', 'JJ', 'CC']\n",
      "VB                   ['DT', 'PRP', 'RB', 'TO']\n",
      "NN                   ['VBZ', 'VBD', 'IN', 'CC', '.']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Now see how 'Light the candle' gets mistagged:\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Step 1: Tag 'Light'\n",
      "   - Position: 0 (first word)\n",
      "   - prev_tag: <START>\n",
      "   - Features: capitalized=True, word='Light'\n",
      "   - Problem: Capitalization + sentence-start â†’ model guesses NNP\n",
      "   - Result: 'Light' â†’ NNP âŒ (should be VB)\n",
      "\n",
      "Step 2: Tag 'the'\n",
      "   - prev_tag: NNP (wrong, but tagger doesn't know!)\n",
      "   - After NNP, model expects: more nouns, verbs, punctuation\n",
      "   - Result: 'the' â†’ DT âœ“ (still correct, \"the\" is unambiguous)\n",
      "\n",
      "Step 3: Tag 'candle'\n",
      "   - prev_tag: DT\n",
      "   - After DT, model expects: noun or adjective\n",
      "   - Result: 'candle' â†’ NN âœ“\n",
      "\n",
      "Sentence tagged as: NNP DT NN (instead of VB DT NN)\n",
      "The error at position 0 cascaded but luckily didn't break everything!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate error cascading: early mistakes affect later predictions\n",
    "print(\"Error Cascading Demo\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nWhen the first word is mistagged, it can affect the entire sentence.\\n\")\n",
    "\n",
    "# Simulate what happens with different \"fake\" previous tags\n",
    "def simulate_tag_influence():\n",
    "    \"\"\"\n",
    "    Show how previous tag patterns influence predictions.\n",
    "    Common tag transition patterns learned from training data:\n",
    "    \"\"\"\n",
    "    transitions = {\n",
    "        # Previous tag â†’ likely current tags (from training statistics)\n",
    "        '<START>': ['DT', 'PRP', 'NNP', 'RB', 'VB'],\n",
    "        'DT': ['NN', 'JJ', 'NNP', 'RB'],      # After \"the\" â†’ noun/adj likely\n",
    "        'MD': ['VB', 'RB'],                    # After \"will/can\" â†’ verb likely\n",
    "        'PRP': ['VBP', 'VBD', 'MD', 'VBZ'],   # After \"I/you\" â†’ verb likely\n",
    "        'NNP': ['NNP', 'VBZ', 'VBD', ','],    # After proper noun â†’ verb/more nouns\n",
    "        'JJ': ['NN', 'NNS', 'JJ', 'CC'],      # After adjective â†’ noun likely\n",
    "        'VB': ['DT', 'PRP', 'RB', 'TO'],      # After verb â†’ object/adverb\n",
    "        'NN': ['VBZ', 'VBD', 'IN', 'CC', '.'], # After noun â†’ verb/prep\n",
    "    }\n",
    "    \n",
    "    print(\"Common Tag Transition Patterns (from training data):\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'If prev_tag is...':<20} {'Next tag likely...'}\")\n",
    "    print(\"-\" * 50)\n",
    "    for prev, nexts in transitions.items():\n",
    "        print(f\"{prev:<20} {nexts}\")\n",
    "    \n",
    "    return transitions\n",
    "\n",
    "transitions = simulate_tag_influence()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"\\nNow see how 'Light the candle' gets mistagged:\")\n",
    "print(\"-\" * 70)\n",
    "print(\"\"\"\n",
    "Step 1: Tag 'Light'\n",
    "   - Position: 0 (first word)\n",
    "   - prev_tag: <START>\n",
    "   - Features: capitalized=True, word='Light'\n",
    "   - Problem: Capitalization + sentence-start â†’ model guesses NNP\n",
    "   - Result: 'Light' â†’ NNP âŒ (should be VB)\n",
    "\n",
    "Step 2: Tag 'the'\n",
    "   - prev_tag: NNP (wrong, but tagger doesn't know!)\n",
    "   - After NNP, model expects: more nouns, verbs, punctuation\n",
    "   - Result: 'the' â†’ DT âœ“ (still correct, \"the\" is unambiguous)\n",
    "\n",
    "Step 3: Tag 'candle'\n",
    "   - prev_tag: DT\n",
    "   - After DT, model expects: noun or adjective\n",
    "   - Result: 'candle' â†’ NN âœ“\n",
    "\n",
    "Sentence tagged as: NNP DT NN (instead of VB DT NN)\n",
    "The error at position 0 cascaded but luckily didn't break everything!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f929e2b8",
   "metadata": {},
   "source": [
    "### Alternative Approaches: How Other Taggers Solve This\n",
    "\n",
    "| Approach | How it works | Pros | Cons |\n",
    "|----------|--------------|------|------|\n",
    "| **Greedy Lâ†’R** (NLTK default) | Tag one word at a time, use previous tags | Fast, simple | Errors cascade |\n",
    "| **Viterbi/HMM** | Find globally optimal tag sequence | Better accuracy | Slower, limited features |\n",
    "| **Bidirectional LSTM** | Neural network sees full sentence | State-of-the-art | Requires GPU, large model |\n",
    "| **Transformer (BERT)** | Attention over entire sentence | Best accuracy | Slow, resource-heavy |\n",
    "\n",
    "NLTK's perceptron tagger uses the **greedy approach** for speed, accepting some accuracy loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904e5f2a",
   "metadata": {},
   "source": [
    "## 7.6 Extracting Words by POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63b3c9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: The beautiful princess quickly ran through the dark forest.\n",
      "She was searching for her magical golden crown.\n",
      "\n",
      "Nouns: ['princess', 'forest', 'crown']\n",
      "Verbs: ['ran', 'was', 'searching']\n",
      "Adjectives: ['beautiful', 'dark', 'magical', 'golden']\n",
      "Adverbs: ['quickly']\n"
     ]
    }
   ],
   "source": [
    "def extract_by_pos(text, target_tags):\n",
    "    \"\"\"Extract words with specific POS tags\"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged = pos_tag(tokens)\n",
    "    return [word for word, tag in tagged if tag in target_tags]\n",
    "\n",
    "text = \"\"\"The beautiful princess quickly ran through the dark forest.\n",
    "She was searching for her magical golden crown.\"\"\"\n",
    "\n",
    "print(f\"Text: {text}\")\n",
    "\n",
    "# Extract different parts of speech\n",
    "nouns = extract_by_pos(text, ['NN', 'NNS', 'NNP', 'NNPS'])\n",
    "verbs = extract_by_pos(text, ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'])\n",
    "adjectives = extract_by_pos(text, ['JJ', 'JJR', 'JJS'])\n",
    "adverbs = extract_by_pos(text, ['RB', 'RBR', 'RBS'])\n",
    "\n",
    "print(f\"\\nNouns: {nouns}\")\n",
    "print(f\"Verbs: {verbs}\")\n",
    "print(f\"Adjectives: {adjectives}\")\n",
    "print(f\"Adverbs: {adverbs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b54b7883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tag Distribution\n",
      "==============================\n",
      "NN       7 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "JJ       3 â–ˆâ–ˆâ–ˆ\n",
      "VBG      2 â–ˆâ–ˆ\n",
      "NNS      2 â–ˆâ–ˆ\n",
      "VBP      2 â–ˆâ–ˆ\n",
      "CC       2 â–ˆâ–ˆ\n",
      ".        2 â–ˆâ–ˆ\n",
      "VBZ      1 â–ˆ\n",
      "WRB      1 â–ˆ\n",
      "RB       1 â–ˆ\n"
     ]
    }
   ],
   "source": [
    "# POS distribution\n",
    "from collections import Counter\n",
    "\n",
    "def pos_distribution(text):\n",
    "    \"\"\"Get distribution of POS tags\"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged = pos_tag(tokens)\n",
    "    return Counter(tag for word, tag in tagged)\n",
    "\n",
    "text = \"\"\"Machine learning is transforming how computers understand and process \n",
    "human language. Natural language processing applications are becoming \n",
    "increasingly sophisticated and accurate.\"\"\"\n",
    "\n",
    "dist = pos_distribution(text)\n",
    "\n",
    "print(\"POS Tag Distribution\")\n",
    "print(\"=\" * 30)\n",
    "for tag, count in dist.most_common():\n",
    "    print(f\"{tag:<6} {count:>3} {'â–ˆ' * count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6038703",
   "metadata": {},
   "source": [
    "## 7.7 Custom POS Taggers\n",
    "\n",
    "NLTK provides several tagger classes that you can train on your own data. Understanding these helps you:\n",
    "1. **Customize** taggers for specific domains (medical, legal, social media)\n",
    "2. **Understand** how statistical tagging works under the hood\n",
    "3. **Improve** accuracy by combining multiple approaches\n",
    "\n",
    "### Overview: The Four N-gram Taggers\n",
    "\n",
    "| Tagger | What It Looks At | Example |\n",
    "|--------|------------------|---------|\n",
    "| **DefaultTagger** | Nothing â€” returns fixed tag | Any word â†’ `NN` |\n",
    "| **UnigramTagger** | Current word only | `\"running\"` â†’ `VBG` |\n",
    "| **BigramTagger** | Previous tag + current word | `(DT, \"light\")` â†’ `NN` |\n",
    "| **TrigramTagger** | Two previous tags + word | `(PRP, MD, \"light\")` â†’ `VB` |\n",
    "\n",
    "### The Backoff Chain\n",
    "\n",
    "When a tagger encounters something it hasn't seen, it asks a **simpler tagger** for help:\n",
    "\n",
    "```\n",
    "TrigramTagger: \"I don't know (PRP, VBZ, 'xyzzy')\"\n",
    "       â†“ ask backoff\n",
    "BigramTagger: \"I don't know (VBZ, 'xyzzy') either\"  \n",
    "       â†“ ask backoff\n",
    "UnigramTagger: \"Never seen 'xyzzy'\"\n",
    "       â†“ ask backoff\n",
    "DefaultTagger: \"I'll just say NN\"\n",
    "       â†‘ returns 'NN'\n",
    "```\n",
    "\n",
    "**Build order:** Simple â†’ Complex (each uses the previous as backoff)\n",
    "```python\n",
    "default  = DefaultTagger('NN')\n",
    "unigram  = UnigramTagger(train, backoff=default)   # falls back to default\n",
    "bigram   = BigramTagger(train, backoff=unigram)    # falls back to unigram  \n",
    "trigram  = TrigramTagger(train, backoff=bigram)    # falls back to bigram\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49abc756",
   "metadata": {},
   "source": [
    "### 7.7.1 Preparing Training Data\n",
    "\n",
    "We'll use the **Brown Corpus** (news category) to train our custom taggers. The data format is:\n",
    "```python\n",
    "[\n",
    "    [('word1', 'TAG1'), ('word2', 'TAG2'), ...],  # sentence 1\n",
    "    [('word1', 'TAG1'), ('word2', 'TAG2'), ...],  # sentence 2\n",
    "    ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0bb5df11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sentences: 3698\n",
      "Test sentences: 925\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import DefaultTagger, UnigramTagger, BigramTagger, TrigramTagger\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# Get tagged sentences from Brown corpus\n",
    "brown_tagged = brown.tagged_sents(categories='news')\n",
    "\n",
    "# Split into train/test\n",
    "train_size = int(len(brown_tagged) * 0.8)\n",
    "train_sents = brown_tagged[:train_size]\n",
    "test_sents = brown_tagged[train_size:]\n",
    "\n",
    "print(f\"Training sentences: {len(train_sents)}\")\n",
    "print(f\"Test sentences: {len(test_sents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ea303f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PEEK INTO TRAINING DATA\n",
      "======================================================================\n",
      "\n",
      "Training set: 3698 sentences\n",
      "Test set: 925 sentences\n",
      "\n",
      "Sample Training Sentence #1:\n",
      "--------------------------------------------------\n",
      "[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ('Grand', 'JJ-TL'), ('Jury', 'NN-TL'), ('said', 'VBD'), ('Friday', 'NR'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'IN'), (\"Atlanta's\", 'NP$'), ('recent', 'JJ'), ('primary', 'NN'), ('election', 'NN'), ('produced', 'VBD'), ('``', '``'), ('no', 'AT'), ('evidence', 'NN'), (\"''\", \"''\"), ('that', 'CS'), ('any', 'DTI'), ('irregularities', 'NNS'), ('took', 'VBD'), ('place', 'NN'), ('.', '.')]\n",
      "\n",
      "Formatted view:\n",
      "  'The' â†’ AT\n",
      "  'Fulton' â†’ NP-TL\n",
      "  'County' â†’ NN-TL\n",
      "  'Grand' â†’ JJ-TL\n",
      "  'Jury' â†’ NN-TL\n",
      "  'said' â†’ VBD\n",
      "  'Friday' â†’ NR\n",
      "  'an' â†’ AT\n",
      "  'investigation' â†’ NN\n",
      "  'of' â†’ IN\n",
      "  ... (15 more words)\n",
      "\n",
      "--------------------------------------------------\n",
      "Sample Training Sentence #2:\n",
      "--------------------------------------------------\n",
      "[('The', 'AT'), ('jury', 'NN'), ('further', 'RBR'), ('said', 'VBD'), ('in', 'IN'), ('term-end', 'NN'), ('presentments', 'NNS'), ('that', 'CS'), ('the', 'AT'), ('City', 'NN-TL'), ('Executive', 'JJ-TL'), ('Committee', 'NN-TL'), (',', ','), ('which', 'WDT'), ('had', 'HVD'), ('over-all', 'JJ'), ('charge', 'NN'), ('of', 'IN'), ('the', 'AT'), ('election', 'NN'), (',', ','), ('``', '``'), ('deserves', 'VBZ'), ('the', 'AT'), ('praise', 'NN'), ('and', 'CC'), ('thanks', 'NNS'), ('of', 'IN'), ('the', 'AT'), ('City', 'NN-TL'), ('of', 'IN-TL'), ('Atlanta', 'NP-TL'), (\"''\", \"''\"), ('for', 'IN'), ('the', 'AT'), ('manner', 'NN'), ('in', 'IN'), ('which', 'WDT'), ('the', 'AT'), ('election', 'NN'), ('was', 'BEDZ'), ('conducted', 'VBN'), ('.', '.')]\n",
      "\n",
      "Formatted view:\n",
      "  'The' â†’ AT\n",
      "  'jury' â†’ NN\n",
      "  'further' â†’ RBR\n",
      "  'said' â†’ VBD\n",
      "  'in' â†’ IN\n",
      "  'term-end' â†’ NN\n",
      "  'presentments' â†’ NNS\n",
      "  'that' â†’ CS\n",
      "  'the' â†’ AT\n",
      "  'City' â†’ NN-TL\n"
     ]
    }
   ],
   "source": [
    "# Peek into the training data\n",
    "print(\"=\" * 70)\n",
    "print(\"PEEK INTO TRAINING DATA\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nTraining set: {len(train_sents)} sentences\")\n",
    "print(f\"Test set: {len(test_sents)} sentences\\n\")\n",
    "\n",
    "print(\"Sample Training Sentence #1:\")\n",
    "print(\"-\" * 50)\n",
    "print(train_sents[0])\n",
    "print(\"\\nFormatted view:\")\n",
    "for word, tag in train_sents[0][:10]:  # First 10 words\n",
    "    print(f\"  '{word}' â†’ {tag}\")\n",
    "if len(train_sents[0]) > 10:\n",
    "    print(f\"  ... ({len(train_sents[0]) - 10} more words)\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "print(\"Sample Training Sentence #2:\")\n",
    "print(\"-\" * 50)\n",
    "print(train_sents[1])\n",
    "print(\"\\nFormatted view:\")\n",
    "for word, tag in train_sents[1][:10]:\n",
    "    print(f\"  '{word}' â†’ {tag}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866659ae",
   "metadata": {},
   "source": [
    "### 7.7.2 Training and Comparing All Taggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6b651745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PEEK INTO TEST DATA: Predictions vs Gold Standard\n",
      "======================================================================\n",
      "\n",
      "Test Sentence #6:\n",
      "  'It is now disclosed that the taxpayer not only pays for high wages , but he pays the employers' strike expenses when the latter undertakes to fight a strike .'\n",
      "\n",
      "Word            Gold     Default  Unigram  Bigram   Trigram \n",
      "---------------------------------------------------------------------------\n",
      "It              PPS      NN    âœ— PPS   âœ“ PPS   âœ“ PPS   âœ“\n",
      "is              BEZ      NN    âœ— BEZ   âœ“ BEZ   âœ“ BEZ   âœ“\n",
      "now             RB       NN    âœ— RB    âœ“ RB    âœ“ RB    âœ“\n",
      "disclosed       VBN      NN    âœ— VBD   âœ— VBD   âœ— VBD   âœ—\n",
      "that            CS       NN    âœ— CS    âœ“ CS    âœ“ CS    âœ“\n",
      "the             AT       NN    âœ— AT    âœ“ AT    âœ“ AT    âœ“\n",
      "taxpayer        NN       NN    âœ“ NN    âœ“ NN    âœ“ NN    âœ“\n",
      "not             *        NN    âœ— *     âœ“ *     âœ“ *     âœ“\n",
      "only            RB       NN    âœ— AP    âœ— RB    âœ“ RB    âœ“\n",
      "pays            VBZ      NN    âœ— VBZ   âœ“ VBZ   âœ“ VBZ   âœ“\n",
      "for             IN       NN    âœ— IN    âœ“ IN    âœ“ IN    âœ“\n",
      "high            JJ       NN    âœ— JJ    âœ“ JJ    âœ“ JJ    âœ“\n",
      "wages           NNS      NN    âœ— NNS   âœ“ NNS   âœ“ NNS   âœ“\n",
      ",               ,        NN    âœ— ,     âœ“ ,     âœ“ ,     âœ“\n",
      "but             CC       NN    âœ— CC    âœ“ CC    âœ“ CC    âœ“\n",
      "he              PPS      NN    âœ— PPS   âœ“ PPS   âœ“ PPS   âœ“\n",
      "pays            VBZ      NN    âœ— VBZ   âœ“ VBZ   âœ“ VBZ   âœ“\n",
      "the             AT       NN    âœ— AT    âœ“ AT    âœ“ AT    âœ“\n",
      "employers'      NNS$     NN    âœ— NN    âœ— NN    âœ— NN    âœ—\n",
      "strike          NN       NN    âœ“ NN    âœ“ NN    âœ“ NN    âœ“\n",
      "expenses        NNS      NN    âœ— NNS   âœ“ NNS   âœ“ NNS   âœ“\n",
      "when            WRB      NN    âœ— WRB   âœ“ WRB   âœ“ WRB   âœ“\n",
      "the             AT       NN    âœ— AT    âœ“ AT    âœ“ AT    âœ“\n",
      "latter          AP       NN    âœ— AP    âœ“ AP    âœ“ AP    âœ“\n",
      "undertakes      VBZ      NN    âœ— NN    âœ— NN    âœ— NN    âœ—\n",
      "to              TO       NN    âœ— TO    âœ“ TO    âœ“ TO    âœ“\n",
      "fight           VB       NN    âœ— NN    âœ— NN    âœ— NN    âœ—\n",
      "a               AT       NN    âœ— AT    âœ“ AT    âœ“ AT    âœ“\n",
      "strike          NN       NN    âœ“ NN    âœ“ NN    âœ“ NN    âœ“\n",
      ".               .        NN    âœ— .     âœ“ .     âœ“ .     âœ“\n"
     ]
    }
   ],
   "source": [
    "# Peek into the TEST data and see predictions vs gold standard\n",
    "print(\"=\" * 70)\n",
    "print(\"PEEK INTO TEST DATA: Predictions vs Gold Standard\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# First, train all taggers\n",
    "default_tagger = DefaultTagger('NN')\n",
    "unigram_tagger = UnigramTagger(train_sents, backoff=default_tagger)\n",
    "bigram_tagger = BigramTagger(train_sents, backoff=unigram_tagger)\n",
    "trigram_tagger = TrigramTagger(train_sents, backoff=bigram_tagger)\n",
    "\n",
    "# Pick a test sentence\n",
    "test_sent = test_sents[5]  # Pick sentence #6 from test set\n",
    "words = [word for word, tag in test_sent]\n",
    "gold_tags = [tag for word, tag in test_sent]\n",
    "\n",
    "# Get predictions from each tagger\n",
    "predictions = {\n",
    "    'Default': default_tagger.tag(words),\n",
    "    'Unigram': unigram_tagger.tag(words),\n",
    "    'Bigram': bigram_tagger.tag(words),\n",
    "    'Trigram': trigram_tagger.tag(words),\n",
    "}\n",
    "\n",
    "print(f\"\\nTest Sentence #{6}:\")\n",
    "print(f\"  '{' '.join(words)}'\\n\")\n",
    "\n",
    "print(f\"{'Word':<15} {'Gold':<8} {'Default':<8} {'Unigram':<8} {'Bigram':<8} {'Trigram':<8}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for i, (word, gold) in enumerate(test_sent):\n",
    "    def_tag = predictions['Default'][i][1]\n",
    "    uni_tag = predictions['Unigram'][i][1]\n",
    "    bi_tag = predictions['Bigram'][i][1]\n",
    "    tri_tag = predictions['Trigram'][i][1]\n",
    "    \n",
    "    # Mark correct predictions with âœ“\n",
    "    def_mark = 'âœ“' if def_tag == gold else 'âœ—'\n",
    "    uni_mark = 'âœ“' if uni_tag == gold else 'âœ—'\n",
    "    bi_mark = 'âœ“' if bi_tag == gold else 'âœ—'\n",
    "    tri_mark = 'âœ“' if tri_tag == gold else 'âœ—'\n",
    "    \n",
    "    print(f\"{word:<15} {gold:<8} {def_tag:<6}{def_mark} {uni_tag:<6}{uni_mark} {bi_tag:<6}{bi_mark} {tri_tag:<6}{tri_mark}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a5d906",
   "metadata": {},
   "source": [
    "### 7.7.3 Error Analysis: Where Taggers Fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b5de5e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ERROR ANALYSIS: Where Taggers Go Wrong\n",
      "======================================================================\n",
      "\n",
      "Error counts (first 50 test sentences):\n",
      "  Default: 1137 errors\n",
      "  Unigram: 224 errors\n",
      "  Bigram: 217 errors\n",
      "  Trigram: 218 errors\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Sample Errors from TrigramTagger (our best tagger):\n",
      "----------------------------------------------------------------------\n",
      "Word            Gold     Predicted Context\n",
      "----------------------------------------------------------------------\n",
      "fallacious      JJ       NN       ...more the fallacious equation is...\n",
      "argue           VB       NN       ...advanced to argue that since...\n",
      "since           CS       IN       ...argue that since business is...\n",
      "restricted      VBN      NN       ...business is restricted under the...\n",
      "Or              CC       NN       ...Or , in...\n",
      "Anatole         NP       NN       ...words of Anatole France ,...\n",
      "The             AT       AT-TL    ..., `` The law in...\n",
      "forbid          VB       NN       ...equality must forbid the rich...\n",
      "as              QL       CS       ...rich , as well as...\n",
      "begging         VBG      NN       ..., from begging in the...\n"
     ]
    }
   ],
   "source": [
    "# Let's examine where taggers DISAGREE and make ERRORS\n",
    "print(\"=\" * 70)\n",
    "print(\"ERROR ANALYSIS: Where Taggers Go Wrong\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Analyze errors across multiple test sentences\n",
    "errors_by_tagger = {'Default': [], 'Unigram': [], 'Bigram': [], 'Trigram': []}\n",
    "\n",
    "for sent in test_sents[:50]:  # Check first 50 test sentences\n",
    "    words = [w for w, t in sent]\n",
    "    gold = [t for w, t in sent]\n",
    "    \n",
    "    preds = {\n",
    "        'Default': [t for w, t in default_tagger.tag(words)],\n",
    "        'Unigram': [t for w, t in unigram_tagger.tag(words)],\n",
    "        'Bigram': [t for w, t in bigram_tagger.tag(words)],\n",
    "        'Trigram': [t for w, t in trigram_tagger.tag(words)],\n",
    "    }\n",
    "    \n",
    "    for i, (word, gold_tag) in enumerate(sent):\n",
    "        for tagger_name, pred_tags in preds.items():\n",
    "            if pred_tags[i] != gold_tag:\n",
    "                errors_by_tagger[tagger_name].append({\n",
    "                    'word': word,\n",
    "                    'gold': gold_tag,\n",
    "                    'predicted': pred_tags[i],\n",
    "                    'context': ' '.join(words[max(0,i-2):i+3])\n",
    "                })\n",
    "\n",
    "print(\"\\nError counts (first 50 test sentences):\")\n",
    "for name, errors in errors_by_tagger.items():\n",
    "    print(f\"  {name}: {len(errors)} errors\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"Sample Errors from TrigramTagger (our best tagger):\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Word':<15} {'Gold':<8} {'Predicted':<8} {'Context'}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for err in errors_by_tagger['Trigram'][:10]:\n",
    "    print(f\"{err['word']:<15} {err['gold']:<8} {err['predicted']:<8} ...{err['context']}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "57f345f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CONFUSION ANALYSIS: Most Common Mistakes\n",
      "======================================================================\n",
      "\n",
      "Top 15 Most Common Tag Confusions (TrigramTagger):\n",
      "--------------------------------------------------\n",
      "Gold Tag     Predicted    Count    Explanation\n",
      "--------------------------------------------------\n",
      "NP           NN           376      \n",
      "JJ           NN           362      Adjective vs noun\n",
      "NNS          NN           361      \n",
      "VB           NN           169      Verb vs noun (ambiguous)\n",
      "VBG          NN           111      \n",
      "VBD          NN           107      \n",
      "IN           TO           107      \n",
      "VBN          NN           104      \n",
      "VBD          VBN          99       Past tense vs past participle\n",
      "NN-TL        NN           96       \n",
      "VBN          VBD          83       Past participle vs past tense\n",
      "TO           IN           83       \n",
      "CD           NN           83       \n",
      "RB           NN           79       \n",
      "NP$          NN           47       \n"
     ]
    }
   ],
   "source": [
    "# Confusion matrix: What tags get confused with what?\n",
    "from collections import Counter\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"CONFUSION ANALYSIS: Most Common Mistakes\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Collect all (gold, predicted) pairs for trigram tagger\n",
    "confusions = []\n",
    "for sent in test_sents:\n",
    "    words = [w for w, t in sent]\n",
    "    gold_tags = [t for w, t in sent]\n",
    "    pred_tags = [t for w, t in trigram_tagger.tag(words)]\n",
    "    \n",
    "    for gold, pred in zip(gold_tags, pred_tags):\n",
    "        if gold != pred:\n",
    "            confusions.append((gold, pred))\n",
    "\n",
    "confusion_counts = Counter(confusions)\n",
    "\n",
    "print(\"\\nTop 15 Most Common Tag Confusions (TrigramTagger):\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Gold Tag':<12} {'Predicted':<12} {'Count':<8} {'Explanation'}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "tag_explanations = {\n",
    "    ('VBN', 'VBD'): 'Past participle vs past tense',\n",
    "    ('VBD', 'VBN'): 'Past tense vs past participle', \n",
    "    ('NN', 'VB'): 'Noun vs verb (ambiguous)',\n",
    "    ('VB', 'NN'): 'Verb vs noun (ambiguous)',\n",
    "    ('JJ', 'NN'): 'Adjective vs noun',\n",
    "    ('NN', 'JJ'): 'Noun vs adjective',\n",
    "    ('NNP', 'NN'): 'Proper noun vs common noun',\n",
    "    ('RB', 'RP'): 'Adverb vs particle',\n",
    "    ('IN', 'RB'): 'Preposition vs adverb',\n",
    "}\n",
    "\n",
    "for (gold, pred), count in confusion_counts.most_common(15):\n",
    "    explanation = tag_explanations.get((gold, pred), '')\n",
    "    print(f\"{gold:<12} {pred:<12} {count:<8} {explanation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b058653",
   "metadata": {},
   "source": [
    "### 7.7.4 Under the Hood: How N-gram Taggers Really Work\n",
    "\n",
    "**Key Insight: N-gram taggers are NOT machine learning!**\n",
    "\n",
    "They don't extract patterns or generalizeâ€”they simply:\n",
    "1. **Training**: Count occurrences and build a lookup table (dictionary)\n",
    "2. **Prediction**: Look up the context in the table\n",
    "\n",
    "This is why training is instantâ€”it's just counting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5771e849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BUILDING UNIGRAM TAGGER FROM SCRATCH\n",
      "============================================================\n",
      "\n",
      "TRAINING PHASE: Counting word-tag pairs...\n",
      "--------------------------------------------------\n",
      "Processed 79,795 word-tag pairs\n",
      "Found 12,518 unique words\n",
      "\n",
      "Sample of lookup table built:\n",
      "--------------------------------------------------\n",
      "  'the' â†’ AT\n",
      "  'is' â†’ BEZ\n",
      "  'running' â†’ VBG\n",
      "  'said' â†’ VBD\n",
      "  'President' â†’ NN-TL\n",
      "\n",
      "--------------------------------------------------\n",
      "Example: How 'that' was decided:\n",
      "  Tag counts for 'that': {'CS': 432, 'WPS': 96, 'DT': 93, 'QL': 4}\n",
      "  Most common tag selected: CS\n"
     ]
    }
   ],
   "source": [
    "# Let's implement a UnigramTagger from scratch to see exactly what it does!\n",
    "\n",
    "class MyUnigramTagger:\n",
    "    \"\"\"\n",
    "    A simplified UnigramTagger to show what really happens.\n",
    "    This is essentially what NLTK's UnigramTagger does!\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, training_data):\n",
    "        # Step 1: Count (word, tag) occurrences\n",
    "        from collections import defaultdict, Counter\n",
    "        \n",
    "        word_tag_counts = defaultdict(Counter)\n",
    "        \n",
    "        print(\"TRAINING PHASE: Counting word-tag pairs...\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        total_pairs = 0\n",
    "        for sentence in training_data:\n",
    "            for word, tag in sentence:\n",
    "                word_tag_counts[word][tag] += 1\n",
    "                total_pairs += 1\n",
    "        \n",
    "        print(f\"Processed {total_pairs:,} word-tag pairs\")\n",
    "        print(f\"Found {len(word_tag_counts):,} unique words\\n\")\n",
    "        \n",
    "        # Step 2: For each word, keep only the MOST COMMON tag\n",
    "        self.lookup_table = {}\n",
    "        \n",
    "        for word, tag_counts in word_tag_counts.items():\n",
    "            most_common_tag = tag_counts.most_common(1)[0][0]\n",
    "            self.lookup_table[word] = most_common_tag\n",
    "        \n",
    "        # Show some examples\n",
    "        print(\"Sample of lookup table built:\")\n",
    "        print(\"-\" * 50)\n",
    "        sample_words = ['the', 'is', 'running', 'said', 'President']\n",
    "        for word in sample_words:\n",
    "            if word in self.lookup_table:\n",
    "                print(f\"  '{word}' â†’ {self.lookup_table[word]}\")\n",
    "        \n",
    "        # Show word with multiple possible tags\n",
    "        print(\"\\n\" + \"-\" * 50)\n",
    "        print(\"Example: How 'that' was decided:\")\n",
    "        print(f\"  Tag counts for 'that': {dict(word_tag_counts['that'])}\")\n",
    "        print(f\"  Most common tag selected: {self.lookup_table['that']}\")\n",
    "    \n",
    "    def tag(self, words):\n",
    "        \"\"\"Prediction is just a dictionary lookup!\"\"\"\n",
    "        return [(word, self.lookup_table.get(word, 'NN')) for word in words]\n",
    "\n",
    "# Train our simple tagger\n",
    "print(\"=\" * 60)\n",
    "print(\"BUILDING UNIGRAM TAGGER FROM SCRATCH\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "my_tagger = MyUnigramTagger(train_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "eb3c2d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PREDICTION PHASE: Just Dictionary Lookups!\n",
      "============================================================\n",
      "\n",
      "Input words: ['The', 'cat', 'is', 'running', 'quickly']\n",
      "\n",
      "Prediction process (word by word):\n",
      "--------------------------------------------------\n",
      "  'The' â†’ lookup_table['The'] = 'AT'\n",
      "  'cat' â†’ NOT IN TABLE â†’ default to 'NN'\n",
      "  'is' â†’ lookup_table['is'] = 'BEZ'\n",
      "  'running' â†’ lookup_table['running'] = 'VBG'\n",
      "  'quickly' â†’ lookup_table['quickly'] = 'RB'\n",
      "\n",
      "Final result: [('The', 'AT'), ('cat', 'NN'), ('is', 'BEZ'), ('running', 'VBG'), ('quickly', 'RB')]\n",
      "\n",
      "============================================================\n",
      "KEY INSIGHT: No math, no learning, just table lookup!\n",
      "This is why 'training' takes milliseconds.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Now let's see what prediction looks like - it's just dict.get()!\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PREDICTION PHASE: Just Dictionary Lookups!\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "test_words = ['The', 'cat', 'is', 'running', 'quickly']\n",
    "\n",
    "print(f\"\\nInput words: {test_words}\\n\")\n",
    "print(\"Prediction process (word by word):\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for word in test_words:\n",
    "    if word in my_tagger.lookup_table:\n",
    "        tag = my_tagger.lookup_table[word]\n",
    "        print(f\"  '{word}' â†’ lookup_table['{word}'] = '{tag}'\")\n",
    "    else:\n",
    "        print(f\"  '{word}' â†’ NOT IN TABLE â†’ default to 'NN'\")\n",
    "\n",
    "print(f\"\\nFinal result: {my_tagger.tag(test_words)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"KEY INSIGHT: No math, no learning, just table lookup!\")\n",
    "print(\"This is why 'training' takes milliseconds.\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "570932c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BIGRAM TAGGER: (prev_tag, word) â†’ tag\n",
      "============================================================\n",
      "BigramTagger: 28,064 context-tag pairs learned\n",
      "\n",
      "Sample entries in BigramTagger's lookup table:\n",
      "--------------------------------------------------\n",
      "  ('AT', 'dog') â†’ NN\n",
      "  ('MD', 'go') â†’ VB\n",
      "  ('TO', 'run') â†’ VB\n",
      "  ('VBD', 'the') â†’ AT\n"
     ]
    }
   ],
   "source": [
    "# BigramTagger is the same idea, but with (prev_tag, word) as the key\n",
    "\n",
    "class MyBigramTagger:\n",
    "    \"\"\"\n",
    "    BigramTagger: lookup key is (previous_tag, current_word)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, training_data, backoff=None):\n",
    "        from collections import defaultdict, Counter\n",
    "        \n",
    "        # Count (prev_tag, word) â†’ tag\n",
    "        context_tag_counts = defaultdict(Counter)\n",
    "        \n",
    "        for sentence in training_data:\n",
    "            prev_tag = '<START>'\n",
    "            for word, tag in sentence:\n",
    "                context = (prev_tag, word)  # The key!\n",
    "                context_tag_counts[context][tag] += 1\n",
    "                prev_tag = tag\n",
    "        \n",
    "        # Build lookup table: (prev_tag, word) â†’ most_common_tag\n",
    "        self.lookup_table = {}\n",
    "        for context, tag_counts in context_tag_counts.items():\n",
    "            self.lookup_table[context] = tag_counts.most_common(1)[0][0]\n",
    "        \n",
    "        self.backoff = backoff\n",
    "        print(f\"BigramTagger: {len(self.lookup_table):,} context-tag pairs learned\")\n",
    "    \n",
    "    def tag(self, words):\n",
    "        result = []\n",
    "        prev_tag = '<START>'\n",
    "        \n",
    "        for word in words:\n",
    "            context = (prev_tag, word)\n",
    "            \n",
    "            if context in self.lookup_table:\n",
    "                tag = self.lookup_table[context]\n",
    "            elif self.backoff:\n",
    "                tag = self.backoff.tag([word])[0][1]\n",
    "            else:\n",
    "                tag = 'NN'\n",
    "            \n",
    "            result.append((word, tag))\n",
    "            prev_tag = tag\n",
    "        \n",
    "        return result\n",
    "\n",
    "# Show the internal data structure\n",
    "print(\"=\" * 60)\n",
    "print(\"BIGRAM TAGGER: (prev_tag, word) â†’ tag\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "my_bigram = MyBigramTagger(train_sents, backoff=my_tagger)\n",
    "\n",
    "print(\"\\nSample entries in BigramTagger's lookup table:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Show some interesting bigram contexts\n",
    "sample_contexts = [\n",
    "    ('AT', 'dog'),      # After article \"the\"\n",
    "    ('MD', 'go'),       # After modal \"will\"\n",
    "    ('TO', 'run'),      # After \"to\"\n",
    "    ('VBD', 'the'),     # After past tense verb\n",
    "]\n",
    "\n",
    "for ctx in sample_contexts:\n",
    "    if ctx in my_bigram.lookup_table:\n",
    "        print(f\"  {ctx} â†’ {my_bigram.lookup_table[ctx]}\")\n",
    "    else:\n",
    "        print(f\"  {ctx} â†’ (not in training data)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef10f835",
   "metadata": {},
   "source": [
    "### N-gram Taggers vs Machine Learning: Key Differences\n",
    "\n",
    "| Aspect | N-gram Taggers | Machine Learning (e.g., Perceptron) |\n",
    "|--------|----------------|-------------------------------------|\n",
    "| **Training** | Count frequencies, build dict | Iteratively adjust weights |\n",
    "| **Storage** | Lookup table (dict) | Weight vectors/matrices |\n",
    "| **Prediction** | `dict[context]` lookup | Compute `features Ã— weights` |\n",
    "| **Generalization** | âŒ None! Only exact matches | âœ“ Can handle unseen combinations |\n",
    "| **Unknown words** | Must use backoff | Uses features (suffix, shape) |\n",
    "| **Training time** | Milliseconds | Seconds to minutes |\n",
    "| **Memory** | O(unique contexts) | O(features Ã— tags) |\n",
    "\n",
    "### The Critical Limitation: No Generalization!\n",
    "\n",
    "```python\n",
    "# If training data has:\n",
    "#   \"the dog\" â†’ (AT, NN)\n",
    "#   \"the cat\" â†’ (AT, NN)\n",
    "#   \"the bird\" â†’ (AT, NN)\n",
    "\n",
    "# N-gram tagger stores THREE separate entries:\n",
    "#   ('AT', 'dog') â†’ NN\n",
    "#   ('AT', 'cat') â†’ NN  \n",
    "#   ('AT', 'bird') â†’ NN\n",
    "\n",
    "# It does NOT learn the pattern: \"after AT, nouns are likely\"\n",
    "# So for \"the elephant\" (never seen), it must backoff!\n",
    "\n",
    "# Machine Learning WOULD learn:\n",
    "#   weight[AT â†’ NN] = +5.0  (strong positive signal)\n",
    "#   This generalizes to ANY word after AT!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b0d61732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PROOF: N-gram Taggers Don't Generalize!\n",
      "======================================================================\n",
      "\n",
      "Checking training data for 'the X' patterns:\n",
      "--------------------------------------------------\n",
      "Found 2382 unique words after 'the' in training\n",
      "Sample: [('Fulton', 'NP-TL'), ('jury', 'NN'), ('City', 'NN-TL'), ('election', 'NN'), ('praise', 'NN')]\n",
      "\n",
      "--------------------------------------------------\n",
      "Testing with words NEVER seen after 'the':\n",
      "--------------------------------------------------\n",
      "\n",
      "BigramTagger prediction process:\n",
      "  Context ('<START>', 'the'):\n",
      "    â†’ FOUND in lookup table\n",
      "    â†’ Tag: AT\n",
      "  Context ('AT', 'flibbertigibbet'):\n",
      "    â†’ BACKOFF (context not seen in training)\n",
      "    â†’ Tag: NN\n",
      "  Context ('NN', 'is'):\n",
      "    â†’ FOUND in lookup table\n",
      "    â†’ Tag: BEZ\n",
      "  Context ('BEZ', 'dancing'):\n",
      "    â†’ BACKOFF (context not seen in training)\n",
      "    â†’ Tag: VBG\n",
      "\n",
      "âš ï¸ 'flibbertigibbet' gets NN only because of DEFAULT backoff,\n",
      "   NOT because the tagger 'learned' that nouns follow 'the'!\n"
     ]
    }
   ],
   "source": [
    "# Prove that N-gram taggers don't generalize\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PROOF: N-gram Taggers Don't Generalize!\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check if common patterns exist in training\n",
    "print(\"\\nChecking training data for 'the X' patterns:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "the_followed_by = {}\n",
    "for sent in train_sents:\n",
    "    for i in range(len(sent) - 1):\n",
    "        if sent[i][0].lower() == 'the':\n",
    "            next_word = sent[i+1][0]\n",
    "            next_tag = sent[i+1][1]\n",
    "            if next_word not in the_followed_by:\n",
    "                the_followed_by[next_word] = next_tag\n",
    "\n",
    "print(f\"Found {len(the_followed_by)} unique words after 'the' in training\")\n",
    "print(f\"Sample: {list(the_followed_by.items())[:5]}\")\n",
    "\n",
    "# Now test with unseen words\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "print(\"Testing with words NEVER seen after 'the':\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "unseen_test = ['the', 'flibbertigibbet', 'is', 'dancing']\n",
    "\n",
    "# What BigramTagger does:\n",
    "print(\"\\nBigramTagger prediction process:\")\n",
    "prev_tag = '<START>'\n",
    "for word in unseen_test:\n",
    "    context = (prev_tag, word)\n",
    "    in_table = context in my_bigram.lookup_table\n",
    "    \n",
    "    if in_table:\n",
    "        tag = my_bigram.lookup_table[context]\n",
    "        source = \"FOUND in lookup table\"\n",
    "    else:\n",
    "        tag = my_tagger.lookup_table.get(word, 'NN')\n",
    "        source = \"BACKOFF (context not seen in training)\"\n",
    "    \n",
    "    print(f\"  Context {context}:\")\n",
    "    print(f\"    â†’ {source}\")\n",
    "    print(f\"    â†’ Tag: {tag}\")\n",
    "    prev_tag = tag\n",
    "\n",
    "print(\"\\nâš ï¸ 'flibbertigibbet' gets NN only because of DEFAULT backoff,\")\n",
    "print(\"   NOT because the tagger 'learned' that nouns follow 'the'!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b71a3863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TIMING: Why N-gram Taggers Are So Fast\n",
      "============================================================\n",
      "\n",
      "N-gram Tagger (Uni+Bi+Tri) training: 1947.0 ms\n",
      "  â†’ Just counting and storing in dictionaries!\n",
      "\n",
      "------------------------------------------------------------\n",
      "What gets stored (the 'model'):\n",
      "------------------------------------------------------------\n",
      "  UnigramTagger: dict with 10,036 entries\n",
      "  BigramTagger:  dict with 2,810 entries\n",
      "  TrigramTagger: dict with 1,182 entries\n",
      "\n",
      "------------------------------------------------------------\n",
      "Peek at actual data structure (UnigramTagger):\n",
      "------------------------------------------------------------\n",
      "  carved â†’ 'VBN'\n",
      "  constructing â†’ 'VBG'\n",
      "  $1,000 â†’ 'NNS'\n",
      "  $39,000 â†’ 'NNS'\n",
      "  dismal â†’ 'JJ'\n",
      "\n",
      "This is literally just: dict[word] = most_common_tag\n",
      "No weights, no gradients, no optimizationâ€”just counting!\n"
     ]
    }
   ],
   "source": [
    "# Compare time complexity: N-gram vs ML tagger\n",
    "\n",
    "import time\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TIMING: Why N-gram Taggers Are So Fast\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Time N-gram tagger training\n",
    "start = time.time()\n",
    "fast_unigram = UnigramTagger(train_sents, backoff=DefaultTagger('NN'))\n",
    "fast_bigram = BigramTagger(train_sents, backoff=fast_unigram)\n",
    "fast_trigram = TrigramTagger(train_sents, backoff=fast_bigram)\n",
    "ngram_time = time.time() - start\n",
    "\n",
    "print(f\"\\nN-gram Tagger (Uni+Bi+Tri) training: {ngram_time*1000:.1f} ms\")\n",
    "print(\"  â†’ Just counting and storing in dictionaries!\")\n",
    "\n",
    "# Show what the \"model\" actually is\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"What gets stored (the 'model'):\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"  UnigramTagger: dict with {len(fast_unigram._context_to_tag):,} entries\")\n",
    "print(f\"  BigramTagger:  dict with {len(fast_bigram._context_to_tag):,} entries\")\n",
    "print(f\"  TrigramTagger: dict with {len(fast_trigram._context_to_tag):,} entries\")\n",
    "\n",
    "# Peek at actual storage\n",
    "print(\"\\n\" + \"-\" * 60)\n",
    "print(\"Peek at actual data structure (UnigramTagger):\")\n",
    "print(\"-\" * 60)\n",
    "sample_items = list(fast_unigram._context_to_tag.items())[:5]\n",
    "for context, tag in sample_items:\n",
    "    print(f\"  {context} â†’ '{tag}'\")\n",
    "\n",
    "print(f\"\\nThis is literally just: dict[word] = most_common_tag\")\n",
    "print(\"No weights, no gradients, no optimizationâ€”just counting!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ccbad7",
   "metadata": {},
   "source": [
    "### Summary: The Algorithm in Plain English\n",
    "\n",
    "**Training (UnigramTagger):**\n",
    "```\n",
    "1. Create empty dictionary: word_to_tag = {}\n",
    "2. For each (word, tag) in training data:\n",
    "      count[word][tag] += 1\n",
    "3. For each word:\n",
    "      word_to_tag[word] = tag_with_highest_count\n",
    "4. Done! (That's it!)\n",
    "```\n",
    "\n",
    "**Training (BigramTagger):**\n",
    "```\n",
    "1. Create empty dictionary: context_to_tag = {}\n",
    "2. For each sentence:\n",
    "      prev_tag = '<START>'\n",
    "      For each (word, tag):\n",
    "          count[(prev_tag, word)][tag] += 1\n",
    "          prev_tag = tag\n",
    "3. For each (prev_tag, word):\n",
    "      context_to_tag[(prev_tag, word)] = tag_with_highest_count\n",
    "4. Done!\n",
    "```\n",
    "\n",
    "**Prediction:**\n",
    "```\n",
    "1. Look up context in dictionary\n",
    "2. If found â†’ return stored tag\n",
    "3. If not found â†’ ask backoff tagger\n",
    "```\n",
    "\n",
    "### Why It's NOT Machine Learning\n",
    "\n",
    "| What ML Does | What N-gram Does |\n",
    "|--------------|------------------|\n",
    "| Learns **patterns** | Memorizes **instances** |\n",
    "| Extracts **features** | Uses **exact match** |\n",
    "| Optimizes **loss function** | Just **counts** |\n",
    "| Can **generalize** | Can only **recall** |\n",
    "| Adjusts **weights** | Stores **frequencies** |\n",
    "\n",
    "**NLTK's `pos_tag()` uses the Perceptron Tagger** â€” which IS machine learning.  \n",
    "**N-gram taggers are for learning/custom domains** â€” simpler but less powerful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0dfa4d",
   "metadata": {},
   "source": [
    "### 7.7.5 Building Domain-Specific Taggers\n",
    "\n",
    "You can train taggers on **your own annotated data** for specialized domains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e59c4161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Medical Domain Tagger\n",
      "==================================================\n",
      "\n",
      "'The patient has elevated blood pressure'\n",
      "  â†’ [('The', 'DT'), ('patient', 'NN'), ('has', 'VBZ'), ('elevated', 'JJ'), ('blood', 'NN'), ('pressure', 'NN')]\n",
      "\n",
      "'Prescribe medication daily'\n",
      "  â†’ [('Prescribe', 'VB'), ('medication', 'NN'), ('daily', 'RB')]\n",
      "\n",
      "ğŸ’¡ Tip: The more training data you provide, the better your custom tagger!\n"
     ]
    }
   ],
   "source": [
    "# Example: Creating a custom tagger with your own tagged data\n",
    "# Format: list of sentences, each sentence is list of (word, tag) tuples\n",
    "\n",
    "# Suppose you have domain-specific tagged data (e.g., medical text)\n",
    "custom_training_data = [\n",
    "    [('The', 'DT'), ('patient', 'NN'), ('has', 'VBZ'), ('diabetes', 'NN'), ('.', '.')],\n",
    "    [('Prescribe', 'VB'), ('insulin', 'NN'), ('daily', 'RB'), ('.', '.')],\n",
    "    [('Blood', 'NN'), ('pressure', 'NN'), ('is', 'VBZ'), ('elevated', 'JJ'), ('.', '.')],\n",
    "    [('The', 'DT'), ('MRI', 'NN'), ('shows', 'VBZ'), ('no', 'DT'), ('abnormalities', 'NNS'), ('.', '.')],\n",
    "    [('Patient', 'NN'), ('reports', 'VBZ'), ('chest', 'NN'), ('pain', 'NN'), ('.', '.')],\n",
    "]\n",
    "\n",
    "# Build a custom tagger chain\n",
    "custom_default = DefaultTagger('NN')\n",
    "custom_unigram = UnigramTagger(custom_training_data, backoff=custom_default)\n",
    "custom_bigram = BigramTagger(custom_training_data, backoff=custom_unigram)\n",
    "\n",
    "print(\"Custom Medical Domain Tagger\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test on medical text\n",
    "test_sentences = [\n",
    "    \"The patient has elevated blood pressure\",\n",
    "    \"Prescribe medication daily\",\n",
    "]\n",
    "\n",
    "for sent in test_sentences:\n",
    "    tokens = word_tokenize(sent)\n",
    "    tagged = custom_bigram.tag(tokens)\n",
    "    print(f\"\\n'{sent}'\")\n",
    "    print(f\"  â†’ {tagged}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ Tip: The more training data you provide, the better your custom tagger!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c5e80d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tagger works!\n",
      "  [('The', 'AT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'NN')]\n",
      "\n",
      "ğŸ’¡ Tip: Save trained taggers to avoid retraining every time!\n"
     ]
    }
   ],
   "source": [
    "# Save and load trained taggers using pickle\n",
    "import pickle\n",
    "\n",
    "# Save tagger to file\n",
    "with open('my_custom_tagger.pkl', 'wb') as f:\n",
    "    pickle.dump(trigram_tagger, f)\n",
    "\n",
    "# Load tagger from file\n",
    "with open('my_custom_tagger.pkl', 'rb') as f:\n",
    "    loaded_tagger = pickle.load(f)\n",
    "\n",
    "# Verify it works\n",
    "test_tokens = word_tokenize(\"The quick brown fox jumps\")\n",
    "print(\"Loaded tagger works!\")\n",
    "print(f\"  {loaded_tagger.tag(test_tokens)}\")\n",
    "\n",
    "# Clean up\n",
    "import os\n",
    "os.remove('my_custom_tagger.pkl')\n",
    "print(\"\\nğŸ’¡ Tip: Save trained taggers to avoid retraining every time!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1443a6",
   "metadata": {},
   "source": [
    "### 7.7.6 RegexpTagger: Pattern-Based Tagging\n",
    "\n",
    "Use **regex patterns** to tag words by their shape (suffixes, prefixes, etc.):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c756ad7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RegexpTagger - Pattern-Based Rules\n",
      "=======================================================\n",
      "Word            Pattern Matched      Tag\n",
      "-------------------------------------------------------\n",
      "running         .*ing$               VBG\n",
      "walked          .*ed$                VBD\n",
      "quickly         .*ly$                RB\n",
      "beautiful       .*ful$               JJ\n",
      "education       .*tion$              NN\n",
      "happiness       .*ness$              NN\n",
      "42              ^[0-9]+$             CD\n",
      "London          ^[A-Z][a-z]+$        NNP\n",
      "cat             .*                   NN\n",
      "\n",
      "ğŸ’¡ RegexpTagger is great for handling unknown words based on morphology!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import RegexpTagger\n",
    "\n",
    "# Define regex patterns for common word endings\n",
    "# Format: (pattern, tag) - patterns are checked in order\n",
    "patterns = [\n",
    "    (r'.*ing$', 'VBG'),    # gerunds: running, eating\n",
    "    (r'.*ed$', 'VBD'),     # past tense: walked, jumped\n",
    "    (r'.*es$', 'VBZ'),     # 3rd person: watches, goes\n",
    "    (r'.*ly$', 'RB'),      # adverbs: quickly, slowly\n",
    "    (r'.*tion$', 'NN'),    # nouns: nation, education\n",
    "    (r'.*ness$', 'NN'),    # nouns: happiness, darkness\n",
    "    (r'.*ful$', 'JJ'),     # adjectives: beautiful, careful\n",
    "    (r'.*able$', 'JJ'),    # adjectives: readable, capable\n",
    "    (r'^[0-9]+$', 'CD'),   # numbers: 123, 456\n",
    "    (r'^[A-Z][a-z]+$', 'NNP'),  # capitalized: John, Paris\n",
    "    (r'.*', 'NN'),         # default: noun\n",
    "]\n",
    "\n",
    "regex_tagger = RegexpTagger(patterns)\n",
    "\n",
    "print(\"RegexpTagger - Pattern-Based Rules\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "test_words = ['running', 'walked', 'quickly', 'beautiful', \n",
    "              'education', 'happiness', '42', 'London', 'cat']\n",
    "\n",
    "print(f\"{'Word':<15} {'Pattern Matched':<20} {'Tag'}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for word in test_words:\n",
    "    tag = regex_tagger.tag([word])[0][1]\n",
    "    # Find which pattern matched\n",
    "    matched = 'default'\n",
    "    for pattern, ptag in patterns:\n",
    "        import re\n",
    "        if re.match(pattern, word) and ptag == tag:\n",
    "            matched = pattern\n",
    "            break\n",
    "    print(f\"{word:<15} {matched:<20} {tag}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ RegexpTagger is great for handling unknown words based on morphology!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c37614eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sophisticated Tagger Chain\n",
      "============================================================\n",
      "\n",
      "TrigramTagger (most context)\n",
      "    â†“ backoff\n",
      "BigramTagger (previous tag)\n",
      "    â†“ backoff\n",
      "UnigramTagger (word lookup)\n",
      "    â†“ backoff\n",
      "RegexpTagger (pattern matching)  â† Better than DefaultTagger!\n",
      "\n",
      "With DefaultTagger backoff:  83.44%\n",
      "With RegexpTagger backoff:   84.89%\n",
      "Improvement: +1.45%\n"
     ]
    }
   ],
   "source": [
    "# Combine RegexpTagger with N-gram taggers for best results\n",
    "# Regex handles unknown words, N-grams handle known patterns\n",
    "\n",
    "# Build a sophisticated tagger chain\n",
    "sophisticated_tagger = TrigramTagger(\n",
    "    train_sents,\n",
    "    backoff=BigramTagger(\n",
    "        train_sents,\n",
    "        backoff=UnigramTagger(\n",
    "            train_sents,\n",
    "            backoff=RegexpTagger(patterns)  # Regex as final backoff!\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Sophisticated Tagger Chain\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "TrigramTagger (most context)\n",
    "    â†“ backoff\n",
    "BigramTagger (previous tag)\n",
    "    â†“ backoff\n",
    "UnigramTagger (word lookup)\n",
    "    â†“ backoff\n",
    "RegexpTagger (pattern matching)  â† Better than DefaultTagger!\n",
    "\"\"\")\n",
    "\n",
    "# Compare accuracy\n",
    "default_chain_acc = trigram_tagger.accuracy(test_sents)\n",
    "regex_chain_acc = sophisticated_tagger.accuracy(test_sents)\n",
    "\n",
    "print(f\"With DefaultTagger backoff:  {default_chain_acc:.2%}\")\n",
    "print(f\"With RegexpTagger backoff:   {regex_chain_acc:.2%}\")\n",
    "print(f\"Improvement: +{(regex_chain_acc - default_chain_acc)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115cac99",
   "metadata": {},
   "source": [
    "### 7.7.7 Summary: Custom Taggers\n",
    "\n",
    "| Tagger | What It Uses | Best For | Generalization |\n",
    "|--------|--------------|----------|----------------|\n",
    "| **DefaultTagger** | Nothing | Final fallback | âŒ None |\n",
    "| **UnigramTagger** | word only | Common words | âŒ Memorizes |\n",
    "| **BigramTagger** | prev_tag + word | Ambiguous words | âŒ Memorizes |\n",
    "| **TrigramTagger** | prev_2_tags + word | Complex patterns | âŒ Memorizes |\n",
    "| **RegexpTagger** | word patterns | Unknown words | âœ“ Patterns |\n",
    "\n",
    "**Best Practice â€” Build a Backoff Chain:**\n",
    "```python\n",
    "tagger = TrigramTagger(train, backoff=\n",
    "    BigramTagger(train, backoff=\n",
    "        UnigramTagger(train, backoff=\n",
    "            RegexpTagger(patterns))))\n",
    "```\n",
    "\n",
    "**Remember:** N-gram taggers just memorize; they don't learn patterns like ML models do!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79e50d2",
   "metadata": {},
   "source": [
    "## 7.8 Practical Application: Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8119087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: The ambitious young scientist quickly discovered a remarkable \n",
      "breakthrough in artificial intelligence. She carefully analyzed the complex \n",
      "data and brilliantly solved the challenging problem.\n",
      "\n",
      "Text Analysis\n",
      "==================================================\n",
      "\n",
      "Nouns (6):\n",
      "  ['scientist', 'breakthrough', 'intelligence', 'data', 'challenging', 'problem']\n",
      "\n",
      "Verbs (3):\n",
      "  ['discovered', 'analyzed', 'solved']\n",
      "\n",
      "Adjectives (5):\n",
      "  ['ambitious', 'young', 'remarkable', 'artificial', 'complex']\n",
      "\n",
      "Adverbs (3):\n",
      "  ['quickly', 'carefully', 'brilliantly']\n",
      "\n",
      "Pronouns (1):\n",
      "  ['She']\n"
     ]
    }
   ],
   "source": [
    "def analyze_text(text):\n",
    "    \"\"\"Comprehensive text analysis using POS tagging\"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged = pos_tag(tokens)\n",
    "    \n",
    "    # Count by category\n",
    "    categories = {\n",
    "        'Nouns': ['NN', 'NNS', 'NNP', 'NNPS'],\n",
    "        'Verbs': ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'],\n",
    "        'Adjectives': ['JJ', 'JJR', 'JJS'],\n",
    "        'Adverbs': ['RB', 'RBR', 'RBS'],\n",
    "        'Pronouns': ['PRP', 'PRP$', 'WP', 'WP$'],\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    for category, tags in categories.items():\n",
    "        words = [w for w, t in tagged if t in tags]\n",
    "        results[category] = {\n",
    "            'count': len(words),\n",
    "            'words': words\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "text = \"\"\"The ambitious young scientist quickly discovered a remarkable \n",
    "breakthrough in artificial intelligence. She carefully analyzed the complex \n",
    "data and brilliantly solved the challenging problem.\"\"\"\n",
    "\n",
    "print(f\"Text: {text}\\n\")\n",
    "\n",
    "analysis = analyze_text(text)\n",
    "\n",
    "print(\"Text Analysis\")\n",
    "print(\"=\" * 50)\n",
    "for category, data in analysis.items():\n",
    "    print(f\"\\n{category} ({data['count']}):\")\n",
    "    print(f\"  {data['words']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac7898e",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Method | Description |\n",
    "|--------|-------------|\n",
    "| `pos_tag(tokens)` | Tag a list of tokens |\n",
    "| `pos_tag(tokens, tagset='universal')` | Use universal tagset |\n",
    "| `pos_tag_sents(list_of_sents)` | Batch tag multiple sentences |\n",
    "| `nltk.help.upenn_tagset('TAG')` | Get tag description |\n",
    "\n",
    "### Common Tags\n",
    "- **Nouns**: NN, NNS, NNP, NNPS\n",
    "- **Verbs**: VB, VBD, VBG, VBN, VBP, VBZ\n",
    "- **Adjectives**: JJ, JJR, JJS\n",
    "- **Adverbs**: RB, RBR, RBS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
