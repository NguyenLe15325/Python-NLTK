{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e89c8955",
   "metadata": {},
   "source": [
    "# NLTK Complete Guide - Section 7: Part-of-Speech (POS) Tagging\n",
    "\n",
    "This notebook covers:\n",
    "- What is POS Tagging?\n",
    "- NLTK POS Taggers\n",
    "- Penn Treebank Tag Set\n",
    "- Custom Taggers\n",
    "- Practical Applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab9b81f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('tagsets', quiet=True)\n",
    "nltk.download('universal_tagset', quiet=True)\n",
    "nltk.download('brown', quiet=True)\n",
    "\n",
    "from nltk import pos_tag, pos_tag_sents\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879207b0",
   "metadata": {},
   "source": [
    "## 7.1 What is POS Tagging?\n",
    "\n",
    "**Part-of-Speech (POS) Tagging** assigns grammatical categories to words:\n",
    "- Noun, Verb, Adjective, Adverb\n",
    "- Pronoun, Preposition, Conjunction\n",
    "- And more specific subcategories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bff8eea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: The quick brown fox jumps over the lazy dog.\n",
      "\n",
      "POS Tags:\n",
      "[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Tokenize and tag\n",
    "tokens = word_tokenize(sentence)\n",
    "tagged = pos_tag(tokens)\n",
    "\n",
    "print(f\"Sentence: {sentence}\\n\")\n",
    "print(\"POS Tags:\")\n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85427d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word         Tag    Description\n",
      "--------------------------------------------------\n",
      "The          DT     Determiner\n",
      "quick        JJ     Adjective\n",
      "brown        NN     Noun (singular)\n",
      "fox          NN     Noun (singular)\n",
      "jumps        VBZ    Verb (3rd person singular)\n",
      "over         IN     Preposition\n",
      "the          DT     Determiner\n",
      "lazy         JJ     Adjective\n",
      "dog          NN     Noun (singular)\n",
      ".            .      Punctuation\n"
     ]
    }
   ],
   "source": [
    "# Pretty print\n",
    "print(f\"{'Word':<12} {'Tag':<6} {'Description'}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "tag_descriptions = {\n",
    "    'DT': 'Determiner',\n",
    "    'JJ': 'Adjective',\n",
    "    'NN': 'Noun (singular)',\n",
    "    'NNS': 'Noun (plural)',\n",
    "    'VBZ': 'Verb (3rd person singular)',\n",
    "    'IN': 'Preposition',\n",
    "    '.': 'Punctuation',\n",
    "}\n",
    "\n",
    "for word, tag in tagged:\n",
    "    desc = tag_descriptions.get(tag, 'Other')\n",
    "    print(f\"{word:<12} {tag:<6} {desc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a4c23b",
   "metadata": {},
   "source": [
    "## 7.2 Penn Treebank Tag Set\n",
    "\n",
    "NLTK uses the Penn Treebank tagset by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4602e41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common Penn Treebank POS Tags\n",
      "============================================================\n",
      "NN     Noun, singular (dog, city)\n",
      "NNS    Noun, plural (dogs, cities)\n",
      "NNP    Proper noun, singular (John, London)\n",
      "NNPS   Proper noun, plural (Americans)\n",
      "VB     Verb, base form (run, eat)\n",
      "VBD    Verb, past tense (ran, ate)\n",
      "VBG    Verb, gerund (running, eating)\n",
      "VBN    Verb, past participle (eaten, written)\n",
      "VBP    Verb, non-3rd person (run, eat)\n",
      "VBZ    Verb, 3rd person singular (runs, eats)\n",
      "JJ     Adjective (big, green)\n",
      "JJR    Adjective, comparative (bigger)\n",
      "JJS    Adjective, superlative (biggest)\n",
      "RB     Adverb (quickly, very)\n",
      "RBR    Adverb, comparative (faster)\n",
      "RBS    Adverb, superlative (fastest)\n",
      "PRP    Personal pronoun (I, you, he)\n",
      "PRP$   Possessive pronoun (my, your)\n",
      "DT     Determiner (the, a, an)\n",
      "IN     Preposition (in, on, at)\n",
      "CC     Coordinating conjunction (and, or)\n",
      "TO     to\n",
      "MD     Modal (can, will, should)\n"
     ]
    }
   ],
   "source": [
    "# Common POS tags\n",
    "common_tags = {\n",
    "    # Nouns\n",
    "    'NN': 'Noun, singular (dog, city)',\n",
    "    'NNS': 'Noun, plural (dogs, cities)',\n",
    "    'NNP': 'Proper noun, singular (John, London)',\n",
    "    'NNPS': 'Proper noun, plural (Americans)',\n",
    "    \n",
    "    # Verbs\n",
    "    'VB': 'Verb, base form (run, eat)',\n",
    "    'VBD': 'Verb, past tense (ran, ate)',\n",
    "    'VBG': 'Verb, gerund (running, eating)',\n",
    "    'VBN': 'Verb, past participle (eaten, written)',\n",
    "    'VBP': 'Verb, non-3rd person (run, eat)',\n",
    "    'VBZ': 'Verb, 3rd person singular (runs, eats)',\n",
    "    \n",
    "    # Adjectives\n",
    "    'JJ': 'Adjective (big, green)',\n",
    "    'JJR': 'Adjective, comparative (bigger)',\n",
    "    'JJS': 'Adjective, superlative (biggest)',\n",
    "    \n",
    "    # Adverbs\n",
    "    'RB': 'Adverb (quickly, very)',\n",
    "    'RBR': 'Adverb, comparative (faster)',\n",
    "    'RBS': 'Adverb, superlative (fastest)',\n",
    "    \n",
    "    # Others\n",
    "    'PRP': 'Personal pronoun (I, you, he)',\n",
    "    'PRP$': 'Possessive pronoun (my, your)',\n",
    "    'DT': 'Determiner (the, a, an)',\n",
    "    'IN': 'Preposition (in, on, at)',\n",
    "    'CC': 'Coordinating conjunction (and, or)',\n",
    "    'TO': 'to',\n",
    "    'MD': 'Modal (can, will, should)',\n",
    "}\n",
    "\n",
    "print(\"Common Penn Treebank POS Tags\")\n",
    "print(\"=\" * 60)\n",
    "for tag, description in common_tags.items():\n",
    "    print(f\"{tag:<6} {description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9694476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n"
     ]
    }
   ],
   "source": [
    "# Get help on a specific tag\n",
    "nltk.help.upenn_tagset('VBG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06acb7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n"
     ]
    }
   ],
   "source": [
    "# All noun tags\n",
    "nltk.help.upenn_tagset('NN.*')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc86843",
   "metadata": {},
   "source": [
    "## 7.3 Universal Tagset\n",
    "\n",
    "Simplified tagset that works across languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a5b7ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word         Penn     Universal \n",
      "-----------------------------------\n",
      "The          DT       DET       \n",
      "quick        JJ       ADJ       \n",
      "brown        NN       NOUN      \n",
      "fox          NN       NOUN      \n",
      "jumps        VBZ      VERB      \n",
      "over         IN       ADP       \n",
      "the          DT       DET       \n",
      "lazy         JJ       ADJ       \n",
      "dog          NN       NOUN      \n",
      ".            .        .         \n"
     ]
    }
   ],
   "source": [
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "tokens = word_tokenize(sentence)\n",
    "\n",
    "# Default (Penn Treebank)\n",
    "penn_tags = pos_tag(tokens)\n",
    "\n",
    "# Universal tagset\n",
    "universal_tags = pos_tag(tokens, tagset='universal')\n",
    "\n",
    "print(f\"{'Word':<12} {'Penn':<8} {'Universal':<10}\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "for (word, penn), (_, univ) in zip(penn_tags, universal_tags):\n",
    "    print(f\"{word:<12} {penn:<8} {univ:<10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4f5f710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Universal Tagset\n",
      "========================================\n",
      "NOUN     Nouns\n",
      "VERB     Verbs\n",
      "ADJ      Adjectives\n",
      "ADV      Adverbs\n",
      "PRON     Pronouns\n",
      "DET      Determiners\n",
      "ADP      Adpositions (prepositions)\n",
      "NUM      Numbers\n",
      "CONJ     Conjunctions\n",
      "PRT      Particles\n",
      ".        Punctuation\n",
      "X        Other\n"
     ]
    }
   ],
   "source": [
    "# Universal tags\n",
    "universal_tagset = {\n",
    "    'NOUN': 'Nouns',\n",
    "    'VERB': 'Verbs',\n",
    "    'ADJ': 'Adjectives',\n",
    "    'ADV': 'Adverbs',\n",
    "    'PRON': 'Pronouns',\n",
    "    'DET': 'Determiners',\n",
    "    'ADP': 'Adpositions (prepositions)',\n",
    "    'NUM': 'Numbers',\n",
    "    'CONJ': 'Conjunctions',\n",
    "    'PRT': 'Particles',\n",
    "    '.': 'Punctuation',\n",
    "    'X': 'Other',\n",
    "}\n",
    "\n",
    "print(\"Universal Tagset\")\n",
    "print(\"=\" * 40)\n",
    "for tag, desc in universal_tagset.items():\n",
    "    print(f\"{tag:<8} {desc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa792027",
   "metadata": {},
   "source": [
    "## 7.4 Tagging Multiple Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44e50080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method 1: Individual sentences\n",
      "==================================================\n",
      "\n",
      "Natural language processing is fascinating.\n",
      "Tags: [('Natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('is', 'VBZ'), ('fascinating', 'VBG'), ('.', '.')]\n",
      "\n",
      "It enables computers to understand human language.\n",
      "Tags: [('It', 'PRP'), ('enables', 'VBZ'), ('computers', 'NNS'), ('to', 'TO'), ('understand', 'VB'), ('human', 'JJ'), ('language', 'NN'), ('.', '.')]\n",
      "\n",
      "Many applications use NLP today.\n",
      "Tags: [('Many', 'JJ'), ('applications', 'NNS'), ('use', 'VBP'), ('NLP', 'NNP'), ('today', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Natural language processing is fascinating.\n",
    "It enables computers to understand human language.\n",
    "Many applications use NLP today.\"\"\"\n",
    "\n",
    "# Method 1: Tag sentence by sentence\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "print(\"Method 1: Individual sentences\")\n",
    "print(\"=\" * 50)\n",
    "for sent in sentences:\n",
    "    tokens = word_tokenize(sent)\n",
    "    tagged = pos_tag(tokens)\n",
    "    print(f\"\\n{sent}\")\n",
    "    print(f\"Tags: {tagged}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d478701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method 2: Batch tagging (pos_tag_sents)\n",
      "==================================================\n",
      "\n",
      "Sentence 1: [('Natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('is', 'VBZ'), ('fascinating', 'VBG'), ('.', '.')]\n",
      "\n",
      "Sentence 2: [('It', 'PRP'), ('enables', 'VBZ'), ('computers', 'NNS'), ('to', 'TO'), ('understand', 'VB'), ('human', 'JJ'), ('language', 'NN'), ('.', '.')]\n",
      "\n",
      "Sentence 3: [('Many', 'JJ'), ('applications', 'NNS'), ('use', 'VBP'), ('NLP', 'NNP'), ('today', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Method 2: Batch tagging (more efficient)\n",
    "tokenized_sents = [word_tokenize(s) for s in sentences]\n",
    "tagged_sents = pos_tag_sents(tokenized_sents)\n",
    "\n",
    "print(\"Method 2: Batch tagging (pos_tag_sents)\")\n",
    "print(\"=\" * 50)\n",
    "for i, tagged in enumerate(tagged_sents, 1):\n",
    "    print(f\"\\nSentence {i}: {tagged}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5e5eeb",
   "metadata": {},
   "source": [
    "## 7.5 Context-Dependent POS\n",
    "\n",
    "The same word can have different POS tags depending on context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "973df61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'book' in different contexts:\n",
      "----------------------------------------\n",
      "I read a book.                 book = NN\n",
      "Please book a table.           book = NN\n"
     ]
    }
   ],
   "source": [
    "# \"book\" as noun vs verb\n",
    "sentences = [\n",
    "    \"I read a book.\",           # book = noun\n",
    "    \"Please book a table.\",     # book = verb\n",
    "]\n",
    "\n",
    "print(\"'book' in different contexts:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for sent in sentences:\n",
    "    tokens = word_tokenize(sent)\n",
    "    tagged = pos_tag(tokens)\n",
    "    book_tag = [t for w, t in tagged if w.lower() == 'book'][0]\n",
    "    print(f\"{sent:<30} book = {book_tag}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea491b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context-Dependent POS Tags\n",
      "=======================================================\n",
      "Sentence                            Word     Tag\n",
      "-------------------------------------------------------\n",
      "I run every day.                    run      VBP\n",
      "The run was exhausting.             run      NN\n",
      "She can fish.                       fish     VB\n",
      "I caught a fish.                    fish     NN\n",
      "Light the candle.                   light    NNP\n",
      "The light is bright.                light    NN\n",
      "This box is light.                  light    JJ\n"
     ]
    }
   ],
   "source": [
    "# More examples of context-dependent tags\n",
    "ambiguous_examples = [\n",
    "    (\"I run every day.\", \"run\"),\n",
    "    (\"The run was exhausting.\", \"run\"),\n",
    "    (\"She can fish.\", \"fish\"),\n",
    "    (\"I caught a fish.\", \"fish\"),\n",
    "    (\"Light the candle.\", \"light\"),\n",
    "    (\"The light is bright.\", \"light\"),\n",
    "    (\"This box is light.\", \"light\"),\n",
    "]\n",
    "\n",
    "print(\"Context-Dependent POS Tags\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"{'Sentence':<35} {'Word':<8} {'Tag'}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for sent, target_word in ambiguous_examples:\n",
    "    tokens = word_tokenize(sent)\n",
    "    tagged = pos_tag(tokens)\n",
    "    word_tag = [t for w, t in tagged if w.lower() == target_word][0]\n",
    "    print(f\"{sent:<35} {target_word:<8} {word_tag}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f017e58",
   "metadata": {},
   "source": [
    "### âš ï¸ Note: Why Some Tags May Be Wrong\n",
    "\n",
    "You may notice that **\"Light the candle\"** tags `light` as `NNP` (proper noun) instead of `VB` (verb). This is a **tagger error** that reveals how POS tagging works under the hood.\n",
    "\n",
    "---\n",
    "\n",
    "## How POS Tagging Works Under the Hood\n",
    "\n",
    "### 1. **Statistical/Probabilistic Approach**\n",
    "\n",
    "NLTK's default tagger (`averaged_perceptron_tagger`) is a **machine learning model** trained on labeled data. It doesn't \"understand\" languageâ€”it makes **statistical predictions** based on patterns learned during training.\n",
    "\n",
    "The tagger calculates:\n",
    "```\n",
    "P(tag | word, context) = probability of a tag given the word and surrounding context\n",
    "```\n",
    "\n",
    "### 2. **Features Used by the Tagger**\n",
    "\n",
    "The perceptron tagger considers multiple **features**:\n",
    "\n",
    "| Feature | Example | Why It Matters |\n",
    "|---------|---------|----------------|\n",
    "| **Current word** | \"light\" | Some words strongly predict certain tags |\n",
    "| **Word suffix** | \"-ing\", \"-ed\", \"-ly\" | Suffixes indicate verb forms, adverbs |\n",
    "| **Word prefix** | \"un-\", \"pre-\" | Can indicate adjectives or verbs |\n",
    "| **Previous tag** | DT â†’ likely NN/JJ next | Tag sequences follow patterns |\n",
    "| **Previous word** | \"the\" before noun | Determiners precede nouns |\n",
    "| **Capitalization** | \"Light\" vs \"light\" | Capitals often indicate proper nouns |\n",
    "| **Word shape** | all caps, mixed case | Helps identify special tokens |\n",
    "\n",
    "### 3. **Why Errors Occur**\n",
    "\n",
    "**Problem 1: Capitalization Bias**\n",
    "- \"Light\" at sentence start is capitalized\n",
    "- Training data has many capitalized proper nouns (names, places)\n",
    "- Tagger may incorrectly associate capitalization with NNP\n",
    "\n",
    "**Problem 2: Ambiguity**\n",
    "- \"light\" can be: noun, verb, or adjective\n",
    "- Without strong contextual signals, the tagger guesses based on training statistics\n",
    "\n",
    "**Problem 3: Training Data Distribution**\n",
    "- If \"light\" appears more often as noun/adjective in training data\n",
    "- The model may be biased toward those tags\n",
    "\n",
    "### 4. **The Tagging Algorithm (Simplified)**\n",
    "\n",
    "```\n",
    "For each word in sentence:\n",
    "    1. Extract features (word, suffix, previous tag, etc.)\n",
    "    2. Calculate score for each possible tag\n",
    "    3. Select tag with highest score\n",
    "    4. Move to next word (using this tag as \"previous tag\")\n",
    "```\n",
    "\n",
    "This is a **greedy left-to-right** approachâ€”early mistakes can cascade!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6f71fe64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effect of Capitalization on POS Tagging\n",
      "=======================================================\n",
      "Sentence                       Word       Tag    Expected\n",
      "-------------------------------------------------------\n",
      "Light the candle.              Light      NNP    VB âœ—\n",
      "light the candle.              light      NN     VB âœ—\n",
      "Please light the candle.       light      VBD    VB âœ—\n"
     ]
    }
   ],
   "source": [
    "# Demonstrating why \"Light\" gets tagged as NNP\n",
    "\n",
    "# Case sensitivity matters!\n",
    "examples = [\n",
    "    \"Light the candle.\",      # Capitalized (sentence start)\n",
    "    \"light the candle.\",      # Lowercase\n",
    "    \"Please light the candle.\", # \"light\" not at start\n",
    "]\n",
    "\n",
    "print(\"Effect of Capitalization on POS Tagging\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"{'Sentence':<30} {'Word':<10} {'Tag':<6} {'Expected'}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for sent in examples:\n",
    "    tokens = word_tokenize(sent)\n",
    "    tagged = pos_tag(tokens)\n",
    "    \n",
    "    # Find 'light' tag\n",
    "    for word, tag in tagged:\n",
    "        if word.lower() == 'light':\n",
    "            expected = 'VB'\n",
    "            status = 'âœ“' if tag == 'VB' else 'âœ—'\n",
    "            print(f\"{sent:<30} {word:<10} {tag:<6} {expected} {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bfbe2c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How Context Influences POS Decisions\n",
      "=================================================================\n",
      "Sentence                       Target   Actual   Expected Match\n",
      "-----------------------------------------------------------------\n",
      "I will light the fire.         light    VB       VB       âœ“\n",
      "The light is dim.              light    NN       NN       âœ“\n",
      "It feels light.                light    NN       JJ       âœ—\n",
      "Turn on the light.             light    NN       NN       âœ“\n"
     ]
    }
   ],
   "source": [
    "# Looking at what features influence tagging decisions\n",
    "# Let's examine how context changes predictions\n",
    "\n",
    "print(\"How Context Influences POS Decisions\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "context_examples = [\n",
    "    # Different preceding words change the prediction\n",
    "    (\"I will light the fire.\", \"light\", \"VB\"),      # Modal 'will' â†’ verb expected\n",
    "    (\"The light is dim.\", \"light\", \"NN\"),           # Determiner 'the' â†’ noun expected  \n",
    "    (\"It feels light.\", \"light\", \"JJ\"),             # 'feels' + adj pattern\n",
    "    (\"Turn on the light.\", \"light\", \"NN\"),          # 'the' + noun pattern\n",
    "]\n",
    "\n",
    "print(f\"{'Sentence':<30} {'Target':<8} {'Actual':<8} {'Expected':<8} {'Match'}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for sent, target, expected in context_examples:\n",
    "    tokens = word_tokenize(sent)\n",
    "    tagged = pos_tag(tokens)\n",
    "    actual = [t for w, t in tagged if w.lower() == target.lower()][0]\n",
    "    match = 'âœ“' if actual == expected else 'âœ—'\n",
    "    print(f\"{sent:<30} {target:<8} {actual:<8} {expected:<8} {match}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17123c8b",
   "metadata": {},
   "source": [
    "### 5. **Understanding Tagger Limitations**\n",
    "\n",
    "| Limitation | Description | Example |\n",
    "|------------|-------------|---------|\n",
    "| **Sentence-initial capitalization** | Words at start look like proper nouns | \"Light the fire\" â†’ Light=NNP âœ— |\n",
    "| **Rare word forms** | Unseen words rely on suffix/shape | Neologisms may be mistagged |\n",
    "| **Garden path sentences** | Ambiguous structure confuses tagger | \"The horse raced past the barn fell\" |\n",
    "| **Domain mismatch** | Trained on news, tested on tweets | Technical jargon may fail |\n",
    "\n",
    "### 6. **Ways to Improve Accuracy**\n",
    "\n",
    "1. **Lowercase sentence-initial words** (if appropriate)\n",
    "2. **Use domain-specific models** for specialized text\n",
    "3. **Combine with other NLP** (NER, parsing) for disambiguation\n",
    "4. **Fine-tune on your data** for better domain accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "81b1edf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Dive: Why 'light' is Hard to Tag Correctly\n",
      "=================================================================\n",
      "Sentence                       Actual   Expected\n",
      "-----------------------------------------------------------------\n",
      "Light the candle.              NNP      VB (imperative verb)\n",
      "light the candle.              NN       VB (imperative verb)\n",
      "Please light the candle.       VBD      VB (verb)\n",
      "Can you light the candle?      VB       VB (verb)\n",
      "I will light the candle.       VB       VB (verb)\n",
      "\n",
      "=================================================================\n",
      "KEY INSIGHT: The tagger struggles with IMPERATIVE sentences!\n",
      "Without a clear subject, it can't identify 'light' as a verb.\n",
      "Adding modal verbs (will, can) provides context â†’ correct VB tag.\n"
     ]
    }
   ],
   "source": [
    "# Examining the tagger's behavior more closely\n",
    "# The perceptron tagger has specific biases we can observe\n",
    "\n",
    "print(\"Deep Dive: Why 'light' is Hard to Tag Correctly\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "problematic = [\n",
    "    (\"Light the candle.\", \"light\", \"VB (imperative verb)\"),\n",
    "    (\"light the candle.\", \"light\", \"VB (imperative verb)\"),\n",
    "    (\"Please light the candle.\", \"light\", \"VB (verb)\"),\n",
    "    (\"Can you light the candle?\", \"light\", \"VB (verb)\"),\n",
    "    (\"I will light the candle.\", \"light\", \"VB (verb)\"),\n",
    "]\n",
    "\n",
    "print(f\"{'Sentence':<30} {'Actual':<8} {'Expected'}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for sent, target, expected in problematic:\n",
    "    tokens = word_tokenize(sent)\n",
    "    tagged = pos_tag(tokens)\n",
    "    actual = [t for w, t in tagged if w.lower() == target][0]\n",
    "    print(f\"{sent:<30} {actual:<8} {expected}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 65)\n",
    "print(\"KEY INSIGHT: The tagger struggles with IMPERATIVE sentences!\")\n",
    "print(\"Without a clear subject, it can't identify 'light' as a verb.\")\n",
    "print(\"Adding modal verbs (will, can) provides context â†’ correct VB tag.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19253a0b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Deep Dive: What the Tagger Actually Looks At\n",
    "\n",
    "### The Chicken-and-Egg Problem\n",
    "\n",
    "You might wonder: *\"If the tagger uses neighboring POS tags, but those neighbors are also ambiguous, how does it decide?\"*\n",
    "\n",
    "**Answer: It processes LEFT-TO-RIGHT and only uses PREVIOUS tags (already decided), never FUTURE tags.**\n",
    "\n",
    "```\n",
    "Sentence:  \"The    light    is    bright\"\n",
    "            â†“       â†“       â†“      â†“\n",
    "Step 1:    DT      ???     ???    ???   (tag \"The\" first)\n",
    "Step 2:    DT  â†’   NN      ???    ???   (use DT to help tag \"light\")\n",
    "Step 3:    DT      NN  â†’   VBZ    ???   (use NN to help tag \"is\")\n",
    "Step 4:    DT      NN      VBZ â†’  JJ    (use VBZ to help tag \"bright\")\n",
    "```\n",
    "\n",
    "This is called **greedy left-to-right decoding**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "34890024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: 'Light the candle'\n",
      "================================================================================\n",
      "\n",
      "Step-by-step feature extraction (LEFT â†’ RIGHT):\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "STEP 1: Tagging word 'Light' at position 0\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸ“‹ FEATURES EXTRACTED:\n",
      "\n",
      "  Word-based features:\n",
      "    word: 'Light'\n",
      "    word.lower(): 'light'\n",
      "    suffix(-3): 'ght'\n",
      "    prefix(3): 'Lig'\n",
      "\n",
      "  Shape features:\n",
      "    is_capitalized: True\n",
      "    is_all_caps: False\n",
      "    is_first_word: True\n",
      "\n",
      "  Context features (ALREADY KNOWN):\n",
      "    prev_word: '<START>'\n",
      "    prev_tag:  '<START>'  â† Already decided!\n",
      "    next_word: 'the' (word known, tag unknown)\n",
      "\n",
      "  ğŸ¯ PREDICTION: 'Light' â†’ NNP\n",
      "     Tags so far: [('Light', 'NNP')]\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "STEP 2: Tagging word 'the' at position 1\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸ“‹ FEATURES EXTRACTED:\n",
      "\n",
      "  Word-based features:\n",
      "    word: 'the'\n",
      "    word.lower(): 'the'\n",
      "    suffix(-3): 'the'\n",
      "    prefix(3): 'the'\n",
      "\n",
      "  Shape features:\n",
      "    is_capitalized: False\n",
      "    is_all_caps: False\n",
      "    is_first_word: False\n",
      "\n",
      "  Context features (ALREADY KNOWN):\n",
      "    prev_word: 'Light'\n",
      "    prev_tag:  'NNP'  â† Already decided!\n",
      "    next_word: 'candle' (word known, tag unknown)\n",
      "\n",
      "  ğŸ¯ PREDICTION: 'the' â†’ DT\n",
      "     Tags so far: [('Light', 'NNP'), ('the', 'DT')]\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "STEP 3: Tagging word 'candle' at position 2\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸ“‹ FEATURES EXTRACTED:\n",
      "\n",
      "  Word-based features:\n",
      "    word: 'candle'\n",
      "    word.lower(): 'candle'\n",
      "    suffix(-3): 'dle'\n",
      "    prefix(3): 'can'\n",
      "\n",
      "  Shape features:\n",
      "    is_capitalized: False\n",
      "    is_all_caps: False\n",
      "    is_first_word: False\n",
      "\n",
      "  Context features (ALREADY KNOWN):\n",
      "    prev_word: 'the'\n",
      "    prev_tag:  'DT'  â† Already decided!\n",
      "    next_word: '<END>' (word known, tag unknown)\n",
      "\n",
      "  ğŸ¯ PREDICTION: 'candle' â†’ NN\n",
      "     Tags so far: [('Light', 'NNP'), ('the', 'DT'), ('candle', 'NN')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Light', 'NNP'), ('the', 'DT'), ('candle', 'NN')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's visualize exactly what features the tagger extracts for each word\n",
    "# This simulates the actual feature extraction process\n",
    "\n",
    "def show_tagger_features(sentence):\n",
    "    \"\"\"\n",
    "    Simulate the features that NLTK's averaged perceptron tagger extracts.\n",
    "    Based on the actual implementation in nltk/tag/perceptron.py\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(sentence)\n",
    "    \n",
    "    print(f\"Sentence: '{sentence}'\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\nStep-by-step feature extraction (LEFT â†’ RIGHT):\\n\")\n",
    "    \n",
    "    # Simulate left-to-right tagging\n",
    "    predicted_tags = []\n",
    "    \n",
    "    for i, word in enumerate(tokens):\n",
    "        print(f\"{'â”€' * 80}\")\n",
    "        print(f\"STEP {i+1}: Tagging word '{word}' at position {i}\")\n",
    "        print(f\"{'â”€' * 80}\")\n",
    "        \n",
    "        # Features the tagger actually uses:\n",
    "        features = {}\n",
    "        \n",
    "        # 1. Word features\n",
    "        features['word'] = word\n",
    "        features['word.lower()'] = word.lower()\n",
    "        features['suffix(-3)'] = word[-3:] if len(word) >= 3 else word\n",
    "        features['suffix(-2)'] = word[-2:] if len(word) >= 2 else word\n",
    "        features['suffix(-1)'] = word[-1:] if len(word) >= 1 else word\n",
    "        features['prefix(1)'] = word[0] if len(word) >= 1 else ''\n",
    "        features['prefix(2)'] = word[:2] if len(word) >= 2 else word\n",
    "        features['prefix(3)'] = word[:3] if len(word) >= 3 else word\n",
    "        \n",
    "        # 2. Word shape features\n",
    "        features['is_capitalized'] = word[0].isupper() if word else False\n",
    "        features['is_all_caps'] = word.isupper()\n",
    "        features['is_all_lower'] = word.islower()\n",
    "        features['has_digit'] = any(c.isdigit() for c in word)\n",
    "        features['has_hyphen'] = '-' in word\n",
    "        \n",
    "        # 3. Position features\n",
    "        features['is_first_word'] = (i == 0)\n",
    "        features['is_last_word'] = (i == len(tokens) - 1)\n",
    "        \n",
    "        # 4. Context features (PREVIOUS words - already known!)\n",
    "        features['prev_word'] = tokens[i-1] if i > 0 else '<START>'\n",
    "        features['prev_word(-2)'] = tokens[i-2] if i > 1 else '<START>'\n",
    "        \n",
    "        # 5. PREVIOUS TAG features (already decided in earlier steps!)\n",
    "        features['prev_tag'] = predicted_tags[i-1] if i > 0 else '<START>'\n",
    "        features['prev_tag(-2)'] = predicted_tags[i-2] if i > 1 else '<START>'\n",
    "        \n",
    "        # 6. Next word (lookahead - words are known, but NOT their tags!)\n",
    "        features['next_word'] = tokens[i+1] if i < len(tokens)-1 else '<END>'\n",
    "        \n",
    "        # Print features\n",
    "        print(\"\\nğŸ“‹ FEATURES EXTRACTED:\")\n",
    "        print(\"\\n  Word-based features:\")\n",
    "        for k in ['word', 'word.lower()', 'suffix(-3)', 'prefix(3)']:\n",
    "            print(f\"    {k}: '{features[k]}'\")\n",
    "        \n",
    "        print(\"\\n  Shape features:\")\n",
    "        for k in ['is_capitalized', 'is_all_caps', 'is_first_word']:\n",
    "            print(f\"    {k}: {features[k]}\")\n",
    "        \n",
    "        print(\"\\n  Context features (ALREADY KNOWN):\")\n",
    "        print(f\"    prev_word: '{features['prev_word']}'\")\n",
    "        print(f\"    prev_tag:  '{features['prev_tag']}'  â† Already decided!\")\n",
    "        print(f\"    next_word: '{features['next_word']}' (word known, tag unknown)\")\n",
    "        \n",
    "        # Get actual prediction\n",
    "        actual_tags = pos_tag(tokens)\n",
    "        current_tag = actual_tags[i][1]\n",
    "        predicted_tags.append(current_tag)\n",
    "        \n",
    "        print(f\"\\n  ğŸ¯ PREDICTION: '{word}' â†’ {current_tag}\")\n",
    "        print(f\"     Tags so far: {list(zip(tokens[:i+1], predicted_tags))}\")\n",
    "    \n",
    "    return list(zip(tokens, predicted_tags))\n",
    "\n",
    "# Run the visualization\n",
    "show_tagger_features(\"Light the candle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c93e87fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How Previous Tags Cascade Through the Sentence\n",
      "======================================================================\n",
      "\n",
      "'The light is bright'\n",
      "--------------------------------------------------\n",
      "Step   Word       Prev Tag     â†’ Current Tag\n",
      "--------------------------------------------------\n",
      "1      The        <START>      â†’ DT\n",
      "2      light      DT           â†’ NN\n",
      "3      is         NN           â†’ VBZ\n",
      "4      bright     VBZ          â†’ JJ\n",
      "\n",
      "'I will light the candle'\n",
      "--------------------------------------------------\n",
      "Step   Word       Prev Tag     â†’ Current Tag\n",
      "--------------------------------------------------\n",
      "1      I          <START>      â†’ PRP\n",
      "2      will       PRP          â†’ MD\n",
      "3      light      MD           â†’ VB\n",
      "4      the        VB           â†’ DT\n",
      "5      candle     DT           â†’ NN\n",
      "\n",
      "======================================================================\n",
      "NOTICE: Each tag decision uses the PREVIOUS tag as a feature!\n",
      "The tagger never looks at FUTURE tags (they don't exist yet).\n"
     ]
    }
   ],
   "source": [
    "# Compare how PREVIOUS TAG influences decisions\n",
    "print(\"How Previous Tags Cascade Through the Sentence\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "examples = [\n",
    "    \"The light is bright\",      # DT â†’ NN â†’ VBZ â†’ JJ\n",
    "    \"I will light the candle\",  # PRP â†’ MD â†’ VB â†’ DT â†’ NN\n",
    "]\n",
    "\n",
    "for sent in examples:\n",
    "    tokens = word_tokenize(sent)\n",
    "    tagged = pos_tag(tokens)\n",
    "    \n",
    "    print(f\"\\n'{sent}'\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'Step':<6} {'Word':<10} {'Prev Tag':<12} {'â†’ Current Tag'}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for i, (word, tag) in enumerate(tagged):\n",
    "        prev_tag = tagged[i-1][1] if i > 0 else '<START>'\n",
    "        print(f\"{i+1:<6} {word:<10} {prev_tag:<12} â†’ {tag}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"NOTICE: Each tag decision uses the PREVIOUS tag as a feature!\")\n",
    "print(\"The tagger never looks at FUTURE tags (they don't exist yet).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480c5a4a",
   "metadata": {},
   "source": [
    "### Complete Feature List Used by NLTK's Averaged Perceptron Tagger\n",
    "\n",
    "The tagger extracts these features for each word at position `i`:\n",
    "\n",
    "| Category | Feature | Description | Example for \"light\" |\n",
    "|----------|---------|-------------|---------------------|\n",
    "| **Word** | `word` | The exact word | \"light\" |\n",
    "| | `word.lower()` | Lowercased | \"light\" |\n",
    "| | `suffix(-1,-2,-3)` | Last 1/2/3 chars | \"t\", \"ht\", \"ght\" |\n",
    "| | `prefix(1,2,3)` | First 1/2/3 chars | \"l\", \"li\", \"lig\" |\n",
    "| **Shape** | `is_upper` | Starts uppercase? | True (if \"Light\") |\n",
    "| | `is_title` | Title case? | True |\n",
    "| | `is_digit` | Contains digits? | False |\n",
    "| | `has_hyphen` | Contains hyphen? | False |\n",
    "| **Position** | `i == 0` | First word? | Depends |\n",
    "| | `i == len-1` | Last word? | Depends |\n",
    "| **Context** | `word[i-1]` | Previous word | \"The\" or \"will\" |\n",
    "| | `word[i-2]` | Word 2 back | varies |\n",
    "| | `word[i+1]` | Next word | \"the\" |\n",
    "| **Tags** | `tag[i-1]` | Previous tag | DT or MD |\n",
    "| | `tag[i-2]` | Tag 2 back | varies |\n",
    "| | `tag[i-1] + tag[i-2]` | Tag bigram | \"MD+PRP\" |\n",
    "\n",
    "### âš ï¸ Key Insight: NO FUTURE TAGS!\n",
    "\n",
    "Notice there's **no `tag[i+1]`** (next tag) feature because:\n",
    "- Tags are assigned **left-to-right**\n",
    "- Future tags **don't exist** when deciding current tag\n",
    "- Only previous tags (already decided) can be used\n",
    "\n",
    "This is why **early mistakes cascade**â€”a wrong tag at position 0 affects all subsequent predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bb2b0fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error Cascading Demo\n",
      "======================================================================\n",
      "\n",
      "When the first word is mistagged, it can affect the entire sentence.\n",
      "\n",
      "Common Tag Transition Patterns (from training data):\n",
      "--------------------------------------------------\n",
      "If prev_tag is...    Next tag likely...\n",
      "--------------------------------------------------\n",
      "<START>              ['DT', 'PRP', 'NNP', 'RB', 'VB']\n",
      "DT                   ['NN', 'JJ', 'NNP', 'RB']\n",
      "MD                   ['VB', 'RB']\n",
      "PRP                  ['VBP', 'VBD', 'MD', 'VBZ']\n",
      "NNP                  ['NNP', 'VBZ', 'VBD', ',']\n",
      "JJ                   ['NN', 'NNS', 'JJ', 'CC']\n",
      "VB                   ['DT', 'PRP', 'RB', 'TO']\n",
      "NN                   ['VBZ', 'VBD', 'IN', 'CC', '.']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Now see how 'Light the candle' gets mistagged:\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Step 1: Tag 'Light'\n",
      "   - Position: 0 (first word)\n",
      "   - prev_tag: <START>\n",
      "   - Features: capitalized=True, word='Light'\n",
      "   - Problem: Capitalization + sentence-start â†’ model guesses NNP\n",
      "   - Result: 'Light' â†’ NNP âŒ (should be VB)\n",
      "\n",
      "Step 2: Tag 'the'\n",
      "   - prev_tag: NNP (wrong, but tagger doesn't know!)\n",
      "   - After NNP, model expects: more nouns, verbs, punctuation\n",
      "   - Result: 'the' â†’ DT âœ“ (still correct, \"the\" is unambiguous)\n",
      "\n",
      "Step 3: Tag 'candle'\n",
      "   - prev_tag: DT\n",
      "   - After DT, model expects: noun or adjective\n",
      "   - Result: 'candle' â†’ NN âœ“\n",
      "\n",
      "Sentence tagged as: NNP DT NN (instead of VB DT NN)\n",
      "The error at position 0 cascaded but luckily didn't break everything!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate error cascading: early mistakes affect later predictions\n",
    "print(\"Error Cascading Demo\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nWhen the first word is mistagged, it can affect the entire sentence.\\n\")\n",
    "\n",
    "# Simulate what happens with different \"fake\" previous tags\n",
    "def simulate_tag_influence():\n",
    "    \"\"\"\n",
    "    Show how previous tag patterns influence predictions.\n",
    "    Common tag transition patterns learned from training data:\n",
    "    \"\"\"\n",
    "    transitions = {\n",
    "        # Previous tag â†’ likely current tags (from training statistics)\n",
    "        '<START>': ['DT', 'PRP', 'NNP', 'RB', 'VB'],\n",
    "        'DT': ['NN', 'JJ', 'NNP', 'RB'],      # After \"the\" â†’ noun/adj likely\n",
    "        'MD': ['VB', 'RB'],                    # After \"will/can\" â†’ verb likely\n",
    "        'PRP': ['VBP', 'VBD', 'MD', 'VBZ'],   # After \"I/you\" â†’ verb likely\n",
    "        'NNP': ['NNP', 'VBZ', 'VBD', ','],    # After proper noun â†’ verb/more nouns\n",
    "        'JJ': ['NN', 'NNS', 'JJ', 'CC'],      # After adjective â†’ noun likely\n",
    "        'VB': ['DT', 'PRP', 'RB', 'TO'],      # After verb â†’ object/adverb\n",
    "        'NN': ['VBZ', 'VBD', 'IN', 'CC', '.'], # After noun â†’ verb/prep\n",
    "    }\n",
    "    \n",
    "    print(\"Common Tag Transition Patterns (from training data):\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'If prev_tag is...':<20} {'Next tag likely...'}\")\n",
    "    print(\"-\" * 50)\n",
    "    for prev, nexts in transitions.items():\n",
    "        print(f\"{prev:<20} {nexts}\")\n",
    "    \n",
    "    return transitions\n",
    "\n",
    "transitions = simulate_tag_influence()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"\\nNow see how 'Light the candle' gets mistagged:\")\n",
    "print(\"-\" * 70)\n",
    "print(\"\"\"\n",
    "Step 1: Tag 'Light'\n",
    "   - Position: 0 (first word)\n",
    "   - prev_tag: <START>\n",
    "   - Features: capitalized=True, word='Light'\n",
    "   - Problem: Capitalization + sentence-start â†’ model guesses NNP\n",
    "   - Result: 'Light' â†’ NNP âŒ (should be VB)\n",
    "\n",
    "Step 2: Tag 'the'\n",
    "   - prev_tag: NNP (wrong, but tagger doesn't know!)\n",
    "   - After NNP, model expects: more nouns, verbs, punctuation\n",
    "   - Result: 'the' â†’ DT âœ“ (still correct, \"the\" is unambiguous)\n",
    "\n",
    "Step 3: Tag 'candle'\n",
    "   - prev_tag: DT\n",
    "   - After DT, model expects: noun or adjective\n",
    "   - Result: 'candle' â†’ NN âœ“\n",
    "\n",
    "Sentence tagged as: NNP DT NN (instead of VB DT NN)\n",
    "The error at position 0 cascaded but luckily didn't break everything!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f929e2b8",
   "metadata": {},
   "source": [
    "### Alternative Approaches: How Other Taggers Solve This\n",
    "\n",
    "| Approach | How it works | Pros | Cons |\n",
    "|----------|--------------|------|------|\n",
    "| **Greedy Lâ†’R** (NLTK default) | Tag one word at a time, use previous tags | Fast, simple | Errors cascade |\n",
    "| **Viterbi/HMM** | Find globally optimal tag sequence | Better accuracy | Slower, limited features |\n",
    "| **Bidirectional LSTM** | Neural network sees full sentence | State-of-the-art | Requires GPU, large model |\n",
    "| **Transformer (BERT)** | Attention over entire sentence | Best accuracy | Slow, resource-heavy |\n",
    "\n",
    "NLTK's perceptron tagger uses the **greedy approach** for speed, accepting some accuracy loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904e5f2a",
   "metadata": {},
   "source": [
    "## 7.6 Extracting Words by POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63b3c9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: The beautiful princess quickly ran through the dark forest.\n",
      "She was searching for her magical golden crown.\n",
      "\n",
      "Nouns: ['princess', 'forest', 'crown']\n",
      "Verbs: ['ran', 'was', 'searching']\n",
      "Adjectives: ['beautiful', 'dark', 'magical', 'golden']\n",
      "Adverbs: ['quickly']\n"
     ]
    }
   ],
   "source": [
    "def extract_by_pos(text, target_tags):\n",
    "    \"\"\"Extract words with specific POS tags\"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged = pos_tag(tokens)\n",
    "    return [word for word, tag in tagged if tag in target_tags]\n",
    "\n",
    "text = \"\"\"The beautiful princess quickly ran through the dark forest.\n",
    "She was searching for her magical golden crown.\"\"\"\n",
    "\n",
    "print(f\"Text: {text}\")\n",
    "\n",
    "# Extract different parts of speech\n",
    "nouns = extract_by_pos(text, ['NN', 'NNS', 'NNP', 'NNPS'])\n",
    "verbs = extract_by_pos(text, ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'])\n",
    "adjectives = extract_by_pos(text, ['JJ', 'JJR', 'JJS'])\n",
    "adverbs = extract_by_pos(text, ['RB', 'RBR', 'RBS'])\n",
    "\n",
    "print(f\"\\nNouns: {nouns}\")\n",
    "print(f\"Verbs: {verbs}\")\n",
    "print(f\"Adjectives: {adjectives}\")\n",
    "print(f\"Adverbs: {adverbs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b54b7883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tag Distribution\n",
      "==============================\n",
      "NN       7 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "JJ       3 â–ˆâ–ˆâ–ˆ\n",
      "VBG      2 â–ˆâ–ˆ\n",
      "NNS      2 â–ˆâ–ˆ\n",
      "VBP      2 â–ˆâ–ˆ\n",
      "CC       2 â–ˆâ–ˆ\n",
      ".        2 â–ˆâ–ˆ\n",
      "VBZ      1 â–ˆ\n",
      "WRB      1 â–ˆ\n",
      "RB       1 â–ˆ\n"
     ]
    }
   ],
   "source": [
    "# POS distribution\n",
    "from collections import Counter\n",
    "\n",
    "def pos_distribution(text):\n",
    "    \"\"\"Get distribution of POS tags\"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged = pos_tag(tokens)\n",
    "    return Counter(tag for word, tag in tagged)\n",
    "\n",
    "text = \"\"\"Machine learning is transforming how computers understand and process \n",
    "human language. Natural language processing applications are becoming \n",
    "increasingly sophisticated and accurate.\"\"\"\n",
    "\n",
    "dist = pos_distribution(text)\n",
    "\n",
    "print(\"POS Tag Distribution\")\n",
    "print(\"=\" * 30)\n",
    "for tag, count in dist.most_common():\n",
    "    print(f\"{tag:<6} {count:>3} {'â–ˆ' * count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6038703",
   "metadata": {},
   "source": [
    "## 7.7 Custom POS Taggers\n",
    "\n",
    "NLTK provides several tagger classes that you can train on your own data. Understanding these helps you:\n",
    "1. **Customize** taggers for specific domains (medical, legal, social media)\n",
    "2. **Understand** how statistical tagging works under the hood\n",
    "3. **Improve** accuracy by combining multiple approaches\n",
    "\n",
    "### Tagger Hierarchy (from simple to complex)\n",
    "\n",
    "```\n",
    "DefaultTagger (baseline)\n",
    "    â†“ backoff\n",
    "UnigramTagger (word â†’ tag lookup)\n",
    "    â†“ backoff  \n",
    "BigramTagger (prev_tag + word â†’ tag)\n",
    "    â†“ backoff\n",
    "TrigramTagger (prev_2_tags + word â†’ tag)\n",
    "```\n",
    "\n",
    "**Backoff Chain**: When a tagger can't make a decision, it \"backs off\" to a simpler tagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0bb5df11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sentences: 3698\n",
      "Test sentences: 925\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import DefaultTagger, UnigramTagger, BigramTagger, TrigramTagger\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# Get tagged sentences from Brown corpus\n",
    "brown_tagged = brown.tagged_sents(categories='news')\n",
    "\n",
    "# Split into train/test\n",
    "train_size = int(len(brown_tagged) * 0.8)\n",
    "train_sents = brown_tagged[:train_size]\n",
    "test_sents = brown_tagged[train_size:]\n",
    "\n",
    "print(f\"Training sentences: {len(train_sents)}\")\n",
    "print(f\"Test sentences: {len(test_sents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ea303f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PEEK INTO TRAINING DATA\n",
      "======================================================================\n",
      "\n",
      "Training set: 3698 sentences\n",
      "Test set: 925 sentences\n",
      "\n",
      "Sample Training Sentence #1:\n",
      "--------------------------------------------------\n",
      "[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ('Grand', 'JJ-TL'), ('Jury', 'NN-TL'), ('said', 'VBD'), ('Friday', 'NR'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'IN'), (\"Atlanta's\", 'NP$'), ('recent', 'JJ'), ('primary', 'NN'), ('election', 'NN'), ('produced', 'VBD'), ('``', '``'), ('no', 'AT'), ('evidence', 'NN'), (\"''\", \"''\"), ('that', 'CS'), ('any', 'DTI'), ('irregularities', 'NNS'), ('took', 'VBD'), ('place', 'NN'), ('.', '.')]\n",
      "\n",
      "Formatted view:\n",
      "  'The' â†’ AT\n",
      "  'Fulton' â†’ NP-TL\n",
      "  'County' â†’ NN-TL\n",
      "  'Grand' â†’ JJ-TL\n",
      "  'Jury' â†’ NN-TL\n",
      "  'said' â†’ VBD\n",
      "  'Friday' â†’ NR\n",
      "  'an' â†’ AT\n",
      "  'investigation' â†’ NN\n",
      "  'of' â†’ IN\n",
      "  ... (15 more words)\n",
      "\n",
      "--------------------------------------------------\n",
      "Sample Training Sentence #2:\n",
      "--------------------------------------------------\n",
      "[('The', 'AT'), ('jury', 'NN'), ('further', 'RBR'), ('said', 'VBD'), ('in', 'IN'), ('term-end', 'NN'), ('presentments', 'NNS'), ('that', 'CS'), ('the', 'AT'), ('City', 'NN-TL'), ('Executive', 'JJ-TL'), ('Committee', 'NN-TL'), (',', ','), ('which', 'WDT'), ('had', 'HVD'), ('over-all', 'JJ'), ('charge', 'NN'), ('of', 'IN'), ('the', 'AT'), ('election', 'NN'), (',', ','), ('``', '``'), ('deserves', 'VBZ'), ('the', 'AT'), ('praise', 'NN'), ('and', 'CC'), ('thanks', 'NNS'), ('of', 'IN'), ('the', 'AT'), ('City', 'NN-TL'), ('of', 'IN-TL'), ('Atlanta', 'NP-TL'), (\"''\", \"''\"), ('for', 'IN'), ('the', 'AT'), ('manner', 'NN'), ('in', 'IN'), ('which', 'WDT'), ('the', 'AT'), ('election', 'NN'), ('was', 'BEDZ'), ('conducted', 'VBN'), ('.', '.')]\n",
      "\n",
      "Formatted view:\n",
      "  'The' â†’ AT\n",
      "  'jury' â†’ NN\n",
      "  'further' â†’ RBR\n",
      "  'said' â†’ VBD\n",
      "  'in' â†’ IN\n",
      "  'term-end' â†’ NN\n",
      "  'presentments' â†’ NNS\n",
      "  'that' â†’ CS\n",
      "  'the' â†’ AT\n",
      "  'City' â†’ NN-TL\n"
     ]
    }
   ],
   "source": [
    "# Peek into the training data\n",
    "print(\"=\" * 70)\n",
    "print(\"PEEK INTO TRAINING DATA\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nTraining set: {len(train_sents)} sentences\")\n",
    "print(f\"Test set: {len(test_sents)} sentences\\n\")\n",
    "\n",
    "print(\"Sample Training Sentence #1:\")\n",
    "print(\"-\" * 50)\n",
    "print(train_sents[0])\n",
    "print(\"\\nFormatted view:\")\n",
    "for word, tag in train_sents[0][:10]:  # First 10 words\n",
    "    print(f\"  '{word}' â†’ {tag}\")\n",
    "if len(train_sents[0]) > 10:\n",
    "    print(f\"  ... ({len(train_sents[0]) - 10} more words)\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "print(\"Sample Training Sentence #2:\")\n",
    "print(\"-\" * 50)\n",
    "print(train_sents[1])\n",
    "print(\"\\nFormatted view:\")\n",
    "for word, tag in train_sents[1][:10]:\n",
    "    print(f\"  '{word}' â†’ {tag}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6b651745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PEEK INTO TEST DATA: Predictions vs Gold Standard\n",
      "======================================================================\n",
      "\n",
      "Test Sentence #6:\n",
      "  'It is now disclosed that the taxpayer not only pays for high wages , but he pays the employers' strike expenses when the latter undertakes to fight a strike .'\n",
      "\n",
      "Word            Gold     Default  Unigram  Bigram   Trigram \n",
      "---------------------------------------------------------------------------\n",
      "It              PPS      NN    âœ— PPS   âœ“ PPS   âœ“ PPS   âœ“\n",
      "is              BEZ      NN    âœ— BEZ   âœ“ BEZ   âœ“ BEZ   âœ“\n",
      "now             RB       NN    âœ— RB    âœ“ RB    âœ“ RB    âœ“\n",
      "disclosed       VBN      NN    âœ— VBD   âœ— VBD   âœ— VBD   âœ—\n",
      "that            CS       NN    âœ— CS    âœ“ CS    âœ“ CS    âœ“\n",
      "the             AT       NN    âœ— AT    âœ“ AT    âœ“ AT    âœ“\n",
      "taxpayer        NN       NN    âœ“ NN    âœ“ NN    âœ“ NN    âœ“\n",
      "not             *        NN    âœ— *     âœ“ *     âœ“ *     âœ“\n",
      "only            RB       NN    âœ— AP    âœ— RB    âœ“ RB    âœ“\n",
      "pays            VBZ      NN    âœ— VBZ   âœ“ VBZ   âœ“ VBZ   âœ“\n",
      "for             IN       NN    âœ— IN    âœ“ IN    âœ“ IN    âœ“\n",
      "high            JJ       NN    âœ— JJ    âœ“ JJ    âœ“ JJ    âœ“\n",
      "wages           NNS      NN    âœ— NNS   âœ“ NNS   âœ“ NNS   âœ“\n",
      ",               ,        NN    âœ— ,     âœ“ ,     âœ“ ,     âœ“\n",
      "but             CC       NN    âœ— CC    âœ“ CC    âœ“ CC    âœ“\n",
      "he              PPS      NN    âœ— PPS   âœ“ PPS   âœ“ PPS   âœ“\n",
      "pays            VBZ      NN    âœ— VBZ   âœ“ VBZ   âœ“ VBZ   âœ“\n",
      "the             AT       NN    âœ— AT    âœ“ AT    âœ“ AT    âœ“\n",
      "employers'      NNS$     NN    âœ— NN    âœ— NN    âœ— NN    âœ—\n",
      "strike          NN       NN    âœ“ NN    âœ“ NN    âœ“ NN    âœ“\n",
      "expenses        NNS      NN    âœ— NNS   âœ“ NNS   âœ“ NNS   âœ“\n",
      "when            WRB      NN    âœ— WRB   âœ“ WRB   âœ“ WRB   âœ“\n",
      "the             AT       NN    âœ— AT    âœ“ AT    âœ“ AT    âœ“\n",
      "latter          AP       NN    âœ— AP    âœ“ AP    âœ“ AP    âœ“\n",
      "undertakes      VBZ      NN    âœ— NN    âœ— NN    âœ— NN    âœ—\n",
      "to              TO       NN    âœ— TO    âœ“ TO    âœ“ TO    âœ“\n",
      "fight           VB       NN    âœ— NN    âœ— NN    âœ— NN    âœ—\n",
      "a               AT       NN    âœ— AT    âœ“ AT    âœ“ AT    âœ“\n",
      "strike          NN       NN    âœ“ NN    âœ“ NN    âœ“ NN    âœ“\n",
      ".               .        NN    âœ— .     âœ“ .     âœ“ .     âœ“\n"
     ]
    }
   ],
   "source": [
    "# Peek into the TEST data and see predictions vs gold standard\n",
    "print(\"=\" * 70)\n",
    "print(\"PEEK INTO TEST DATA: Predictions vs Gold Standard\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# First, train all taggers\n",
    "default_tagger = DefaultTagger('NN')\n",
    "unigram_tagger = UnigramTagger(train_sents, backoff=default_tagger)\n",
    "bigram_tagger = BigramTagger(train_sents, backoff=unigram_tagger)\n",
    "trigram_tagger = TrigramTagger(train_sents, backoff=bigram_tagger)\n",
    "\n",
    "# Pick a test sentence\n",
    "test_sent = test_sents[5]  # Pick sentence #6 from test set\n",
    "words = [word for word, tag in test_sent]\n",
    "gold_tags = [tag for word, tag in test_sent]\n",
    "\n",
    "# Get predictions from each tagger\n",
    "predictions = {\n",
    "    'Default': default_tagger.tag(words),\n",
    "    'Unigram': unigram_tagger.tag(words),\n",
    "    'Bigram': bigram_tagger.tag(words),\n",
    "    'Trigram': trigram_tagger.tag(words),\n",
    "}\n",
    "\n",
    "print(f\"\\nTest Sentence #{6}:\")\n",
    "print(f\"  '{' '.join(words)}'\\n\")\n",
    "\n",
    "print(f\"{'Word':<15} {'Gold':<8} {'Default':<8} {'Unigram':<8} {'Bigram':<8} {'Trigram':<8}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for i, (word, gold) in enumerate(test_sent):\n",
    "    def_tag = predictions['Default'][i][1]\n",
    "    uni_tag = predictions['Unigram'][i][1]\n",
    "    bi_tag = predictions['Bigram'][i][1]\n",
    "    tri_tag = predictions['Trigram'][i][1]\n",
    "    \n",
    "    # Mark correct predictions with âœ“\n",
    "    def_mark = 'âœ“' if def_tag == gold else 'âœ—'\n",
    "    uni_mark = 'âœ“' if uni_tag == gold else 'âœ—'\n",
    "    bi_mark = 'âœ“' if bi_tag == gold else 'âœ—'\n",
    "    tri_mark = 'âœ“' if tri_tag == gold else 'âœ—'\n",
    "    \n",
    "    print(f\"{word:<15} {gold:<8} {def_tag:<6}{def_mark} {uni_tag:<6}{uni_mark} {bi_tag:<6}{bi_mark} {tri_tag:<6}{tri_mark}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b5de5e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ERROR ANALYSIS: Where Taggers Go Wrong\n",
      "======================================================================\n",
      "\n",
      "Error counts (first 50 test sentences):\n",
      "  Default: 1137 errors\n",
      "  Unigram: 224 errors\n",
      "  Bigram: 217 errors\n",
      "  Trigram: 218 errors\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Sample Errors from TrigramTagger (our best tagger):\n",
      "----------------------------------------------------------------------\n",
      "Word            Gold     Predicted Context\n",
      "----------------------------------------------------------------------\n",
      "fallacious      JJ       NN       ...more the fallacious equation is...\n",
      "argue           VB       NN       ...advanced to argue that since...\n",
      "since           CS       IN       ...argue that since business is...\n",
      "restricted      VBN      NN       ...business is restricted under the...\n",
      "Or              CC       NN       ...Or , in...\n",
      "Anatole         NP       NN       ...words of Anatole France ,...\n",
      "The             AT       AT-TL    ..., `` The law in...\n",
      "forbid          VB       NN       ...equality must forbid the rich...\n",
      "as              QL       CS       ...rich , as well as...\n",
      "begging         VBG      NN       ..., from begging in the...\n"
     ]
    }
   ],
   "source": [
    "# Let's examine where taggers DISAGREE and make ERRORS\n",
    "print(\"=\" * 70)\n",
    "print(\"ERROR ANALYSIS: Where Taggers Go Wrong\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Analyze errors across multiple test sentences\n",
    "errors_by_tagger = {'Default': [], 'Unigram': [], 'Bigram': [], 'Trigram': []}\n",
    "\n",
    "for sent in test_sents[:50]:  # Check first 50 test sentences\n",
    "    words = [w for w, t in sent]\n",
    "    gold = [t for w, t in sent]\n",
    "    \n",
    "    preds = {\n",
    "        'Default': [t for w, t in default_tagger.tag(words)],\n",
    "        'Unigram': [t for w, t in unigram_tagger.tag(words)],\n",
    "        'Bigram': [t for w, t in bigram_tagger.tag(words)],\n",
    "        'Trigram': [t for w, t in trigram_tagger.tag(words)],\n",
    "    }\n",
    "    \n",
    "    for i, (word, gold_tag) in enumerate(sent):\n",
    "        for tagger_name, pred_tags in preds.items():\n",
    "            if pred_tags[i] != gold_tag:\n",
    "                errors_by_tagger[tagger_name].append({\n",
    "                    'word': word,\n",
    "                    'gold': gold_tag,\n",
    "                    'predicted': pred_tags[i],\n",
    "                    'context': ' '.join(words[max(0,i-2):i+3])\n",
    "                })\n",
    "\n",
    "print(\"\\nError counts (first 50 test sentences):\")\n",
    "for name, errors in errors_by_tagger.items():\n",
    "    print(f\"  {name}: {len(errors)} errors\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"Sample Errors from TrigramTagger (our best tagger):\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Word':<15} {'Gold':<8} {'Predicted':<8} {'Context'}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for err in errors_by_tagger['Trigram'][:10]:\n",
    "    print(f\"{err['word']:<15} {err['gold']:<8} {err['predicted']:<8} ...{err['context']}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "57f345f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CONFUSION ANALYSIS: Most Common Mistakes\n",
      "======================================================================\n",
      "\n",
      "Top 15 Most Common Tag Confusions (TrigramTagger):\n",
      "--------------------------------------------------\n",
      "Gold Tag     Predicted    Count    Explanation\n",
      "--------------------------------------------------\n",
      "NP           NN           376      \n",
      "JJ           NN           362      Adjective vs noun\n",
      "NNS          NN           361      \n",
      "VB           NN           169      Verb vs noun (ambiguous)\n",
      "VBG          NN           111      \n",
      "VBD          NN           107      \n",
      "IN           TO           107      \n",
      "VBN          NN           104      \n",
      "VBD          VBN          99       Past tense vs past participle\n",
      "NN-TL        NN           96       \n",
      "VBN          VBD          83       Past participle vs past tense\n",
      "TO           IN           83       \n",
      "CD           NN           83       \n",
      "RB           NN           79       \n",
      "NP$          NN           47       \n"
     ]
    }
   ],
   "source": [
    "# Confusion matrix: What tags get confused with what?\n",
    "from collections import Counter\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"CONFUSION ANALYSIS: Most Common Mistakes\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Collect all (gold, predicted) pairs for trigram tagger\n",
    "confusions = []\n",
    "for sent in test_sents:\n",
    "    words = [w for w, t in sent]\n",
    "    gold_tags = [t for w, t in sent]\n",
    "    pred_tags = [t for w, t in trigram_tagger.tag(words)]\n",
    "    \n",
    "    for gold, pred in zip(gold_tags, pred_tags):\n",
    "        if gold != pred:\n",
    "            confusions.append((gold, pred))\n",
    "\n",
    "confusion_counts = Counter(confusions)\n",
    "\n",
    "print(\"\\nTop 15 Most Common Tag Confusions (TrigramTagger):\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Gold Tag':<12} {'Predicted':<12} {'Count':<8} {'Explanation'}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "tag_explanations = {\n",
    "    ('VBN', 'VBD'): 'Past participle vs past tense',\n",
    "    ('VBD', 'VBN'): 'Past tense vs past participle', \n",
    "    ('NN', 'VB'): 'Noun vs verb (ambiguous)',\n",
    "    ('VB', 'NN'): 'Verb vs noun (ambiguous)',\n",
    "    ('JJ', 'NN'): 'Adjective vs noun',\n",
    "    ('NN', 'JJ'): 'Noun vs adjective',\n",
    "    ('NNP', 'NN'): 'Proper noun vs common noun',\n",
    "    ('RB', 'RP'): 'Adverb vs particle',\n",
    "    ('IN', 'RB'): 'Preposition vs adverb',\n",
    "}\n",
    "\n",
    "for (gold, pred), count in confusion_counts.most_common(15):\n",
    "    explanation = tag_explanations.get((gold, pred), '')\n",
    "    print(f\"{gold:<12} {pred:<12} {count:<8} {explanation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8421245",
   "metadata": {},
   "source": [
    "### 7.7.1 DefaultTagger - The Simplest Baseline\n",
    "\n",
    "The `DefaultTagger` assigns the **same tag to every word**. It's not useful alone but serves as the final fallback in a backoff chain.\n",
    "\n",
    "**How it works:**\n",
    "```\n",
    "For ANY word â†’ return 'NN' (or whatever default tag you set)\n",
    "```\n",
    "\n",
    "**Why use it?** \n",
    "- Handles unknown words that other taggers haven't seen\n",
    "- 'NN' is a good default because nouns are the most common open-class words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da5d43d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Tagger accuracy: 12.10%\n"
     ]
    }
   ],
   "source": [
    "# Default Tagger (assigns same tag to everything)\n",
    "default_tagger = DefaultTagger('NN')\n",
    "print(f\"Default Tagger accuracy: {default_tagger.accuracy(test_sents):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8b0bc76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DefaultTagger('NN') in action:\n",
      "Input:  ['The', 'cat', 'quickly', 'ran', 'away']\n",
      "Output: [('The', 'NN'), ('cat', 'NN'), ('quickly', 'NN'), ('ran', 'NN'), ('away', 'NN')]\n",
      "\n",
      "Notice: Even 'The' (determiner) and 'quickly' (adverb) get tagged as NN!\n"
     ]
    }
   ],
   "source": [
    "# See how DefaultTagger works\n",
    "default_tagger = DefaultTagger('NN')\n",
    "\n",
    "# It tags EVERYTHING as NN\n",
    "test_sentence = ['The', 'cat', 'quickly', 'ran', 'away']\n",
    "print(\"DefaultTagger('NN') in action:\")\n",
    "print(f\"Input:  {test_sentence}\")\n",
    "print(f\"Output: {default_tagger.tag(test_sentence)}\")\n",
    "print(\"\\nNotice: Even 'The' (determiner) and 'quickly' (adverb) get tagged as NN!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30726be4",
   "metadata": {},
   "source": [
    "### 7.7.2 UnigramTagger - Word-to-Tag Lookup Table\n",
    "\n",
    "The `UnigramTagger` learns the **most frequent tag for each word** from training data.\n",
    "\n",
    "**How it works:**\n",
    "```\n",
    "Training: Count (word, tag) pairs\n",
    "    \"the\" â†’ DT (seen 5000 times)\n",
    "    \"the\" â†’ NN (seen 2 times)\n",
    "    â†’ Store: \"the\" â†’ DT (most common)\n",
    "\n",
    "Tagging: Look up word in table\n",
    "    \"the\" â†’ DT âœ“\n",
    "    \"xyzzy\" â†’ None (unknown word, backoff!)\n",
    "```\n",
    "\n",
    "**Key insight:** UnigramTagger ignores contextâ€”it only looks at the word itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd48e4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram Tagger accuracy: 82.67%\n"
     ]
    }
   ],
   "source": [
    "# Unigram Tagger (learns most common tag for each word)\n",
    "unigram_tagger = UnigramTagger(train_sents, backoff=default_tagger)\n",
    "print(f\"Unigram Tagger accuracy: {unigram_tagger.accuracy(test_sents):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "11c669b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UnigramTagger: One Tag Per Word (ignores context)\n",
      "=======================================================\n",
      "Sentence                       light tagged as\n",
      "-------------------------------------------------------\n",
      "I will light the fire          NN\n",
      "The light is bright            NN\n",
      "This bag is light              NN\n",
      "\n",
      "âš ï¸ Problem: 'light' ALWAYS gets the same tag!\n",
      "   UnigramTagger doesn't know about context.\n"
     ]
    }
   ],
   "source": [
    "# Peek inside the UnigramTagger's learned model\n",
    "# It stores a dictionary: context â†’ most_common_tag\n",
    "\n",
    "# Train a unigram tagger\n",
    "unigram_tagger = UnigramTagger(train_sents, backoff=default_tagger)\n",
    "\n",
    "# Show how it tags words - each word gets ONE consistent tag\n",
    "print(\"UnigramTagger: One Tag Per Word (ignores context)\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "test_sentences = [\n",
    "    \"I will light the fire\",      # light should be VB\n",
    "    \"The light is bright\",        # light should be NN\n",
    "    \"This bag is light\",          # light should be JJ\n",
    "]\n",
    "\n",
    "print(f\"{'Sentence':<30} {'light tagged as'}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for sent in test_sentences:\n",
    "    tokens = word_tokenize(sent)\n",
    "    tagged = unigram_tagger.tag(tokens)\n",
    "    light_tag = [t for w, t in tagged if w == 'light'][0]\n",
    "    print(f\"{sent:<30} {light_tag}\")\n",
    "\n",
    "print(\"\\nâš ï¸ Problem: 'light' ALWAYS gets the same tag!\")\n",
    "print(\"   UnigramTagger doesn't know about context.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ffddb8",
   "metadata": {},
   "source": [
    "### 7.7.3 BigramTagger - Uses Previous Tag\n",
    "\n",
    "The `BigramTagger` considers the **previous tag** when deciding the current tag.\n",
    "\n",
    "**How it works:**\n",
    "```\n",
    "Training: Count (prev_tag, word) â†’ tag\n",
    "    (DT, \"light\") â†’ NN (after \"the\", light is usually noun)\n",
    "    (MD, \"light\") â†’ VB (after \"will\", light is usually verb)\n",
    "\n",
    "Tagging: Look up (prev_tag, word)\n",
    "    prev_tag=DT, word=\"light\" â†’ NN\n",
    "    prev_tag=MD, word=\"light\" â†’ VB\n",
    "```\n",
    "\n",
    "**Key insight:** Same word can get different tags based on what came before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ace359d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram Tagger accuracy: 83.61%\n"
     ]
    }
   ],
   "source": [
    "# Bigram Tagger (considers previous word)\n",
    "bigram_tagger = BigramTagger(train_sents, backoff=unigram_tagger)\n",
    "print(f\"Bigram Tagger accuracy: {bigram_tagger.accuracy(test_sents):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c0fa124c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BigramTagger: Same Word, Different Tags Based on Context\n",
      "============================================================\n",
      "Sentence                            Word       Prev Tag   Result\n",
      "------------------------------------------------------------\n",
      "The permit was approved             permit     AT         NN\n",
      "They permit smoking here            permit     PPSS       VB\n",
      "A research grant                    research   AT         NN\n",
      "They research topics                research   PPSS       NN\n",
      "\n",
      "âœ“ BigramTagger considers previous tag to disambiguate!\n"
     ]
    }
   ],
   "source": [
    "# See how BigramTagger uses previous tag context\n",
    "bigram_tagger = BigramTagger(train_sents, backoff=unigram_tagger)\n",
    "\n",
    "# Show how same word gets different tags with different previous tags\n",
    "print(\"BigramTagger: Same Word, Different Tags Based on Context\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test sentences where previous tag affects the decision\n",
    "test_cases = [\n",
    "    (\"The permit was approved\", \"permit\", \"DT\"),     # After DT â†’ noun\n",
    "    (\"They permit smoking here\", \"permit\", \"PRP\"),   # After PRP â†’ verb\n",
    "    (\"A research grant\", \"research\", \"DT\"),          # After DT â†’ noun  \n",
    "    (\"They research topics\", \"research\", \"PRP\"),     # After PRP â†’ verb\n",
    "]\n",
    "\n",
    "print(f\"{'Sentence':<35} {'Word':<10} {'Prev Tag':<10} {'Result'}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for sent, target, expected_prev in test_cases:\n",
    "    tokens = word_tokenize(sent)\n",
    "    tagged = bigram_tagger.tag(tokens)\n",
    "    \n",
    "    # Find the target word and its tag\n",
    "    for i, (w, t) in enumerate(tagged):\n",
    "        if w.lower() == target:\n",
    "            prev_tag = tagged[i-1][1] if i > 0 else '<START>'\n",
    "            print(f\"{sent:<35} {target:<10} {prev_tag:<10} {t}\")\n",
    "\n",
    "print(\"\\nâœ“ BigramTagger considers previous tag to disambiguate!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04064ab8",
   "metadata": {},
   "source": [
    "### 7.7.4 TrigramTagger - Uses Two Previous Tags\n",
    "\n",
    "The `TrigramTagger` looks at the **two previous tags** for even more context.\n",
    "\n",
    "**How it works:**\n",
    "```\n",
    "Training: Count (prev_tag_2, prev_tag_1, word) â†’ tag\n",
    "    (PRP, MD, \"light\") â†’ VB (\"I will light\" â†’ verb)\n",
    "    (DT, JJ, \"light\") â†’ NN (\"the bright light\" â†’ noun)\n",
    "```\n",
    "\n",
    "**Tradeoff:** More context = better decisions, but also:\n",
    "- More sparse data (many 3-tuples never seen in training)\n",
    "- More likely to need backoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a60900df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigram Tagger accuracy: 83.44%\n"
     ]
    }
   ],
   "source": [
    "# Trigram Tagger (considers two previous words)\n",
    "trigram_tagger = TrigramTagger(train_sents, backoff=bigram_tagger)\n",
    "print(f\"Trigram Tagger accuracy: {trigram_tagger.accuracy(test_sents):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc09506a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tagger Comparison\n",
      "===================================\n",
      "Default    12.10%\n",
      "Unigram    82.67%\n",
      "Bigram     83.61%\n",
      "Trigram    83.44%\n"
     ]
    }
   ],
   "source": [
    "# Compare all taggers\n",
    "print(\"\\nTagger Comparison\")\n",
    "print(\"=\" * 35)\n",
    "taggers = [\n",
    "    (\"Default\", default_tagger),\n",
    "    (\"Unigram\", unigram_tagger),\n",
    "    (\"Bigram\", bigram_tagger),\n",
    "    (\"Trigram\", trigram_tagger),\n",
    "]\n",
    "\n",
    "for name, tagger in taggers:\n",
    "    acc = tagger.accuracy(test_sents)\n",
    "    print(f\"{name:<10} {acc:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4037f0",
   "metadata": {},
   "source": [
    "### 7.7.5 Understanding the Backoff Chain\n",
    "\n",
    "**The Problem:** N-gram taggers fail when they encounter unseen contexts.\n",
    "\n",
    "```\n",
    "TrigramTagger sees: (PRP, VBZ, \"xyzzy\")\n",
    "    â†’ Never seen this in training!\n",
    "    â†’ Can't make prediction\n",
    "    â†’ BACKOFF to BigramTagger\n",
    "\n",
    "BigramTagger sees: (VBZ, \"xyzzy\")\n",
    "    â†’ Also never seen!\n",
    "    â†’ BACKOFF to UnigramTagger\n",
    "\n",
    "UnigramTagger sees: \"xyzzy\"\n",
    "    â†’ Never seen this word!\n",
    "    â†’ BACKOFF to DefaultTagger\n",
    "\n",
    "DefaultTagger:\n",
    "    â†’ Returns 'NN' (default)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a3b78194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backoff Chain: How Taggers Collaborate\n",
      "=================================================================\n",
      "UnigramTagger handles: 0/2502 words (0.0%)\n",
      "  â†’ Remaining 2502 words need backoff\n",
      "\n",
      "Example: Tagging 'The zorbflex is amazing'\n",
      "--------------------------------------------------\n",
      "  'The' â†’ AT  [DefaultTagger (unknown â†’ backoff to NN)]\n",
      "  'zorbflex' â†’ NN  [DefaultTagger (unknown â†’ backoff to NN)]\n",
      "  'is' â†’ BEZ  [DefaultTagger (unknown â†’ backoff to NN)]\n",
      "  'amazing' â†’ NN  [DefaultTagger (unknown â†’ backoff to NN)]\n"
     ]
    }
   ],
   "source": [
    "# Visualize the backoff chain in action\n",
    "# Show which tagger handles common vs rare words\n",
    "\n",
    "print(\"Backoff Chain: How Taggers Collaborate\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "# Check coverage of each tagger level\n",
    "def count_coverage(tagger, test_sents):\n",
    "    \"\"\"Count how many words a tagger can handle without backoff\"\"\"\n",
    "    handled = 0\n",
    "    total = 0\n",
    "    for sent in test_sents:\n",
    "        for word, gold_tag in sent:\n",
    "            total += 1\n",
    "            # Check if this tagger (not its backoff) can handle the word\n",
    "            if hasattr(tagger, '_context_to_tag'):\n",
    "                # For N-gram taggers, check if context exists\n",
    "                if (word,) in tagger._context_to_tag:\n",
    "                    handled += 1\n",
    "    return handled, total\n",
    "\n",
    "uni_handled, total = count_coverage(unigram_tagger, test_sents[:100])\n",
    "print(f\"UnigramTagger handles: {uni_handled}/{total} words ({uni_handled/total:.1%})\")\n",
    "print(f\"  â†’ Remaining {total-uni_handled} words need backoff\\n\")\n",
    "\n",
    "# Show example of backoff with unknown word\n",
    "print(\"Example: Tagging 'The zorbflex is amazing'\")\n",
    "print(\"-\" * 50)\n",
    "tokens = ['The', 'zorbflex', 'is', 'amazing']\n",
    "tagged = trigram_tagger.tag(tokens)\n",
    "\n",
    "for word, tag in tagged:\n",
    "    if (word,) in unigram_tagger._context_to_tag:\n",
    "        source = \"UnigramTagger (known word)\"\n",
    "    else:\n",
    "        source = \"DefaultTagger (unknown â†’ backoff to NN)\"\n",
    "    print(f\"  '{word}' â†’ {tag}  [{source}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1129f528",
   "metadata": {},
   "source": [
    "### 7.7.6 Why Accuracy Improves (But Not By Much)\n",
    "\n",
    "Let's understand why adding more context helps, but with diminishing returns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e1d10a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why Each Tagger Level Helps\n",
      "======================================================================\n",
      "\n",
      "Contexts learned from training data:\n",
      "  UnigramTagger: 10,036 unique words\n",
      "  BigramTagger:  2,810 unique (prev_tag, word) pairs\n",
      "  TrigramTagger: 1,182 unique (tag, tag, word) triples\n",
      "\n",
      "âš ï¸ The Sparsity Problem:\n",
      "  - Training has limited examples of each context\n",
      "  - TrigramTagger has MANY contexts but few examples of each\n",
      "  - Most words get handled by Unigram or Bigram, not Trigram\n",
      "\n",
      "Accuracy comparison:\n",
      "  Default only    12.10%  (+12.10%)\n",
      "  + Unigram       82.67%  (+70.58%)\n",
      "  + Bigram        83.61%  (+0.93%)\n",
      "  + Trigram       83.44%  (+-0.17%)\n"
     ]
    }
   ],
   "source": [
    "# Analyze why each tagger level adds accuracy\n",
    "print(\"Why Each Tagger Level Helps\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Count how many unique contexts each tagger learned\n",
    "print(f\"\\nContexts learned from training data:\")\n",
    "print(f\"  UnigramTagger: {len(unigram_tagger._context_to_tag):,} unique words\")\n",
    "print(f\"  BigramTagger:  {len(bigram_tagger._context_to_tag):,} unique (prev_tag, word) pairs\")\n",
    "print(f\"  TrigramTagger: {len(trigram_tagger._context_to_tag):,} unique (tag, tag, word) triples\")\n",
    "\n",
    "# Show the sparsity problem\n",
    "print(f\"\\nâš ï¸ The Sparsity Problem:\")\n",
    "print(f\"  - Training has limited examples of each context\")\n",
    "print(f\"  - TrigramTagger has MANY contexts but few examples of each\")\n",
    "print(f\"  - Most words get handled by Unigram or Bigram, not Trigram\")\n",
    "\n",
    "# Demonstrate with accuracy breakdown\n",
    "print(f\"\\nAccuracy comparison:\")\n",
    "taggers_info = [\n",
    "    (\"Default only\", DefaultTagger('NN')),\n",
    "    (\"+ Unigram\", UnigramTagger(train_sents, backoff=DefaultTagger('NN'))),\n",
    "    (\"+ Bigram\", bigram_tagger),\n",
    "    (\"+ Trigram\", trigram_tagger),\n",
    "]\n",
    "\n",
    "prev_acc = 0\n",
    "for name, tagger in taggers_info:\n",
    "    acc = tagger.accuracy(test_sents)\n",
    "    improvement = acc - prev_acc\n",
    "    print(f\"  {name:<15} {acc:.2%}  (+{improvement:.2%})\")\n",
    "    prev_acc = acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8ad40b",
   "metadata": {},
   "source": [
    "### 7.7.7 Building a Custom Tagger for Your Domain\n",
    "\n",
    "You can train taggers on **your own data** for better domain-specific accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e59c4161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Medical Domain Tagger\n",
      "==================================================\n",
      "\n",
      "'The patient has elevated blood pressure'\n",
      "  â†’ [('The', 'DT'), ('patient', 'NN'), ('has', 'VBZ'), ('elevated', 'JJ'), ('blood', 'NN'), ('pressure', 'NN')]\n",
      "\n",
      "'Prescribe medication daily'\n",
      "  â†’ [('Prescribe', 'VB'), ('medication', 'NN'), ('daily', 'RB')]\n",
      "\n",
      "ğŸ’¡ Tip: The more training data you provide, the better your custom tagger!\n"
     ]
    }
   ],
   "source": [
    "# Example: Creating a custom tagger with your own tagged data\n",
    "# Format: list of sentences, each sentence is list of (word, tag) tuples\n",
    "\n",
    "# Suppose you have domain-specific tagged data (e.g., medical text)\n",
    "custom_training_data = [\n",
    "    [('The', 'DT'), ('patient', 'NN'), ('has', 'VBZ'), ('diabetes', 'NN'), ('.', '.')],\n",
    "    [('Prescribe', 'VB'), ('insulin', 'NN'), ('daily', 'RB'), ('.', '.')],\n",
    "    [('Blood', 'NN'), ('pressure', 'NN'), ('is', 'VBZ'), ('elevated', 'JJ'), ('.', '.')],\n",
    "    [('The', 'DT'), ('MRI', 'NN'), ('shows', 'VBZ'), ('no', 'DT'), ('abnormalities', 'NNS'), ('.', '.')],\n",
    "    [('Patient', 'NN'), ('reports', 'VBZ'), ('chest', 'NN'), ('pain', 'NN'), ('.', '.')],\n",
    "]\n",
    "\n",
    "# Build a custom tagger chain\n",
    "custom_default = DefaultTagger('NN')\n",
    "custom_unigram = UnigramTagger(custom_training_data, backoff=custom_default)\n",
    "custom_bigram = BigramTagger(custom_training_data, backoff=custom_unigram)\n",
    "\n",
    "print(\"Custom Medical Domain Tagger\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test on medical text\n",
    "test_sentences = [\n",
    "    \"The patient has elevated blood pressure\",\n",
    "    \"Prescribe medication daily\",\n",
    "]\n",
    "\n",
    "for sent in test_sentences:\n",
    "    tokens = word_tokenize(sent)\n",
    "    tagged = custom_bigram.tag(tokens)\n",
    "    print(f\"\\n'{sent}'\")\n",
    "    print(f\"  â†’ {tagged}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ Tip: The more training data you provide, the better your custom tagger!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c5e80d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tagger works!\n",
      "  [('The', 'AT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'NN')]\n",
      "\n",
      "ğŸ’¡ Tip: Save trained taggers to avoid retraining every time!\n"
     ]
    }
   ],
   "source": [
    "# Save and load trained taggers using pickle\n",
    "import pickle\n",
    "\n",
    "# Save tagger to file\n",
    "with open('my_custom_tagger.pkl', 'wb') as f:\n",
    "    pickle.dump(trigram_tagger, f)\n",
    "\n",
    "# Load tagger from file\n",
    "with open('my_custom_tagger.pkl', 'rb') as f:\n",
    "    loaded_tagger = pickle.load(f)\n",
    "\n",
    "# Verify it works\n",
    "test_tokens = word_tokenize(\"The quick brown fox jumps\")\n",
    "print(\"Loaded tagger works!\")\n",
    "print(f\"  {loaded_tagger.tag(test_tokens)}\")\n",
    "\n",
    "# Clean up\n",
    "import os\n",
    "os.remove('my_custom_tagger.pkl')\n",
    "print(\"\\nğŸ’¡ Tip: Save trained taggers to avoid retraining every time!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16e599a",
   "metadata": {},
   "source": [
    "### 7.7.8 RegexpTagger - Pattern-Based Tagging\n",
    "\n",
    "Sometimes you want rules based on **word patterns** rather than exact matches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c756ad7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RegexpTagger - Pattern-Based Rules\n",
      "=======================================================\n",
      "Word            Pattern Matched      Tag\n",
      "-------------------------------------------------------\n",
      "running         .*ing$               VBG\n",
      "walked          .*ed$                VBD\n",
      "quickly         .*ly$                RB\n",
      "beautiful       .*ful$               JJ\n",
      "education       .*tion$              NN\n",
      "happiness       .*ness$              NN\n",
      "42              ^[0-9]+$             CD\n",
      "London          ^[A-Z][a-z]+$        NNP\n",
      "cat             .*                   NN\n",
      "\n",
      "ğŸ’¡ RegexpTagger is great for handling unknown words based on morphology!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import RegexpTagger\n",
    "\n",
    "# Define regex patterns for common word endings\n",
    "# Format: (pattern, tag) - patterns are checked in order\n",
    "patterns = [\n",
    "    (r'.*ing$', 'VBG'),    # gerunds: running, eating\n",
    "    (r'.*ed$', 'VBD'),     # past tense: walked, jumped\n",
    "    (r'.*es$', 'VBZ'),     # 3rd person: watches, goes\n",
    "    (r'.*ly$', 'RB'),      # adverbs: quickly, slowly\n",
    "    (r'.*tion$', 'NN'),    # nouns: nation, education\n",
    "    (r'.*ness$', 'NN'),    # nouns: happiness, darkness\n",
    "    (r'.*ful$', 'JJ'),     # adjectives: beautiful, careful\n",
    "    (r'.*able$', 'JJ'),    # adjectives: readable, capable\n",
    "    (r'^[0-9]+$', 'CD'),   # numbers: 123, 456\n",
    "    (r'^[A-Z][a-z]+$', 'NNP'),  # capitalized: John, Paris\n",
    "    (r'.*', 'NN'),         # default: noun\n",
    "]\n",
    "\n",
    "regex_tagger = RegexpTagger(patterns)\n",
    "\n",
    "print(\"RegexpTagger - Pattern-Based Rules\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "test_words = ['running', 'walked', 'quickly', 'beautiful', \n",
    "              'education', 'happiness', '42', 'London', 'cat']\n",
    "\n",
    "print(f\"{'Word':<15} {'Pattern Matched':<20} {'Tag'}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for word in test_words:\n",
    "    tag = regex_tagger.tag([word])[0][1]\n",
    "    # Find which pattern matched\n",
    "    matched = 'default'\n",
    "    for pattern, ptag in patterns:\n",
    "        import re\n",
    "        if re.match(pattern, word) and ptag == tag:\n",
    "            matched = pattern\n",
    "            break\n",
    "    print(f\"{word:<15} {matched:<20} {tag}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ RegexpTagger is great for handling unknown words based on morphology!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c37614eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sophisticated Tagger Chain\n",
      "============================================================\n",
      "\n",
      "TrigramTagger (most context)\n",
      "    â†“ backoff\n",
      "BigramTagger (previous tag)\n",
      "    â†“ backoff\n",
      "UnigramTagger (word lookup)\n",
      "    â†“ backoff\n",
      "RegexpTagger (pattern matching)  â† Better than DefaultTagger!\n",
      "\n",
      "With DefaultTagger backoff:  83.44%\n",
      "With RegexpTagger backoff:   84.89%\n",
      "Improvement: +1.45%\n"
     ]
    }
   ],
   "source": [
    "# Combine RegexpTagger with N-gram taggers for best results\n",
    "# Regex handles unknown words, N-grams handle known patterns\n",
    "\n",
    "# Build a sophisticated tagger chain\n",
    "sophisticated_tagger = TrigramTagger(\n",
    "    train_sents,\n",
    "    backoff=BigramTagger(\n",
    "        train_sents,\n",
    "        backoff=UnigramTagger(\n",
    "            train_sents,\n",
    "            backoff=RegexpTagger(patterns)  # Regex as final backoff!\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Sophisticated Tagger Chain\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "TrigramTagger (most context)\n",
    "    â†“ backoff\n",
    "BigramTagger (previous tag)\n",
    "    â†“ backoff\n",
    "UnigramTagger (word lookup)\n",
    "    â†“ backoff\n",
    "RegexpTagger (pattern matching)  â† Better than DefaultTagger!\n",
    "\"\"\")\n",
    "\n",
    "# Compare accuracy\n",
    "default_chain_acc = trigram_tagger.accuracy(test_sents)\n",
    "regex_chain_acc = sophisticated_tagger.accuracy(test_sents)\n",
    "\n",
    "print(f\"With DefaultTagger backoff:  {default_chain_acc:.2%}\")\n",
    "print(f\"With RegexpTagger backoff:   {regex_chain_acc:.2%}\")\n",
    "print(f\"Improvement: +{(regex_chain_acc - default_chain_acc)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115cac99",
   "metadata": {},
   "source": [
    "### Summary: Custom Tagger Types\n",
    "\n",
    "| Tagger | Context Used | Best For |\n",
    "|--------|--------------|----------|\n",
    "| **DefaultTagger** | None | Final fallback, baseline |\n",
    "| **UnigramTagger** | word only | Words with consistent tags |\n",
    "| **BigramTagger** | prev_tag + word | Ambiguous words |\n",
    "| **TrigramTagger** | prev_2_tags + word | Complex patterns |\n",
    "| **RegexpTagger** | word patterns | Unknown words, morphology |\n",
    "\n",
    "### Best Practice: Build a Backoff Chain\n",
    "\n",
    "```python\n",
    "tagger = TrigramTagger(train, backoff=\n",
    "    BigramTagger(train, backoff=\n",
    "        UnigramTagger(train, backoff=\n",
    "            RegexpTagger(patterns))))\n",
    "```\n",
    "\n",
    "This combines the **precision** of N-gram taggers with the **coverage** of pattern-based rules!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79e50d2",
   "metadata": {},
   "source": [
    "## 7.8 Practical Application: Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8119087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: The ambitious young scientist quickly discovered a remarkable \n",
      "breakthrough in artificial intelligence. She carefully analyzed the complex \n",
      "data and brilliantly solved the challenging problem.\n",
      "\n",
      "Text Analysis\n",
      "==================================================\n",
      "\n",
      "Nouns (6):\n",
      "  ['scientist', 'breakthrough', 'intelligence', 'data', 'challenging', 'problem']\n",
      "\n",
      "Verbs (3):\n",
      "  ['discovered', 'analyzed', 'solved']\n",
      "\n",
      "Adjectives (5):\n",
      "  ['ambitious', 'young', 'remarkable', 'artificial', 'complex']\n",
      "\n",
      "Adverbs (3):\n",
      "  ['quickly', 'carefully', 'brilliantly']\n",
      "\n",
      "Pronouns (1):\n",
      "  ['She']\n"
     ]
    }
   ],
   "source": [
    "def analyze_text(text):\n",
    "    \"\"\"Comprehensive text analysis using POS tagging\"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged = pos_tag(tokens)\n",
    "    \n",
    "    # Count by category\n",
    "    categories = {\n",
    "        'Nouns': ['NN', 'NNS', 'NNP', 'NNPS'],\n",
    "        'Verbs': ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'],\n",
    "        'Adjectives': ['JJ', 'JJR', 'JJS'],\n",
    "        'Adverbs': ['RB', 'RBR', 'RBS'],\n",
    "        'Pronouns': ['PRP', 'PRP$', 'WP', 'WP$'],\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    for category, tags in categories.items():\n",
    "        words = [w for w, t in tagged if t in tags]\n",
    "        results[category] = {\n",
    "            'count': len(words),\n",
    "            'words': words\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "text = \"\"\"The ambitious young scientist quickly discovered a remarkable \n",
    "breakthrough in artificial intelligence. She carefully analyzed the complex \n",
    "data and brilliantly solved the challenging problem.\"\"\"\n",
    "\n",
    "print(f\"Text: {text}\\n\")\n",
    "\n",
    "analysis = analyze_text(text)\n",
    "\n",
    "print(\"Text Analysis\")\n",
    "print(\"=\" * 50)\n",
    "for category, data in analysis.items():\n",
    "    print(f\"\\n{category} ({data['count']}):\")\n",
    "    print(f\"  {data['words']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac7898e",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Method | Description |\n",
    "|--------|-------------|\n",
    "| `pos_tag(tokens)` | Tag a list of tokens |\n",
    "| `pos_tag(tokens, tagset='universal')` | Use universal tagset |\n",
    "| `pos_tag_sents(list_of_sents)` | Batch tag multiple sentences |\n",
    "| `nltk.help.upenn_tagset('TAG')` | Get tag description |\n",
    "\n",
    "### Common Tags\n",
    "- **Nouns**: NN, NNS, NNP, NNPS\n",
    "- **Verbs**: VB, VBD, VBG, VBN, VBP, VBZ\n",
    "- **Adjectives**: JJ, JJR, JJS\n",
    "- **Adverbs**: RB, RBR, RBS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
