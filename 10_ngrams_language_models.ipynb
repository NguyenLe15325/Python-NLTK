{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbabdd27",
   "metadata": {},
   "source": [
    "# NLTK Complete Guide - Section 10: N-Grams & Language Models\n",
    "\n",
    "This notebook covers:\n",
    "- What are N-Grams?\n",
    "- Generating N-Grams\n",
    "- N-Gram Frequency Analysis\n",
    "- Collocations\n",
    "- Simple Language Models\n",
    "- Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e02d353",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('gutenberg', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "from nltk import ngrams, bigrams, trigrams\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from nltk.collocations import BigramCollocationFinder, TrigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures, TrigramAssocMeasures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85450599",
   "metadata": {},
   "source": [
    "## 10.1 What are N-Grams?\n",
    "\n",
    "**N-grams** are contiguous sequences of n items from text. They're the foundation of statistical language models and many NLP applications.\n",
    "\n",
    "### The Core Idea\n",
    "\n",
    "Given a sequence of words, an n-gram captures a \"sliding window\" of n consecutive items:\n",
    "\n",
    "| Type | N | Window Size | Example (\"I love NLP\") |\n",
    "|------|---|-------------|------------------------|\n",
    "| Unigram | 1 | 1 word | [\"I\", \"love\", \"NLP\"] |\n",
    "| Bigram | 2 | 2 words | [(\"I\", \"love\"), (\"love\", \"NLP\")] |\n",
    "| Trigram | 3 | 3 words | [(\"I\", \"love\", \"NLP\")] |\n",
    "| 4-gram | 4 | 4 words | Not enough words! |\n",
    "\n",
    "### Why N-Grams Matter\n",
    "\n",
    "1. **Capture Local Context**: \"New York\" means something different than \"New\" and \"York\" separately\n",
    "2. **Statistical Patterns**: We can count how often word sequences appear\n",
    "3. **Prediction**: If we see \"New\", what word likely follows? (\"York\", \"Zealand\", \"Year\"...)\n",
    "4. **Simplicity**: No deep learning needed - just counting!\n",
    "\n",
    "### The Trade-off: N Size\n",
    "\n",
    "| Small N (1-2) | Large N (4+) |\n",
    "|---------------|--------------|\n",
    "| ✅ More training examples | ❌ Fewer training examples |\n",
    "| ✅ Better coverage | ❌ Sparse data problem |\n",
    "| ❌ Less context | ✅ More context |\n",
    "| ❌ Less accurate predictions | ✅ More accurate (when data exists) |\n",
    "\n",
    "**Sweet spot**: Trigrams (n=3) often balance context and data availability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "470d353d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I love natural language processing\n",
      "Tokens: ['I', 'love', 'natural', 'language', 'processing']\n",
      "\n",
      "Unigrams (1): [('I',), ('love',), ('natural',), ('language',), ('processing',)]\n",
      "Bigrams (2):  [('I', 'love'), ('love', 'natural'), ('natural', 'language'), ('language', 'processing')]\n",
      "Trigrams (3): [('I', 'love', 'natural'), ('love', 'natural', 'language'), ('natural', 'language', 'processing')]\n",
      "4-grams (4):  [('I', 'love', 'natural', 'language'), ('love', 'natural', 'language', 'processing')]\n"
     ]
    }
   ],
   "source": [
    "text = \"I love natural language processing\"\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Tokens: {tokens}\\n\")\n",
    "\n",
    "# Generate n-grams\n",
    "unigrams = list(ngrams(tokens, 1))\n",
    "bi_grams = list(ngrams(tokens, 2))\n",
    "tri_grams = list(ngrams(tokens, 3))\n",
    "four_grams = list(ngrams(tokens, 4))\n",
    "\n",
    "print(f\"Unigrams (1): {unigrams}\")\n",
    "print(f\"Bigrams (2):  {bi_grams}\")\n",
    "print(f\"Trigrams (3): {tri_grams}\")\n",
    "print(f\"4-grams (4):  {four_grams}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c2c8a4",
   "metadata": {},
   "source": [
    "## 10.2 NLTK Convenience Functions\n",
    "\n",
    "NLTK provides helper functions that wrap the general `ngrams()` function for common cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d561193b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: The quick brown fox jumps over the lazy dog\n",
      "\n",
      "Bigrams (using bigrams()):\n",
      "  ('the', 'quick')\n",
      "  ('quick', 'brown')\n",
      "  ('brown', 'fox')\n",
      "  ('fox', 'jumps')\n",
      "  ('jumps', 'over')\n",
      "  ('over', 'the')\n",
      "  ('the', 'lazy')\n",
      "  ('lazy', 'dog')\n",
      "\n",
      "Trigrams (using trigrams()):\n",
      "  ('the', 'quick', 'brown')\n",
      "  ('quick', 'brown', 'fox')\n",
      "  ('brown', 'fox', 'jumps')\n",
      "  ('fox', 'jumps', 'over')\n",
      "  ('jumps', 'over', 'the')\n",
      "  ('over', 'the', 'lazy')\n",
      "  ('the', 'lazy', 'dog')\n"
     ]
    }
   ],
   "source": [
    "text = \"The quick brown fox jumps over the lazy dog\"\n",
    "tokens = word_tokenize(text.lower())\n",
    "\n",
    "print(f\"Text: {text}\\n\")\n",
    "\n",
    "# Using convenience functions\n",
    "print(\"Bigrams (using bigrams()):\")\n",
    "for bg in bigrams(tokens):\n",
    "    print(f\"  {bg}\")\n",
    "\n",
    "print(\"\\nTrigrams (using trigrams()):\")\n",
    "for tg in trigrams(tokens):\n",
    "    print(f\"  {tg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c09fcc3",
   "metadata": {},
   "source": [
    "## 10.3 N-Gram with Padding\n",
    "\n",
    "### The Boundary Problem\n",
    "\n",
    "Consider the sentence \"I love NLP\" with bigrams:\n",
    "- `(\"I\", \"love\")` ✅\n",
    "- `(\"love\", \"NLP\")` ✅\n",
    "\n",
    "But what about:\n",
    "- What comes **before** \"I\"? (sentence start)\n",
    "- What comes **after** \"NLP\"? (sentence end)\n",
    "\n",
    "Without handling boundaries, the model can't learn:\n",
    "- How sentences typically **start**\n",
    "- How sentences typically **end**\n",
    "\n",
    "### The Solution: Padding Tokens\n",
    "\n",
    "Add special markers:\n",
    "- `<s>` = Start of sentence\n",
    "- `</s>` = End of sentence\n",
    "\n",
    "```\n",
    "Original:  I love NLP\n",
    "Padded:    <s> I love NLP </s>\n",
    "\n",
    "Bigrams:   (<s>, I), (I, love), (love, NLP), (NLP, </s>)\n",
    "```\n",
    "\n",
    "Now the model learns:\n",
    "- Sentences often start with \"I\", \"The\", \"She\", etc.\n",
    "- Sentences often end with nouns, periods, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4256aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I love NLP\n",
      "Tokens: ['I', 'love', 'NLP']\n",
      "\n",
      "Bigrams without padding:\n",
      "[('I', 'love'), ('love', 'NLP')]\n",
      "\n",
      "Bigrams with padding:\n",
      "Padded tokens: ['<s>', 'I', 'love', 'NLP', '</s>']\n",
      "Padded bigrams: [('<s>', 'I'), ('I', 'love'), ('love', 'NLP'), ('NLP', '</s>')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.lm.preprocessing import padded_everygram_pipeline, pad_both_ends\n",
    "\n",
    "text = \"I love NLP\"\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Tokens: {tokens}\\n\")\n",
    "\n",
    "# Without padding\n",
    "print(\"Bigrams without padding:\")\n",
    "print(list(bigrams(tokens)))\n",
    "\n",
    "# With padding\n",
    "print(\"\\nBigrams with padding:\")\n",
    "padded = list(pad_both_ends(tokens, n=2))\n",
    "print(f\"Padded tokens: {padded}\")\n",
    "print(f\"Padded bigrams: {list(bigrams(padded))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a18db3",
   "metadata": {},
   "source": [
    "## 10.4 N-Gram Frequency Analysis\n",
    "\n",
    "Counting n-gram frequencies reveals patterns in text:\n",
    "- **Common bigrams**: \"of the\", \"in the\", \"to be\" - often function words\n",
    "- **Rare bigrams**: Usually content-specific or unusual combinations\n",
    "\n",
    "This is the foundation of language models: **frequent patterns are more likely**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8234671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 1777\n",
      "Sample: ['emma', 'by', 'jane', 'austen', 'volume', 'i', 'chapter', 'i', 'emma', 'woodhouse', 'handsome', 'clever', 'and', 'rich', 'with', 'a', 'comfortable', 'home', 'and', 'happy']\n"
     ]
    }
   ],
   "source": [
    "# Load sample text\n",
    "text = gutenberg.raw('austen-emma.txt')[:10000]  # First 10K chars\n",
    "tokens = word_tokenize(text.lower())\n",
    "\n",
    "# Filter to alphabetic tokens only\n",
    "tokens = [t for t in tokens if t.isalpha()]\n",
    "\n",
    "print(f\"Total tokens: {len(tokens)}\")\n",
    "print(f\"Sample: {tokens[:20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dab0187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 15 Most Common Bigrams:\n",
      "----------------------------------------\n",
      "miss       taylor        13\n",
      "of         her           11\n",
      "it         was            9\n",
      "she        had            9\n",
      "of         the            7\n",
      "was        a              7\n",
      "he         was            7\n",
      "for        her            6\n",
      "her        own            6\n",
      "her        father         6\n",
      "in         the            5\n",
      "of         a              4\n",
      "a          very           4\n",
      "had        been           4\n",
      "thought    of             4\n"
     ]
    }
   ],
   "source": [
    "# Bigram frequencies\n",
    "bi_grams = list(bigrams(tokens))\n",
    "bigram_freq = Counter(bi_grams)\n",
    "\n",
    "print(\"Top 15 Most Common Bigrams:\")\n",
    "print(\"-\" * 40)\n",
    "for bg, count in bigram_freq.most_common(15):\n",
    "    print(f\"{bg[0]:<10} {bg[1]:<10} {count:>5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3570e8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 15 Most Common Trigrams:\n",
      "--------------------------------------------------\n",
      "it         was        a              3\n",
      "how        she        had            3\n",
      "a          mile       from           3\n",
      "a          house      of             3\n",
      "house      of         her            3\n",
      "of         her        own            3\n",
      "had        miss       taylor         2\n",
      "miss       taylor     had            2\n",
      "her        own        the            2\n",
      "not        at         all            2\n",
      "of         miss       taylor         2\n",
      "of         the        family         2\n",
      "only       half       a              2\n",
      "half       a          mile           2\n",
      "mile       from       them           2\n"
     ]
    }
   ],
   "source": [
    "# Trigram frequencies\n",
    "tri_grams = list(trigrams(tokens))\n",
    "trigram_freq = Counter(tri_grams)\n",
    "\n",
    "print(\"Top 15 Most Common Trigrams:\")\n",
    "print(\"-\" * 50)\n",
    "for tg, count in trigram_freq.most_common(15):\n",
    "    print(f\"{tg[0]:<10} {tg[1]:<10} {tg[2]:<10} {count:>5}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021a6229",
   "metadata": {},
   "source": [
    "## 10.5 Collocations\n",
    "\n",
    "**Collocations** are word combinations that occur together more often than chance would predict. Unlike simple frequency counting, collocation finding uses **statistical measures** to identify meaningful phrases.\n",
    "\n",
    "### Why Not Just Use Frequency?\n",
    "\n",
    "High-frequency bigrams like \"of the\" or \"in the\" aren't interesting - they appear often simply because \"the\" is common. We want phrases where the words have a **special relationship**.\n",
    "\n",
    "### Statistical Measures\n",
    "\n",
    "| Measure | What It Finds | Good For |\n",
    "|---------|---------------|----------|\n",
    "| **PMI** (Pointwise Mutual Information) | Words that co-occur more than expected | Rare but meaningful phrases |\n",
    "| **Chi-Square** | Statistical significance of co-occurrence | Technical/domain terms |\n",
    "| **Likelihood Ratio** | How much more likely together vs apart | Balanced approach |\n",
    "\n",
    "### PMI Formula (Under the Hood)\n",
    "\n",
    "$$PMI(x, y) = \\log_2 \\frac{P(x, y)}{P(x) \\cdot P(y)}$$\n",
    "\n",
    "- If words are **independent**: $P(x,y) = P(x) \\cdot P(y)$, so $PMI = 0$\n",
    "- If words **attract**: $P(x,y) > P(x) \\cdot P(y)$, so $PMI > 0$\n",
    "- If words **repel**: $P(x,y) < P(x) \\cdot P(y)$, so $PMI < 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27700f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 121,268\n"
     ]
    }
   ],
   "source": [
    "# Load more text\n",
    "text = gutenberg.raw('austen-emma.txt')\n",
    "tokens = word_tokenize(text.lower())\n",
    "tokens = [t for t in tokens if t.isalpha() and len(t) > 2]\n",
    "\n",
    "print(f\"Total tokens: {len(tokens):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2d275f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 15 Bigram Collocations (PMI):\n",
      "----------------------------------------\n",
      "  sore throat\n",
      "  brunswick square\n",
      "  william larkins\n",
      "  baked apples\n",
      "  box hill\n",
      "  sixteen miles\n",
      "  maple grove\n",
      "  hair cut\n",
      "  south end\n",
      "  colonel campbell\n",
      "  protest against\n",
      "  robert martin\n",
      "  five couple\n",
      "  vast deal\n",
      "  ready wit\n"
     ]
    }
   ],
   "source": [
    "# Find bigram collocations\n",
    "bigram_finder = BigramCollocationFinder.from_words(tokens)\n",
    "\n",
    "# Filter low-frequency bigrams\n",
    "bigram_finder.apply_freq_filter(5)\n",
    "\n",
    "# Get top collocations using PMI (Pointwise Mutual Information)\n",
    "bigram_measures = BigramAssocMeasures()\n",
    "\n",
    "print(\"Top 15 Bigram Collocations (PMI):\")\n",
    "print(\"-\" * 40)\n",
    "for colloc in bigram_finder.nbest(bigram_measures.pmi, 15):\n",
    "    print(f\"  {colloc[0]} {colloc[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5da8d8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 by Likelihood Ratio:\n",
      "  frank churchill\n",
      "  had been\n",
      "  miss woodhouse\n",
      "  have been\n",
      "  any thing\n",
      "  could not\n",
      "  she had\n",
      "  miss bates\n",
      "  miss fairfax\n",
      "  did not\n",
      "\n",
      "Top 10 by Chi-Square:\n",
      "  maple grove\n",
      "  brunswick square\n",
      "  box hill\n",
      "  william larkins\n",
      "  sore throat\n",
      "  frank churchill\n",
      "  colonel campbell\n",
      "  robert martin\n",
      "  baked apples\n",
      "  great deal\n"
     ]
    }
   ],
   "source": [
    "# Different scoring methods\n",
    "print(\"Top 10 by Likelihood Ratio:\")\n",
    "for colloc in bigram_finder.nbest(bigram_measures.likelihood_ratio, 10):\n",
    "    print(f\"  {colloc[0]} {colloc[1]}\")\n",
    "\n",
    "print(\"\\nTop 10 by Chi-Square:\")\n",
    "for colloc in bigram_finder.nbest(bigram_measures.chi_sq, 10):\n",
    "    print(f\"  {colloc[0]} {colloc[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a3b973c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 15 Trigram Collocations:\n",
      "--------------------------------------------------\n",
      "  bad sore throat\n",
      "  lovely woman reigns\n",
      "  woman reigns alone\n",
      "  beg your pardon\n",
      "  monarch the seas\n",
      "  box hill party\n",
      "  but frozen maid\n",
      "  husbands and wives\n",
      "  laid down upon\n",
      "  fair but frozen\n",
      "  kitty fair but\n",
      "  eating and drinking\n",
      "  woman lovely woman\n",
      "  pray take care\n",
      "  like maple grove\n"
     ]
    }
   ],
   "source": [
    "# Trigram collocations\n",
    "trigram_finder = TrigramCollocationFinder.from_words(tokens)\n",
    "trigram_finder.apply_freq_filter(3)\n",
    "\n",
    "trigram_measures = TrigramAssocMeasures()\n",
    "\n",
    "print(\"Top 15 Trigram Collocations:\")\n",
    "print(\"-\" * 50)\n",
    "for colloc in trigram_finder.nbest(trigram_measures.pmi, 15):\n",
    "    print(f\"  {' '.join(colloc)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7791db8",
   "metadata": {},
   "source": [
    "## 10.6 Simple Language Model\n",
    "\n",
    "### What is a Language Model?\n",
    "\n",
    "A **language model** assigns probabilities to sequences of words. It answers: \"How likely is this sentence?\"\n",
    "\n",
    "### The Chain Rule of Probability\n",
    "\n",
    "For a sentence $W = w_1, w_2, ..., w_n$:\n",
    "\n",
    "$$P(W) = P(w_1) \\cdot P(w_2|w_1) \\cdot P(w_3|w_1,w_2) \\cdot ... \\cdot P(w_n|w_1,...,w_{n-1})$$\n",
    "\n",
    "**Problem**: We'd need to count every possible word history - impossible!\n",
    "\n",
    "### The Markov Assumption (Key Insight!)\n",
    "\n",
    "**Assume** the next word depends only on the previous $n-1$ words:\n",
    "\n",
    "- **Bigram**: $P(w_i|w_1...w_{i-1}) \\approx P(w_i|w_{i-1})$\n",
    "- **Trigram**: $P(w_i|w_1...w_{i-1}) \\approx P(w_i|w_{i-2}, w_{i-1})$\n",
    "\n",
    "This is the **Markov assumption** - the future depends only on the recent past.\n",
    "\n",
    "### Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "We estimate probabilities by **counting**:\n",
    "\n",
    "$$P(w_n|w_{n-1}) = \\frac{Count(w_{n-1}, w_n)}{Count(w_{n-1})}$$\n",
    "\n",
    "Example: What's $P(\\text{knightley}|\\text{mr})$?\n",
    "\n",
    "$$P(\\text{knightley}|\\text{mr}) = \\frac{\\text{Times \"mr knightley\" appears}}{\\text{Times \"mr\" appears}}$$\n",
    "\n",
    "### Building Our Own Bigram Model\n",
    "\n",
    "Let's implement this from scratch to understand how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e27b1ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleBigramModel:\n",
    "    \"\"\"Simple bigram language model\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.bigram_counts = defaultdict(Counter)\n",
    "        self.unigram_counts = Counter()\n",
    "    \n",
    "    def train(self, tokens):\n",
    "        \"\"\"Train on a list of tokens\"\"\"\n",
    "        # Count unigrams\n",
    "        self.unigram_counts = Counter(tokens)\n",
    "        \n",
    "        # Count bigrams (word1 -> word2)\n",
    "        for w1, w2 in bigrams(tokens):\n",
    "            self.bigram_counts[w1][w2] += 1\n",
    "    \n",
    "    def probability(self, word, context):\n",
    "        \"\"\"P(word | context)\"\"\"\n",
    "        if context not in self.bigram_counts:\n",
    "            return 0\n",
    "        \n",
    "        total = sum(self.bigram_counts[context].values())\n",
    "        return self.bigram_counts[context][word] / total\n",
    "    \n",
    "    def next_word_probs(self, context):\n",
    "        \"\"\"Get probabilities for all possible next words\"\"\"\n",
    "        if context not in self.bigram_counts:\n",
    "            return {}\n",
    "        \n",
    "        total = sum(self.bigram_counts[context].values())\n",
    "        return {word: count/total \n",
    "                for word, count in self.bigram_counts[context].items()}\n",
    "    \n",
    "    def generate(self, start_word, length=10):\n",
    "        \"\"\"Generate text starting from a word\"\"\"\n",
    "        words = [start_word]\n",
    "        current = start_word\n",
    "        \n",
    "        for _ in range(length - 1):\n",
    "            if current not in self.bigram_counts:\n",
    "                break\n",
    "            \n",
    "            # Get next word probabilities\n",
    "            probs = self.next_word_probs(current)\n",
    "            if not probs:\n",
    "                break\n",
    "            \n",
    "            # Choose next word weighted by probability\n",
    "            next_words = list(probs.keys())\n",
    "            weights = list(probs.values())\n",
    "            current = random.choices(next_words, weights=weights)[0]\n",
    "            words.append(current)\n",
    "        \n",
    "        return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08a49aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 6,932\n",
      "Unique bigram contexts: 6,931\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "text = gutenberg.raw('austen-emma.txt')\n",
    "tokens = word_tokenize(text.lower())\n",
    "tokens = [t for t in tokens if t.isalpha()]\n",
    "\n",
    "model = SimpleBigramModel()\n",
    "model.train(tokens)\n",
    "\n",
    "print(f\"Vocabulary size: {len(model.unigram_counts):,}\")\n",
    "print(f\"Unique bigram contexts: {len(model.bigram_counts):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a448c6c3",
   "metadata": {},
   "source": [
    "### How the Model Works Internally\n",
    "\n",
    "The `SimpleBigramModel` stores two data structures:\n",
    "\n",
    "1. **`unigram_counts`**: How often each word appears\n",
    "   ```\n",
    "   {\"the\": 5000, \"mr\": 800, \"emma\": 500, ...}\n",
    "   ```\n",
    "\n",
    "2. **`bigram_counts`**: For each word, what words follow it\n",
    "   ```\n",
    "   {\"mr\": {\"knightley\": 200, \"woodhouse\": 150, \"elton\": 100, ...},\n",
    "    \"the\": {\"house\": 50, \"young\": 40, ...}}\n",
    "   ```\n",
    "\n",
    "When we call `probability(\"knightley\", \"mr\")`:\n",
    "- Look up all words following \"mr\": 200 + 150 + 100 + ... = 500 total\n",
    "- \"knightley\" appears 200 times after \"mr\"\n",
    "- Return: 200/500 = 0.40 (40%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aad2c4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words that follow 'mr':\n",
      "------------------------------\n",
      "  knightley       22.58%\n",
      "  elton           22.58%\n",
      "  weston          6.45%\n",
      "  woodhouse       4.84%\n",
      "  dixon           3.23%\n",
      "  richard         3.23%\n",
      "  churchill       3.23%\n",
      "  perry           3.23%\n",
      "  martin          1.61%\n",
      "  robert          1.61%\n"
     ]
    }
   ],
   "source": [
    "# Check probabilities\n",
    "context = \"mr\"\n",
    "print(f\"Words that follow '{context}':\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "probs = model.next_word_probs(context)\n",
    "sorted_probs = sorted(probs.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for word, prob in sorted_probs[:10]:\n",
    "    print(f\"  {word:<15} {prob:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fdf1330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text samples:\n",
      "============================================================\n",
      "\n",
      "'the' → the sofa for elton as they proceeded a sincerity it is beyond\n",
      "\n",
      "'she' → she heard and much exultation i ever shall grant you cried emma\n",
      "\n",
      "'he' → he would rather not let us to speak therefore every sacrifice for\n",
      "\n",
      "'it' → it will be going this belief on so active half a moment\n",
      "\n",
      "'mr' → mr knightley you heard you apprehended from him and the same they\n"
     ]
    }
   ],
   "source": [
    "# Generate text\n",
    "print(\"Generated text samples:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "start_words = [\"the\", \"she\", \"he\", \"it\", \"mr\"]\n",
    "\n",
    "for start in start_words:\n",
    "    generated = model.generate(start, length=12)\n",
    "    print(f\"\\n'{start}' → {generated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f427bc",
   "metadata": {},
   "source": [
    "### Text Generation: How It Works Step by Step\n",
    "\n",
    "The `generate()` method uses **random sampling** weighted by probabilities:\n",
    "\n",
    "```\n",
    "Start: \"mr\"\n",
    "Step 1: P(next | \"mr\") = {\"knightley\": 0.40, \"woodhouse\": 0.30, \"elton\": 0.20, ...}\n",
    "        Random choice → \"knightley\" (40% chance)\n",
    "        \n",
    "Step 2: P(next | \"knightley\") = {\"was\": 0.25, \"had\": 0.20, \"could\": 0.15, ...}\n",
    "        Random choice → \"was\" (25% chance)\n",
    "        \n",
    "Step 3: P(next | \"was\") = {\"not\": 0.15, \"a\": 0.12, \"very\": 0.10, ...}\n",
    "        ...continue...\n",
    "```\n",
    "\n",
    "**Key insight**: Each word choice is **probabilistic**, so running generation multiple times gives different results!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91695e57",
   "metadata": {},
   "source": [
    "## 10.7 NLTK's Language Model\n",
    "\n",
    "NLTK provides a more sophisticated implementation with:\n",
    "- Built-in padding handling\n",
    "- Support for different n-gram sizes\n",
    "- Various smoothing techniques (we'll use MLE - Maximum Likelihood Estimation)\n",
    "\n",
    "### MLE vs Smoothed Models\n",
    "\n",
    "| Model | Handles Unseen N-grams? | Use Case |\n",
    "|-------|------------------------|----------|\n",
    "| **MLE** | ❌ Returns 0 probability | When training data is comprehensive |\n",
    "| **Laplace** | ✅ Adds 1 to all counts | Simple smoothing |\n",
    "| **Kneser-Ney** | ✅ Advanced smoothing | Production systems |\n",
    "\n",
    "For learning purposes, MLE is clearest to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01b03f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 378\n",
      "Sample: ['emma', 'by', 'jane', 'austen', 'volume', 'i', 'chapter', 'i', 'emma', 'woodhouse']\n"
     ]
    }
   ],
   "source": [
    "from nltk.lm import MLE\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "\n",
    "# Prepare training data\n",
    "text = gutenberg.raw('austen-emma.txt')[:50000]\n",
    "sentences = sent_tokenize(text)\n",
    "tokenized_sents = [word_tokenize(s.lower()) for s in sentences]\n",
    "tokenized_sents = [[t for t in s if t.isalpha()] for s in tokenized_sents]\n",
    "\n",
    "# Remove empty sentences\n",
    "tokenized_sents = [s for s in tokenized_sents if len(s) > 0]\n",
    "\n",
    "print(f\"Number of sentences: {len(tokenized_sents)}\")\n",
    "print(f\"Sample: {tokenized_sents[0][:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b47a9d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 1,650\n"
     ]
    }
   ],
   "source": [
    "# Create training data with padding\n",
    "n = 3  # trigram model\n",
    "train_data, vocab = padded_everygram_pipeline(n, tokenized_sents)\n",
    "\n",
    "# Train MLE (Maximum Likelihood Estimation) model\n",
    "lm = MLE(n)\n",
    "lm.fit(train_data, vocab)\n",
    "\n",
    "print(f\"Vocabulary size: {len(lm.vocab):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db38bfd6",
   "metadata": {},
   "source": [
    "### What `padded_everygram_pipeline` Does\n",
    "\n",
    "This function prepares training data by:\n",
    "\n",
    "1. **Padding each sentence** with `<s>` and `</s>` markers\n",
    "2. **Generating all n-grams** from 1 to n (hence \"everygram\")\n",
    "3. **Building a vocabulary** of all unique words\n",
    "\n",
    "For a trigram model (n=3), each sentence generates:\n",
    "- Unigrams: (word1), (word2), ...\n",
    "- Bigrams: (word1, word2), (word2, word3), ...\n",
    "- Trigrams: (word1, word2, word3), ...\n",
    "\n",
    "The model uses **backoff**: if it hasn't seen a trigram, it falls back to bigram probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "efa7a741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(word | context)\n",
      "----------------------------------------\n",
      "P(very | she was) = 0.0455\n",
      "P(not | she was) = 0.0455\n",
      "P(knightley | mr) = 0.0000\n",
      "P(woodhouse | mr) = 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Score some words given context\n",
    "print(\"P(word | context)\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "contexts = [\n",
    "    ([\"she\", \"was\"], \"very\"),\n",
    "    ([\"she\", \"was\"], \"not\"),\n",
    "    ([\"mr\"], \"knightley\"),\n",
    "    ([\"mr\"], \"woodhouse\"),\n",
    "]\n",
    "\n",
    "for context, word in contexts:\n",
    "    prob = lm.score(word, context)\n",
    "    print(f\"P({word} | {' '.join(context)}) = {prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f356b1",
   "metadata": {},
   "source": [
    "### Scoring Words: The `score()` Method\n",
    "\n",
    "`lm.score(word, context)` returns $P(\\text{word} | \\text{context})$\n",
    "\n",
    "For a trigram model with context `[\"she\", \"was\"]`:\n",
    "- It looks up: How often does \"very\" follow \"she was\"?\n",
    "- Divides by: How often does \"she was\" appear?\n",
    "\n",
    "If the exact trigram isn't found, it may back off to bigram or unigram."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190f410b",
   "metadata": {},
   "source": [
    "### Understanding Padding Tokens in Generation\n",
    "\n",
    "The `padded_everygram_pipeline` adds special boundary markers:\n",
    "- `<s>` - Start of sentence\n",
    "- `</s>` - End of sentence\n",
    "\n",
    "**Why Generation Can Be Short**\n",
    "\n",
    "When the model generates without a seed, it starts from `<s>`:\n",
    "1. `<s>` → model picks a sentence-starting word\n",
    "2. Eventually generates `</s>` (learned as \"end of sentence\")\n",
    "3. After `</s>`, the most likely next token is... another `</s>`!\n",
    "\n",
    "This is because in training data, `</s>` is followed by `<s>` (next sentence) or nothing. The model gets \"stuck\" generating end markers.\n",
    "\n",
    "**Solution**: Use `text_seed` to start with real words, giving the model meaningful context.\n",
    "\n",
    "### Why Trigram Models Need 2-Word Seeds\n",
    "\n",
    "For an n-gram model, the context size is **n-1 words**:\n",
    "\n",
    "| Model | Context Size | Good Seed |\n",
    "|-------|-------------|-----------|\n",
    "| Bigram (n=2) | 1 word | `[\"she\"]` |\n",
    "| Trigram (n=3) | 2 words | `[\"she\", \"was\"]` |\n",
    "| 4-gram (n=4) | 3 words | `[\"she\", \"was\", \"very\"]` |\n",
    "\n",
    "With insufficient context, the model can't find matching n-grams and may stop early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e2ea3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text (NLTK MLE model):\n",
      "==================================================\n",
      "1. (she was...) sure whenever he does not read\n",
      "2. (the young...) man had made highbury feel a sort of pride and importance which the connexion would\n",
      "3. (mr knightley...) to mean\n",
      "4. (it was...) most unlikely therefore that he had made his fortune entirely to make atonement to herself\n",
      "5. (emma could...) not walk half so far\n"
     ]
    }
   ],
   "source": [
    "# Generate text using NLTK's model\n",
    "print(\"Generated text (NLTK MLE model):\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def generate_clean_text(model, num_words=15, seed=None, text_seed=None):\n",
    "    \"\"\"Generate text with a starting word to avoid short outputs\"\"\"\n",
    "    # Use text_seed to start generation with actual words (not <s>)\n",
    "    raw = model.generate(num_words, text_seed=text_seed, random_seed=seed)\n",
    "    # Filter out any padding tokens\n",
    "    cleaned = [word for word in raw if word not in ['<s>', '</s>']]\n",
    "    return ' '.join(cleaned)\n",
    "\n",
    "# Use different starting words/phrases for better results\n",
    "# Using 2-word seeds works better with trigram model\n",
    "start_words = [\n",
    "    ['she', 'was'],\n",
    "    ['the', 'young'],\n",
    "    ['mr', 'knightley'],\n",
    "    ['it', 'was'],\n",
    "    ['emma', 'could']\n",
    "]\n",
    "\n",
    "for i, seed_words in enumerate(start_words):\n",
    "    text = generate_clean_text(lm, num_words=15, seed=i*10, text_seed=seed_words)\n",
    "    print(f\"{i+1}. ({' '.join(seed_words)}...) {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6809cef0",
   "metadata": {},
   "source": [
    "### Text Generation Deep Dive\n",
    "\n",
    "The `lm.generate()` method works like this:\n",
    "\n",
    "```python\n",
    "def generate(num_words, text_seed, random_seed):\n",
    "    # 1. Start with the seed words as context\n",
    "    context = text_seed  # e.g., [\"she\", \"was\"]\n",
    "    output = list(context)\n",
    "    \n",
    "    # 2. For each word to generate:\n",
    "    for _ in range(num_words):\n",
    "        # Get probability distribution over all words given context\n",
    "        # P(word | context[-2:]) for trigram\n",
    "        probs = get_next_word_distribution(context)\n",
    "        \n",
    "        # Randomly sample from distribution\n",
    "        next_word = random_sample(probs)\n",
    "        \n",
    "        # Add to output and update context\n",
    "        output.append(next_word)\n",
    "        context = output[-(n-1):]  # Keep last n-1 words\n",
    "    \n",
    "    return output\n",
    "```\n",
    "\n",
    "**Key points**:\n",
    "1. **Context window slides**: Only the last n-1 words matter\n",
    "2. **Random sampling**: Same seed gives different outputs each run (unless `random_seed` is set)\n",
    "3. **Probability-weighted**: Common continuations are chosen more often"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cd230c",
   "metadata": {},
   "source": [
    "## 10.8 Practical: N-Gram Text Analysis\n",
    "\n",
    "Let's put it all together with a reusable analysis function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bef415ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_ngrams(text, n=2, top_k=10, remove_stopwords=True):\n",
    "    \"\"\"Comprehensive n-gram analysis\"\"\"\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [t for t in tokens if t.isalpha()]\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [t for t in tokens if t not in stop_words]\n",
    "    \n",
    "    # Generate n-grams\n",
    "    grams = list(ngrams(tokens, n))\n",
    "    freq = Counter(grams)\n",
    "    \n",
    "    return {\n",
    "        'total_ngrams': len(grams),\n",
    "        'unique_ngrams': len(freq),\n",
    "        'top_ngrams': freq.most_common(top_k),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "13cd1a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:\n",
      "Machine learning is a subset of artificial intelligence.\n",
      "Machine learning enables computers to learn from data.\n",
      "Deep learning is a subset of machine learning.\n",
      "Natural language processing uses machine learning.\n",
      "Machine learning models can process natural language.\n",
      "\n",
      "==================================================\n",
      "\n",
      "1-grams Analysis:\n",
      "  Total: 28\n",
      "  Unique: 16\n",
      "  Top 1-grams:\n",
      "    learning: 6\n",
      "    machine: 5\n",
      "    subset: 2\n",
      "    natural: 2\n",
      "    language: 2\n",
      "    artificial: 1\n",
      "    intelligence: 1\n",
      "    enables: 1\n",
      "    computers: 1\n",
      "    learn: 1\n",
      "\n",
      "2-grams Analysis:\n",
      "  Total: 27\n",
      "  Unique: 21\n",
      "  Top 2-grams:\n",
      "    machine learning: 5\n",
      "    learning subset: 2\n",
      "    natural language: 2\n",
      "    subset artificial: 1\n",
      "    artificial intelligence: 1\n",
      "    intelligence machine: 1\n",
      "    learning enables: 1\n",
      "    enables computers: 1\n",
      "    computers learn: 1\n",
      "    learn data: 1\n",
      "\n",
      "3-grams Analysis:\n",
      "  Total: 26\n",
      "  Unique: 26\n",
      "  Top 3-grams:\n",
      "    machine learning subset: 1\n",
      "    learning subset artificial: 1\n",
      "    subset artificial intelligence: 1\n",
      "    artificial intelligence machine: 1\n",
      "    intelligence machine learning: 1\n",
      "    machine learning enables: 1\n",
      "    learning enables computers: 1\n",
      "    enables computers learn: 1\n",
      "    computers learn data: 1\n",
      "    learn data deep: 1\n"
     ]
    }
   ],
   "source": [
    "# Analyze a text\n",
    "text = \"\"\"Machine learning is a subset of artificial intelligence.\n",
    "Machine learning enables computers to learn from data.\n",
    "Deep learning is a subset of machine learning.\n",
    "Natural language processing uses machine learning.\n",
    "Machine learning models can process natural language.\"\"\"\n",
    "\n",
    "print(f\"Text:\\n{text}\\n\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for n in [1, 2, 3]:\n",
    "    result = analyze_ngrams(text, n=n, remove_stopwords=True)\n",
    "    \n",
    "    print(f\"\\n{n}-grams Analysis:\")\n",
    "    print(f\"  Total: {result['total_ngrams']}\")\n",
    "    print(f\"  Unique: {result['unique_ngrams']}\")\n",
    "    print(f\"  Top {n}-grams:\")\n",
    "    for gram, count in result['top_ngrams']:\n",
    "        print(f\"    {' '.join(gram)}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8fa37a",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### N-Gram Functions\n",
    "\n",
    "| Function | Description |\n",
    "|----------|-------------|\n",
    "| `ngrams(tokens, n)` | Generate n-grams of any size |\n",
    "| `bigrams(tokens)` | Shortcut for `ngrams(tokens, 2)` |\n",
    "| `trigrams(tokens)` | Shortcut for `ngrams(tokens, 3)` |\n",
    "| `pad_both_ends(tokens, n)` | Add `<s>` and `</s>` markers |\n",
    "| `padded_everygram_pipeline(n, sents)` | Full preprocessing for LM training |\n",
    "\n",
    "### Collocation Finders\n",
    "\n",
    "| Class | Use |\n",
    "|-------|-----|\n",
    "| `BigramCollocationFinder` | Find significant 2-word phrases |\n",
    "| `TrigramCollocationFinder` | Find significant 3-word phrases |\n",
    "\n",
    "### Collocation Measures\n",
    "\n",
    "| Measure | Formula Intuition | Best For |\n",
    "|---------|-------------------|----------|\n",
    "| **PMI** | $\\log \\frac{P(x,y)}{P(x)P(y)}$ | Rare meaningful phrases |\n",
    "| **Chi-Square** | Statistical test for independence | Technical terms |\n",
    "| **Likelihood Ratio** | Ratio of hypotheses | Balanced results |\n",
    "\n",
    "### Language Model Key Concepts\n",
    "\n",
    "| Concept | Explanation |\n",
    "|---------|-------------|\n",
    "| **Markov Assumption** | Next word depends only on previous n-1 words |\n",
    "| **MLE** | Estimate P(word\\|context) by counting |\n",
    "| **Padding** | `<s>` and `</s>` tokens mark sentence boundaries |\n",
    "| **Context Window** | The n-1 words used to predict the next word |\n",
    "| **Backoff** | Fall back to smaller n-grams if exact match not found |\n",
    "\n",
    "### Text Generation Process\n",
    "\n",
    "1. **Initialize** with seed words (n-1 words for n-gram model)\n",
    "2. **Look up** probability distribution for next word given context\n",
    "3. **Sample** randomly from distribution (weighted by probability)\n",
    "4. **Slide** context window and repeat\n",
    "\n",
    "### Limitations of N-Gram Models\n",
    "\n",
    "| Limitation | Why It Happens |\n",
    "|------------|----------------|\n",
    "| **Short memory** | Only sees n-1 previous words |\n",
    "| **Data sparsity** | Many valid n-grams never seen in training |\n",
    "| **No semantics** | Treats words as arbitrary symbols |\n",
    "| **Large storage** | Must store all observed n-grams |\n",
    "\n",
    "### When to Use N-Grams\n",
    "\n",
    "✅ **Good for**: Autocomplete, spell checking, simple text generation, keyphrase extraction, language identification\n",
    "\n",
    "❌ **Not ideal for**: Long-form coherent text, understanding meaning, handling rare words\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Smoothing techniques**: Laplace, Good-Turing, Kneser-Ney\n",
    "- **Neural language models**: Word2Vec, LSTM, Transformers (GPT)\n",
    "- **Perplexity**: Evaluating language model quality"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
