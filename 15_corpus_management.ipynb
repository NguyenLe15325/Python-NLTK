{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b6dbd25",
   "metadata": {},
   "source": [
    "# NLTK Complete Guide - Section 15: Corpus Management\n",
    "\n",
    "This notebook covers:\n",
    "- Built-in Corpora\n",
    "- Loading Custom Corpora\n",
    "- Creating Your Own Corpus\n",
    "- Corpus Readers\n",
    "- Practical Applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1873c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "\n",
    "nltk.download('gutenberg', quiet=True)\n",
    "nltk.download('brown', quiet=True)\n",
    "nltk.download('reuters', quiet=True)\n",
    "nltk.download('inaugural', quiet=True)\n",
    "nltk.download('webtext', quiet=True)\n",
    "nltk.download('nps_chat', quiet=True)\n",
    "nltk.download('treebank', quiet=True)\n",
    "\n",
    "from nltk.corpus import gutenberg, brown, reuters, inaugural, webtext\n",
    "from nltk.corpus import PlaintextCorpusReader, TaggedCorpusReader\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cc9069",
   "metadata": {},
   "source": [
    "## 15.1 Built-in Corpora Overview\n",
    "\n",
    "NLTK includes many corpora for different purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf2439b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available corpora\n",
    "corpora_info = {\n",
    "    'gutenberg': 'Classic literature (18 texts)',\n",
    "    'brown': 'Categorized text (news, fiction, etc.)',\n",
    "    'reuters': 'News articles with categories',\n",
    "    'inaugural': 'US Presidential inaugural addresses',\n",
    "    'webtext': 'Web and chat text',\n",
    "    'treebank': 'Parsed Wall Street Journal',\n",
    "    'movie_reviews': 'Positive/negative movie reviews',\n",
    "    'stopwords': 'Stop words in multiple languages',\n",
    "    'wordnet': 'Lexical database',\n",
    "    'names': 'Male and female names',\n",
    "}\n",
    "\n",
    "print(\"Popular NLTK Corpora\")\n",
    "print(\"=\" * 55)\n",
    "for corpus, description in corpora_info.items():\n",
    "    print(f\"{corpus:<15} {description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c34c9ed",
   "metadata": {},
   "source": [
    "## 15.2 Gutenberg Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5d8d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List files in Gutenberg corpus\n",
    "print(\"Gutenberg Files:\")\n",
    "print(\"-\" * 40)\n",
    "for fileid in gutenberg.fileids():\n",
    "    words = len(gutenberg.words(fileid))\n",
    "    print(f\"{fileid:<30} {words:>8,} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dcba03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access methods\n",
    "fileid = 'austen-emma.txt'\n",
    "\n",
    "print(f\"Accessing '{fileid}':\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Raw text\n",
    "raw = gutenberg.raw(fileid)\n",
    "print(f\"Raw text (first 200 chars): {raw[:200]}...\")\n",
    "\n",
    "# Words\n",
    "words = gutenberg.words(fileid)\n",
    "print(f\"\\nWords (first 20): {list(words[:20])}\")\n",
    "\n",
    "# Sentences\n",
    "sents = gutenberg.sents(fileid)\n",
    "print(f\"\\nFirst sentence: {list(sents[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5945fb9a",
   "metadata": {},
   "source": [
    "## 15.3 Brown Corpus (Categorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d44fa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categories in Brown corpus\n",
    "print(\"Brown Corpus Categories:\")\n",
    "print(\"-\" * 40)\n",
    "for cat in brown.categories():\n",
    "    words = len(brown.words(categories=cat))\n",
    "    files = len(brown.fileids(categories=cat))\n",
    "    print(f\"{cat:<20} {files:>3} files, {words:>8,} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5a0355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access by category\n",
    "news_words = brown.words(categories='news')\n",
    "fiction_words = brown.words(categories='fiction')\n",
    "\n",
    "print(f\"News words: {len(news_words):,}\")\n",
    "print(f\"Fiction words: {len(fiction_words):,}\")\n",
    "\n",
    "# Multiple categories\n",
    "multi_words = brown.words(categories=['news', 'editorial'])\n",
    "print(f\"News + Editorial words: {len(multi_words):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8613ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tagged words (Brown has POS tags)\n",
    "tagged = brown.tagged_words(categories='news')[:10]\n",
    "print(\"Tagged words (news):\")\n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed25551",
   "metadata": {},
   "source": [
    "## 15.4 Reuters Corpus (Multi-label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de62d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Reuters files: {len(reuters.fileids())}\")\n",
    "print(f\"Reuters categories: {len(reuters.categories())}\")\n",
    "print(f\"\\nSample categories: {reuters.categories()[:15]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f37a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files can have multiple categories\n",
    "sample_file = reuters.fileids()[0]\n",
    "print(f\"File: {sample_file}\")\n",
    "print(f\"Categories: {reuters.categories(sample_file)}\")\n",
    "print(f\"\\nText: {reuters.raw(sample_file)[:300]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183fb18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split (built-in)\n",
    "train_files = [f for f in reuters.fileids() if f.startswith('training/')]\n",
    "test_files = [f for f in reuters.fileids() if f.startswith('test/')]\n",
    "\n",
    "print(f\"Training files: {len(train_files)}\")\n",
    "print(f\"Test files: {len(test_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a325c28d",
   "metadata": {},
   "source": [
    "## 15.5 Creating Custom Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae02320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample corpus directory\n",
    "corpus_dir = './my_corpus'\n",
    "os.makedirs(corpus_dir, exist_ok=True)\n",
    "\n",
    "# Create sample files\n",
    "texts = {\n",
    "    'doc1.txt': \"\"\"Natural language processing is a field of computer science.\n",
    "It deals with the interaction between computers and humans.\n",
    "NLP is used in many applications today.\"\"\",\n",
    "    \n",
    "    'doc2.txt': \"\"\"Machine learning is transforming how we build software.\n",
    "Deep learning models can understand complex patterns.\n",
    "AI is becoming more accessible to developers.\"\"\",\n",
    "    \n",
    "    'doc3.txt': \"\"\"Python is a popular programming language.\n",
    "It is widely used in data science and web development.\n",
    "Python has a rich ecosystem of libraries.\"\"\",\n",
    "}\n",
    "\n",
    "for filename, content in texts.items():\n",
    "    with open(os.path.join(corpus_dir, filename), 'w') as f:\n",
    "        f.write(content)\n",
    "\n",
    "print(f\"Created corpus in '{corpus_dir}' with {len(texts)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30a1003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load custom corpus with PlaintextCorpusReader\n",
    "my_corpus = PlaintextCorpusReader(corpus_dir, r'.*\\.txt')\n",
    "\n",
    "print(\"Custom Corpus:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Files: {my_corpus.fileids()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c21eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access methods work the same as built-in corpora\n",
    "print(f\"\\nTotal words: {len(my_corpus.words())}\")\n",
    "print(f\"Total sentences: {len(my_corpus.sents())}\")\n",
    "\n",
    "print(f\"\\nWords in doc1.txt: {list(my_corpus.words('doc1.txt'))}\")\n",
    "\n",
    "print(f\"\\nFirst sentence of doc2.txt: {list(my_corpus.sents('doc2.txt')[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b2f033",
   "metadata": {},
   "source": [
    "## 15.6 Categorized Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5755ef9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import CategorizedPlaintextCorpusReader\n",
    "\n",
    "# Create categorized corpus directory\n",
    "cat_corpus_dir = './categorized_corpus'\n",
    "os.makedirs(f'{cat_corpus_dir}/tech', exist_ok=True)\n",
    "os.makedirs(f'{cat_corpus_dir}/science', exist_ok=True)\n",
    "\n",
    "# Tech documents\n",
    "tech_docs = {\n",
    "    'tech/software.txt': \"Software development requires programming skills and creativity.\",\n",
    "    'tech/hardware.txt': \"Computer hardware includes processors, memory, and storage devices.\",\n",
    "}\n",
    "\n",
    "# Science documents\n",
    "science_docs = {\n",
    "    'science/biology.txt': \"Biology studies living organisms and their interactions.\",\n",
    "    'science/physics.txt': \"Physics explains the fundamental laws of the universe.\",\n",
    "}\n",
    "\n",
    "for filepath, content in {**tech_docs, **science_docs}.items():\n",
    "    with open(os.path.join(cat_corpus_dir, filepath), 'w') as f:\n",
    "        f.write(content)\n",
    "\n",
    "print(\"Created categorized corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6622ac44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load categorized corpus (categories from directory structure)\n",
    "cat_corpus = CategorizedPlaintextCorpusReader(\n",
    "    cat_corpus_dir,\n",
    "    r'.*/.*\\.txt',\n",
    "    cat_pattern=r'(\\w+)/.*'  # Category from first directory\n",
    ")\n",
    "\n",
    "print(f\"Categories: {cat_corpus.categories()}\")\n",
    "print(f\"All files: {cat_corpus.fileids()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9128f578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access by category\n",
    "print(f\"\\nTech files: {cat_corpus.fileids(categories='tech')}\")\n",
    "print(f\"Science files: {cat_corpus.fileids(categories='science')}\")\n",
    "\n",
    "print(f\"\\nTech words: {list(cat_corpus.words(categories='tech'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163b316c",
   "metadata": {},
   "source": [
    "## 15.7 Corpus Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fd6361",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_statistics(corpus, name=\"Corpus\"):\n",
    "    \"\"\"Calculate comprehensive corpus statistics\"\"\"\n",
    "    stats = {\n",
    "        'name': name,\n",
    "        'files': len(corpus.fileids()),\n",
    "        'words': len(corpus.words()),\n",
    "        'unique_words': len(set(w.lower() for w in corpus.words() if w.isalpha())),\n",
    "        'sentences': len(corpus.sents()),\n",
    "        'chars': len(corpus.raw()),\n",
    "    }\n",
    "    \n",
    "    stats['avg_word_length'] = sum(len(w) for w in corpus.words() if w.isalpha()) / stats['words']\n",
    "    stats['avg_sent_length'] = stats['words'] / stats['sentences']\n",
    "    stats['lexical_diversity'] = stats['unique_words'] / stats['words']\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea874301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare corpora statistics\n",
    "corpora_to_analyze = [\n",
    "    (gutenberg, 'Gutenberg'),\n",
    "    (brown, 'Brown'),\n",
    "    (inaugural, 'Inaugural'),\n",
    "]\n",
    "\n",
    "print(\"Corpus Statistics Comparison\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for corpus, name in corpora_to_analyze:\n",
    "    stats = corpus_statistics(corpus, name)\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Files: {stats['files']:,}\")\n",
    "    print(f\"  Words: {stats['words']:,}\")\n",
    "    print(f\"  Unique words: {stats['unique_words']:,}\")\n",
    "    print(f\"  Sentences: {stats['sentences']:,}\")\n",
    "    print(f\"  Avg word length: {stats['avg_word_length']:.2f}\")\n",
    "    print(f\"  Avg sentence length: {stats['avg_sent_length']:.1f} words\")\n",
    "    print(f\"  Lexical diversity: {stats['lexical_diversity']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042e2b4d",
   "metadata": {},
   "source": [
    "## 15.8 Corpus Utility Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2faeed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorpusManager:\n",
    "    \"\"\"Utility class for corpus management\"\"\"\n",
    "    \n",
    "    def __init__(self, corpus_path, pattern=r'.*\\.txt'):\n",
    "        self.path = corpus_path\n",
    "        self.corpus = PlaintextCorpusReader(corpus_path, pattern)\n",
    "    \n",
    "    def add_document(self, filename, content):\n",
    "        \"\"\"Add a new document to the corpus\"\"\"\n",
    "        filepath = os.path.join(self.path, filename)\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            f.write(content)\n",
    "        # Reload corpus\n",
    "        self.corpus = PlaintextCorpusReader(self.path, r'.*\\.txt')\n",
    "    \n",
    "    def get_statistics(self):\n",
    "        \"\"\"Get corpus statistics\"\"\"\n",
    "        return corpus_statistics(self.corpus, self.path)\n",
    "    \n",
    "    def search(self, term):\n",
    "        \"\"\"Search for term in corpus\"\"\"\n",
    "        results = []\n",
    "        for fileid in self.corpus.fileids():\n",
    "            text = self.corpus.raw(fileid).lower()\n",
    "            if term.lower() in text:\n",
    "                count = text.count(term.lower())\n",
    "                results.append((fileid, count))\n",
    "        return sorted(results, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    def get_concordance(self, word, width=40):\n",
    "        \"\"\"Get concordance for a word\"\"\"\n",
    "        from nltk import Text\n",
    "        text = Text(self.corpus.words())\n",
    "        text.concordance(word, width=width)\n",
    "    \n",
    "    def vocabulary(self, min_freq=1):\n",
    "        \"\"\"Get vocabulary with frequency filter\"\"\"\n",
    "        from collections import Counter\n",
    "        words = [w.lower() for w in self.corpus.words() if w.isalpha()]\n",
    "        freq = Counter(words)\n",
    "        return {w: c for w, c in freq.items() if c >= min_freq}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee08a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the manager\n",
    "manager = CorpusManager('./my_corpus')\n",
    "\n",
    "# Add a new document\n",
    "manager.add_document('doc4.txt', \"\"\"Data analysis is essential for business intelligence.\n",
    "Visualization helps communicate insights effectively.\n",
    "Python and R are popular tools for data analysis.\"\"\")\n",
    "\n",
    "print(f\"Files: {manager.corpus.fileids()}\")\n",
    "\n",
    "# Search\n",
    "print(f\"\\nSearch for 'Python': {manager.search('python')}\")\n",
    "\n",
    "# Statistics\n",
    "stats = manager.get_statistics()\n",
    "print(f\"\\nTotal words: {stats['words']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39949571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "import shutil\n",
    "shutil.rmtree('./my_corpus', ignore_errors=True)\n",
    "shutil.rmtree('./categorized_corpus', ignore_errors=True)\n",
    "print(\"Cleaned up temporary corpus directories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa05e9f",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Corpus Reader | Use Case |\n",
    "|---------------|----------|\n",
    "| `PlaintextCorpusReader` | Plain text files |\n",
    "| `CategorizedPlaintextCorpusReader` | Categorized text |\n",
    "| `TaggedCorpusReader` | POS-tagged text |\n",
    "| `BracketParseCorpusReader` | Parsed trees |\n",
    "\n",
    "### Common Methods\n",
    "- `corpus.fileids()` - List files\n",
    "- `corpus.raw()` - Raw text\n",
    "- `corpus.words()` - Tokenized words\n",
    "- `corpus.sents()` - Sentences\n",
    "- `corpus.categories()` - Categories (if available)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
