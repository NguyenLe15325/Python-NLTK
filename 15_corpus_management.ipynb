{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b6dbd25",
   "metadata": {},
   "source": [
    "# NLTK Complete Guide - Section 15: Corpus Management\n",
    "\n",
    "## What is a Corpus?\n",
    "\n",
    "A **corpus** (plural: corpora) is a large, structured collection of text used for linguistic research and NLP tasks. Think of it as a dataset specifically designed for text analysis.\n",
    "\n",
    "### Why Corpora Matter\n",
    "\n",
    "| Purpose | Description |\n",
    "|---------|-------------|\n",
    "| **Training Data** | Machine learning models need labeled text |\n",
    "| **Linguistic Research** | Study language patterns, frequency, usage |\n",
    "| **Benchmarking** | Standard datasets for comparing algorithms |\n",
    "| **Vocabulary Building** | Create word lists, dictionaries |\n",
    "| **Domain Adaptation** | Customize NLP tools for specific fields |\n",
    "\n",
    "### Types of Corpora\n",
    "\n",
    "| Type | Description | Example |\n",
    "|------|-------------|---------|\n",
    "| **Raw Text** | Plain text without annotations | Project Gutenberg books |\n",
    "| **Categorized** | Text organized by topic/category | Brown Corpus (news, fiction, etc.) |\n",
    "| **Tagged** | Words labeled with POS tags | Penn Treebank |\n",
    "| **Annotated** | Multiple annotation layers | Named entities, syntax trees |\n",
    "| **Parallel** | Same text in multiple languages | Europarl (EU proceedings) |\n",
    "\n",
    "### What This Notebook Covers\n",
    "\n",
    "| Topic | Description |\n",
    "|-------|-------------|\n",
    "| **Built-in Corpora** | Explore NLTK's rich corpus collection |\n",
    "| **Corpus Readers** | Access methods for different corpus types |\n",
    "| **Custom Corpora** | Create your own corpus from files |\n",
    "| **Categorization** | Organize documents by category |\n",
    "| **Statistics** | Analyze corpus properties |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce1873c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "\n",
    "nltk.download('gutenberg', quiet=True)\n",
    "nltk.download('brown', quiet=True)\n",
    "nltk.download('reuters', quiet=True)\n",
    "nltk.download('inaugural', quiet=True)\n",
    "nltk.download('webtext', quiet=True)\n",
    "nltk.download('nps_chat', quiet=True)\n",
    "nltk.download('treebank', quiet=True)\n",
    "\n",
    "from nltk.corpus import gutenberg, brown, reuters, inaugural, webtext\n",
    "from nltk.corpus import PlaintextCorpusReader, TaggedCorpusReader\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6224b3",
   "metadata": {},
   "source": [
    "### Understanding the Imports\n",
    "\n",
    "| Import | Purpose |\n",
    "|--------|---------|\n",
    "| `gutenberg` | Classic literature from Project Gutenberg |\n",
    "| `brown` | Categorized text from various genres |\n",
    "| `reuters` | News articles with topic categories |\n",
    "| `inaugural` | US Presidential inaugural addresses |\n",
    "| `webtext` | Internet text (forums, reviews, etc.) |\n",
    "| `PlaintextCorpusReader` | Reader for your own text files |\n",
    "| `TaggedCorpusReader` | Reader for POS-tagged text |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cc9069",
   "metadata": {},
   "source": [
    "## 15.1 Built-in Corpora Overview\n",
    "\n",
    "NLTK comes with **over 100 corpora** covering various languages, domains, and annotation types. These are invaluable for:\n",
    "- Learning NLP techniques\n",
    "- Prototyping and testing\n",
    "- Benchmarking algorithms\n",
    "\n",
    "### Corpus Access Pattern\n",
    "\n",
    "All NLTK corpora follow a consistent access pattern:\n",
    "\n",
    "```python\n",
    "from nltk.corpus import corpus_name\n",
    "\n",
    "# Common methods available:\n",
    "corpus_name.fileids()      # List of files\n",
    "corpus_name.raw()          # Raw text as string\n",
    "corpus_name.words()        # List of words\n",
    "corpus_name.sents()        # List of sentences\n",
    "corpus_name.categories()   # Categories (if applicable)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "faf2439b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POPULAR NLTK CORPORA\n",
      "===========================================================================\n",
      "Corpus          Description                         Common Use\n",
      "---------------------------------------------------------------------------\n",
      "gutenberg       Classic literature (18 texts)       Language modeling, style analysis\n",
      "brown           Categorized American English        POS tagging, genre classification\n",
      "reuters         News articles (10,788 docs)         Multi-label classification\n",
      "inaugural       US Presidential speeches            Historical language analysis\n",
      "webtext         Web and chat text                   Informal language processing\n",
      "treebank        Parsed Wall Street Journal          Syntactic parsing\n",
      "movie_reviews   2000 movie reviews                  Sentiment analysis\n",
      "stopwords       Stop words (multiple languages)     Text preprocessing\n",
      "wordnet         Lexical database                    Word meanings, synonyms\n",
      "names           8000+ male/female names             Name classification\n",
      "\n",
      "üí° All corpora can be downloaded with: nltk.download('corpus_name')\n"
     ]
    }
   ],
   "source": [
    "# Survey of popular NLTK corpora and their uses\n",
    "corpora_info = {\n",
    "    'gutenberg': ('Classic literature (18 texts)', 'Language modeling, style analysis'),\n",
    "    'brown': ('Categorized American English', 'POS tagging, genre classification'),\n",
    "    'reuters': ('News articles (10,788 docs)', 'Multi-label classification'),\n",
    "    'inaugural': ('US Presidential speeches', 'Historical language analysis'),\n",
    "    'webtext': ('Web and chat text', 'Informal language processing'),\n",
    "    'treebank': ('Parsed Wall Street Journal', 'Syntactic parsing'),\n",
    "    'movie_reviews': ('2000 movie reviews', 'Sentiment analysis'),\n",
    "    'stopwords': ('Stop words (multiple languages)', 'Text preprocessing'),\n",
    "    'wordnet': ('Lexical database', 'Word meanings, synonyms'),\n",
    "    'names': ('8000+ male/female names', 'Name classification'),\n",
    "}\n",
    "\n",
    "print(\"POPULAR NLTK CORPORA\")\n",
    "print(\"=\" * 75)\n",
    "print(f\"{'Corpus':<15} {'Description':<35} {'Common Use'}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for corpus, (description, use) in corpora_info.items():\n",
    "    print(f\"{corpus:<15} {description:<35} {use}\")\n",
    "\n",
    "print(\"\\nüí° All corpora can be downloaded with: nltk.download('corpus_name')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c34c9ed",
   "metadata": {},
   "source": [
    "## 15.2 Gutenberg Corpus\n",
    "\n",
    "The **Gutenberg Corpus** contains 18 classic literary works from Project Gutenberg, including:\n",
    "- Jane Austen novels\n",
    "- Shakespeare plays\n",
    "- Milton's Paradise Lost\n",
    "- The Bible (King James Version)\n",
    "\n",
    "### Why Use Gutenberg?\n",
    "\n",
    "- **Clean text** - Well-formatted literary prose\n",
    "- **Public domain** - No copyright issues\n",
    "- **Diverse styles** - Different authors and time periods\n",
    "- **Long texts** - Substantial content for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f5d8d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GUTENBERG CORPUS FILES\n",
      "=======================================================\n",
      "File                                  Words        Chars\n",
      "-------------------------------------------------------\n",
      "austen-emma.txt                     192,427      887,071\n",
      "austen-persuasion.txt                98,171      466,292\n",
      "austen-sense.txt                    141,576      673,022\n",
      "bible-kjv.txt                     1,010,654    4,332,554\n",
      "blake-poems.txt                       8,354       38,153\n",
      "bryant-stories.txt                   55,563      249,439\n",
      "burgess-busterbrown.txt              18,963       84,663\n",
      "carroll-alice.txt                    34,110      144,395\n",
      "chesterton-ball.txt                  96,996      457,450\n",
      "chesterton-brown.txt                 86,063      406,629\n",
      "chesterton-thursday.txt              69,213      320,525\n",
      "edgeworth-parents.txt               210,663      935,158\n",
      "melville-moby_dick.txt              260,819    1,242,990\n",
      "milton-paradise.txt                  96,825      468,220\n",
      "shakespeare-caesar.txt               25,833      112,310\n",
      "shakespeare-hamlet.txt               37,360      162,881\n",
      "shakespeare-macbeth.txt              23,140      100,351\n",
      "whitman-leaves.txt                  154,883      711,215\n",
      "-------------------------------------------------------\n",
      "TOTAL                             2,621,613\n",
      "\n",
      "üí° Corpus Statistics:\n",
      "   ‚Ä¢ 18 classic literary works\n",
      "   ‚Ä¢ Over 2,621,613 words total\n",
      "   ‚Ä¢ Spans multiple centuries of English\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explore the Gutenberg corpus\n",
    "print(\"GUTENBERG CORPUS FILES\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"{'File':<32} {'Words':>10} {'Chars':>12}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "total_words = 0\n",
    "for fileid in gutenberg.fileids():\n",
    "    words = len(gutenberg.words(fileid))\n",
    "    chars = len(gutenberg.raw(fileid))\n",
    "    total_words += words\n",
    "    print(f\"{fileid:<32} {words:>10,} {chars:>12,}\")\n",
    "\n",
    "print(\"-\" * 55)\n",
    "print(f\"{'TOTAL':<32} {total_words:>10,}\")\n",
    "\n",
    "print(f\"\"\"\n",
    "üí° Corpus Statistics:\n",
    "   ‚Ä¢ {len(gutenberg.fileids())} classic literary works\n",
    "   ‚Ä¢ Over {total_words:,} words total\n",
    "   ‚Ä¢ Spans multiple centuries of English\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90dcba03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCESSING 'austen-emma.txt'\n",
      "============================================================\n",
      "\n",
      "üìÑ raw() - Returns raw text string:\n",
      "   Length: 887,071 characters\n",
      "   Preview: \"[Emma by Jane Austen 1816]\n",
      "\n",
      "VOLUME I\n",
      "\n",
      "CHAPTER I\n",
      "\n",
      "\n",
      "Emma Woodhouse, handsome, clever, and rich, with a...\"\n",
      "\n",
      "üìù words() - Returns list of word tokens:\n",
      "   Count: 192,427 words\n",
      "   First 10: ['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', 'VOLUME', 'I', 'CHAPTER']\n",
      "\n",
      "üìñ sents() - Returns list of sentences:\n",
      "   Count: 7,752 sentences\n",
      "   First sentence: [ Emma by Jane Austen 1816 ]\n",
      "\n",
      "üí° When to use which method:\n",
      "   ‚Ä¢ raw()   ‚Üí Need original formatting, regex searching\n",
      "   ‚Ä¢ words() ‚Üí Word frequency, vocabulary analysis\n",
      "   ‚Ä¢ sents() ‚Üí Sentence-level analysis, n-grams\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Demonstrating different access methods for a corpus\n",
    "fileid = 'austen-emma.txt'\n",
    "\n",
    "print(f\"ACCESSING '{fileid}'\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# METHOD 1: raw() - Get raw text as a single string\n",
    "raw = gutenberg.raw(fileid)\n",
    "print(f\"\\nüìÑ raw() - Returns raw text string:\")\n",
    "print(f\"   Length: {len(raw):,} characters\")\n",
    "print(f\"   Preview: \\\"{raw[:100]}...\\\"\")\n",
    "\n",
    "# METHOD 2: words() - Get list of words (tokens)\n",
    "words = gutenberg.words(fileid)\n",
    "print(f\"\\nüìù words() - Returns list of word tokens:\")\n",
    "print(f\"   Count: {len(words):,} words\")\n",
    "print(f\"   First 10: {list(words[:10])}\")\n",
    "\n",
    "# METHOD 3: sents() - Get list of sentences (each is a list of words)\n",
    "sents = gutenberg.sents(fileid)\n",
    "print(f\"\\nüìñ sents() - Returns list of sentences:\")\n",
    "print(f\"   Count: {len(sents):,} sentences\")\n",
    "print(f\"   First sentence: {' '.join(sents[0])}\")\n",
    "\n",
    "print(f\"\"\"\n",
    "üí° When to use which method:\n",
    "   ‚Ä¢ raw()   ‚Üí Need original formatting, regex searching\n",
    "   ‚Ä¢ words() ‚Üí Word frequency, vocabulary analysis\n",
    "   ‚Ä¢ sents() ‚Üí Sentence-level analysis, n-grams\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5945fb9a",
   "metadata": {},
   "source": [
    "## 15.3 Brown Corpus (Categorized Text)\n",
    "\n",
    "The **Brown Corpus** is a groundbreaking corpus compiled in the 1960s at Brown University. It was the first major structured corpus of American English.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "| Feature | Description |\n",
    "|---------|-------------|\n",
    "| **Size** | ~1 million words |\n",
    "| **Categories** | 15 genres (news, fiction, academic, etc.) |\n",
    "| **POS Tagged** | Every word has a part-of-speech tag |\n",
    "| **Balanced** | Systematic sampling across genres |\n",
    "\n",
    "### Categories in Brown\n",
    "\n",
    "| Category | Description | Examples |\n",
    "|----------|-------------|----------|\n",
    "| `news` | Newspaper text | Press reporting |\n",
    "| `editorial` | Opinion pieces | Newspaper editorials |\n",
    "| `fiction` | Imaginative prose | Science fiction, romance |\n",
    "| `government` | Official documents | Reports, regulations |\n",
    "| `hobbies` | Special interest | Crafts, collecting |\n",
    "| `humor` | Comedic writing | Satire, jokes |\n",
    "| `learned` | Academic writing | Science, humanities |\n",
    "| `mystery` | Detective fiction | Crime novels |\n",
    "| `religion` | Religious writing | Sermons, theology |\n",
    "| `romance` | Love stories | Romantic fiction |\n",
    "| `science_fiction` | Sci-fi stories | Space, future |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d44fa95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BROWN CORPUS CATEGORIES\n",
      "============================================================\n",
      "Category                Files        Words  Avg Words/File\n",
      "------------------------------------------------------------\n",
      "adventure                  29       69,342           2,391\n",
      "belles_lettres             75      173,096           2,307\n",
      "editorial                  27       61,604           2,281\n",
      "fiction                    29       68,488           2,361\n",
      "government                 30       70,117           2,337\n",
      "hobbies                    36       82,345           2,287\n",
      "humor                       9       21,695           2,410\n",
      "learned                    80      181,888           2,273\n",
      "lore                       48      110,299           2,297\n",
      "mystery                    24       57,169           2,382\n",
      "news                       44      100,554           2,285\n",
      "religion                   17       39,399           2,317\n",
      "reviews                    17       40,704           2,394\n",
      "romance                    29       70,022           2,414\n",
      "science_fiction             6       14,470           2,411\n",
      "------------------------------------------------------------\n",
      "TOTAL                     500    1,161,192\n",
      "\n",
      "üí° The Brown Corpus is perfect for:\n",
      "   ‚Ä¢ Studying differences between genres\n",
      "   ‚Ä¢ Training POS taggers\n",
      "   ‚Ä¢ Building language models\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explore Brown corpus categories\n",
    "print(\"BROWN CORPUS CATEGORIES\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Category':<20} {'Files':>8} {'Words':>12} {'Avg Words/File':>15}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for cat in brown.categories():\n",
    "    files = len(brown.fileids(categories=cat))\n",
    "    words = len(brown.words(categories=cat))\n",
    "    avg = words // files\n",
    "    print(f\"{cat:<20} {files:>8} {words:>12,} {avg:>15,}\")\n",
    "\n",
    "total_files = len(brown.fileids())\n",
    "total_words = len(brown.words())\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'TOTAL':<20} {total_files:>8} {total_words:>12,}\")\n",
    "\n",
    "print(f\"\"\"\n",
    "üí° The Brown Corpus is perfect for:\n",
    "   ‚Ä¢ Studying differences between genres\n",
    "   ‚Ä¢ Training POS taggers\n",
    "   ‚Ä¢ Building language models\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d5a0355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCESSING TEXT BY CATEGORY\n",
      "=======================================================\n",
      "\n",
      "üì∞ News category:\n",
      "   Words: 100,554\n",
      "   Sample: The Fulton County Grand Jury said Friday an investigation of Atlanta's recent primary election produced...\n",
      "\n",
      "üìö Fiction category:\n",
      "   Words: 68,488\n",
      "   Sample: Thirty-three Scotty did not go back to school . His parents talked seriously and lengthily...\n",
      "\n",
      "üîÄ Combining categories:\n",
      "   News + Editorial + Government: 232,275 words\n",
      "\n",
      "üìÅ By file ID:\n",
      "   All file IDs: 500 files\n",
      "   Sample: ['ca01', 'ca02', 'ca03']\n"
     ]
    }
   ],
   "source": [
    "# Accessing text by category\n",
    "print(\"ACCESSING TEXT BY CATEGORY\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Single category\n",
    "news_words = brown.words(categories='news')\n",
    "fiction_words = brown.words(categories='fiction')\n",
    "\n",
    "print(f\"\\nüì∞ News category:\")\n",
    "print(f\"   Words: {len(news_words):,}\")\n",
    "print(f\"   Sample: {' '.join(news_words[:15])}...\")\n",
    "\n",
    "print(f\"\\nüìö Fiction category:\")\n",
    "print(f\"   Words: {len(fiction_words):,}\")\n",
    "print(f\"   Sample: {' '.join(fiction_words[:15])}...\")\n",
    "\n",
    "# Multiple categories combined\n",
    "print(f\"\\nüîÄ Combining categories:\")\n",
    "multi_words = brown.words(categories=['news', 'editorial', 'government'])\n",
    "print(f\"   News + Editorial + Government: {len(multi_words):,} words\")\n",
    "\n",
    "# By file ID\n",
    "print(f\"\\nüìÅ By file ID:\")\n",
    "print(f\"   All file IDs: {len(brown.fileids())} files\")\n",
    "print(f\"   Sample: {brown.fileids()[:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c8613ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS-TAGGED DATA IN BROWN CORPUS\n",
      "=======================================================\n",
      "\n",
      "Tagged words from 'news' category:\n",
      "[('The', 'AT'), ('Fulton', 'NP-TL'), ('County', 'NN-TL'), ('Grand', 'JJ-TL'), ('Jury', 'NN-TL'), ('said', 'VBD'), ('Friday', 'NR'), ('an', 'AT'), ('investigation', 'NN'), ('of', 'IN'), (\"Atlanta's\", 'NP$'), ('recent', 'JJ'), ('primary', 'NN'), ('election', 'NN'), ('produced', 'VBD')]\n",
      "\n",
      "Format: (word, POS_tag)\n",
      "\n",
      "Common Brown POS tags:\n",
      "  NN  = Noun (singular)\n",
      "  NNS = Noun (plural)\n",
      "  VB  = Verb (base form)\n",
      "  VBD = Verb (past tense)\n",
      "  JJ  = Adjective\n",
      "  RB  = Adverb\n",
      "  IN  = Preposition\n",
      "  AT  = Article (the, a)\n",
      "\n",
      "üí° This pre-tagged data is valuable for:\n",
      "   ‚Ä¢ Training your own POS tagger\n",
      "   ‚Ä¢ Studying word usage by category\n",
      "   ‚Ä¢ Grammar analysis\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Brown corpus is POS-tagged - each word has a part-of-speech tag\n",
    "print(\"POS-TAGGED DATA IN BROWN CORPUS\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Get tagged words\n",
    "tagged = brown.tagged_words(categories='news')[:15]\n",
    "\n",
    "print(f\"\\nTagged words from 'news' category:\")\n",
    "print(f\"{tagged}\")\n",
    "\n",
    "print(f\"\"\"\n",
    "Format: (word, POS_tag)\n",
    "\n",
    "Common Brown POS tags:\n",
    "  NN  = Noun (singular)\n",
    "  NNS = Noun (plural)\n",
    "  VB  = Verb (base form)\n",
    "  VBD = Verb (past tense)\n",
    "  JJ  = Adjective\n",
    "  RB  = Adverb\n",
    "  IN  = Preposition\n",
    "  AT  = Article (the, a)\n",
    "\n",
    "üí° This pre-tagged data is valuable for:\n",
    "   ‚Ä¢ Training your own POS tagger\n",
    "   ‚Ä¢ Studying word usage by category\n",
    "   ‚Ä¢ Grammar analysis\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed25551",
   "metadata": {},
   "source": [
    "## 15.4 Reuters Corpus (Multi-label Classification)\n",
    "\n",
    "The **Reuters Corpus** is a collection of news articles from Reuters newswire, commonly used for text classification research.\n",
    "\n",
    "### Key Features\n",
    "\n",
    "| Feature | Description |\n",
    "|---------|-------------|\n",
    "| **Size** | 10,788 news documents |\n",
    "| **Categories** | 90 topic categories |\n",
    "| **Multi-label** | Documents can have multiple categories |\n",
    "| **Pre-split** | Training and test sets defined |\n",
    "\n",
    "### Multi-label vs Multi-class\n",
    "\n",
    "| Type | Description | Example |\n",
    "|------|-------------|---------|\n",
    "| **Multi-class** | Each doc has ONE label | Email ‚Üí spam OR not spam |\n",
    "| **Multi-label** | Each doc can have MULTIPLE labels | News ‚Üí \"grain\" AND \"wheat\" AND \"trade\" |\n",
    "\n",
    "Reuters is multi-label: a news article about wheat exports might be tagged with both \"grain\" and \"trade\" categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4de62d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REUTERS CORPUS OVERVIEW\n",
      "=======================================================\n",
      "\n",
      "üìä Corpus Statistics:\n",
      "   Total documents: 10,788\n",
      "   Total categories: 90\n",
      "\n",
      "üìÅ Sample categories (first 15):\n",
      "   ['acq', 'alum', 'barley', 'bop', 'carcass', 'castor-oil', 'cocoa', 'coconut', 'coconut-oil', 'coffee', 'copper', 'copra-cake', 'corn', 'cotton', 'cotton-oil']\n",
      "\n",
      "üí° Most popular categories deal with:\n",
      "   ‚Ä¢ Commodities (grain, crude oil, coffee)\n",
      "   ‚Ä¢ Finance (money, interest rates)\n",
      "   ‚Ä¢ Trade (imports, exports)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explore Reuters corpus\n",
    "print(\"REUTERS CORPUS OVERVIEW\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "print(f\"\"\"\n",
    "üìä Corpus Statistics:\n",
    "   Total documents: {len(reuters.fileids()):,}\n",
    "   Total categories: {len(reuters.categories())}\n",
    "   \n",
    "üìÅ Sample categories (first 15):\n",
    "   {reuters.categories()[:15]}\n",
    "   \n",
    "üí° Most popular categories deal with:\n",
    "   ‚Ä¢ Commodities (grain, crude oil, coffee)\n",
    "   ‚Ä¢ Finance (money, interest rates)\n",
    "   ‚Ä¢ Trade (imports, exports)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09f37a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MULTI-LABEL CLASSIFICATION EXAMPLE\n",
      "=======================================================\n",
      "\n",
      "üìÑ Document: test/15023\n",
      "üìë Categories: ['earn']\n",
      "\n",
      "üìù Text preview:\n",
      "   CITYTRUST BANCORP INC &lt;CITR> 1ST QTR NET\n",
      "  Shr 1.40 dlrs vs 1.16 dlrs\n",
      "      Net 5,776,000 vs 4,429,000\n",
      "      Avg shrs 4,132,828 vs 3,834,117\n",
      "  \n",
      "\n",
      "...\n",
      "\n",
      "üè∑Ô∏è Examples of multi-label documents:\n",
      "   test/14832: ['corn', 'grain', 'rice', 'rubber', 'sugar', 'tin', 'trade']\n",
      "   test/14840: ['coffee', 'lumber', 'palm-oil', 'rubber', 'veg-oil']\n",
      "   test/14858: ['carcass', 'corn', 'grain', 'livestock', 'oilseed', 'rice', 'soybean', 'trade']\n",
      "   test/14892: ['oilseed', 'palm-oil', 'soy-oil', 'soybean', 'veg-oil']\n",
      "   test/14913: ['dlr', 'money-fx', 'yen']\n"
     ]
    }
   ],
   "source": [
    "# Multi-label: Documents can have multiple categories\n",
    "sample_file = reuters.fileids()[100]  # Pick a document\n",
    "\n",
    "print(\"MULTI-LABEL CLASSIFICATION EXAMPLE\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "print(f\"\\nüìÑ Document: {sample_file}\")\n",
    "print(f\"üìë Categories: {reuters.categories(sample_file)}\")\n",
    "print(f\"\\nüìù Text preview:\")\n",
    "print(f\"   {reuters.raw(sample_file)[:400]}...\")\n",
    "\n",
    "# Show some documents with multiple categories\n",
    "print(f\"\\nüè∑Ô∏è Examples of multi-label documents:\")\n",
    "multi_label_count = 0\n",
    "for fileid in reuters.fileids()[:100]:\n",
    "    cats = reuters.categories(fileid)\n",
    "    if len(cats) >= 3:\n",
    "        print(f\"   {fileid}: {cats}\")\n",
    "        multi_label_count += 1\n",
    "        if multi_label_count >= 5:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "183fb18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BUILT-IN TRAIN/TEST SPLIT\n",
      "=======================================================\n",
      "\n",
      "üìä Split Statistics:\n",
      "   Training documents: 7,769\n",
      "   Test documents:     3,019\n",
      "   Split ratio:        72.0% / 28.0%\n",
      "\n",
      "üí° Why built-in splits matter:\n",
      "   ‚Ä¢ Ensures fair comparison between algorithms\n",
      "   ‚Ä¢ Standard benchmark for research papers\n",
      "   ‚Ä¢ No need to create your own split\n",
      "\n",
      "üìÅ Sample training file: training/1\n",
      "üìÅ Sample test file:     test/14826\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reuters has a built-in train/test split\n",
    "# File IDs starting with 'training/' are for training\n",
    "# File IDs starting with 'test/' are for testing\n",
    "\n",
    "train_files = [f for f in reuters.fileids() if f.startswith('training/')]\n",
    "test_files = [f for f in reuters.fileids() if f.startswith('test/')]\n",
    "\n",
    "print(\"BUILT-IN TRAIN/TEST SPLIT\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"\"\"\n",
    "üìä Split Statistics:\n",
    "   Training documents: {len(train_files):,}\n",
    "   Test documents:     {len(test_files):,}\n",
    "   Split ratio:        {len(train_files)/len(reuters.fileids())*100:.1f}% / {len(test_files)/len(reuters.fileids())*100:.1f}%\n",
    "   \n",
    "üí° Why built-in splits matter:\n",
    "   ‚Ä¢ Ensures fair comparison between algorithms\n",
    "   ‚Ä¢ Standard benchmark for research papers\n",
    "   ‚Ä¢ No need to create your own split\n",
    "   \n",
    "üìÅ Sample training file: {train_files[0]}\n",
    "üìÅ Sample test file:     {test_files[0]}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a325c28d",
   "metadata": {},
   "source": [
    "## 15.5 Creating Your Own Custom Corpus\n",
    "\n",
    "NLTK isn't limited to built-in corpora. You can create corpus readers for your own text collections!\n",
    "\n",
    "### PlaintextCorpusReader\n",
    "\n",
    "The simplest way to create a custom corpus:\n",
    "\n",
    "```python\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "\n",
    "# Point to a directory containing .txt files\n",
    "corpus = PlaintextCorpusReader('./my_texts/', r'.*\\.txt')\n",
    "\n",
    "# Now use standard corpus methods\n",
    "corpus.fileids()\n",
    "corpus.words()\n",
    "corpus.sents()\n",
    "```\n",
    "\n",
    "### Why Use Corpus Readers?\n",
    "\n",
    "| Benefit | Description |\n",
    "|---------|-------------|\n",
    "| **Consistent API** | Same methods as built-in corpora |\n",
    "| **Lazy loading** | Files loaded only when needed |\n",
    "| **Automatic tokenization** | Words/sentences extracted automatically |\n",
    "| **Scalable** | Handle large document collections |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ae02320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUSTOM CORPUS CREATED\n",
      "=======================================================\n",
      "üìÅ Directory: ./my_corpus\n",
      "üìÑ Files created: 3\n",
      "   ‚Ä¢ doc1.txt\n",
      "   ‚Ä¢ doc2.txt\n",
      "   ‚Ä¢ doc3.txt\n"
     ]
    }
   ],
   "source": [
    "# Create a sample corpus directory with text files\n",
    "\n",
    "corpus_dir = './my_corpus'\n",
    "os.makedirs(corpus_dir, exist_ok=True)\n",
    "\n",
    "# Create sample documents on different topics\n",
    "texts = {\n",
    "    'doc1.txt': \"\"\"Natural language processing is a field of computer science.\n",
    "It deals with the interaction between computers and humans using natural language.\n",
    "NLP combines computational linguistics with machine learning and deep learning.\n",
    "Applications include translation, sentiment analysis, and chatbots.\"\"\",\n",
    "    \n",
    "    'doc2.txt': \"\"\"Machine learning is transforming how we build software systems.\n",
    "Deep learning models can understand complex patterns in data.\n",
    "Neural networks have achieved remarkable results in image and speech recognition.\n",
    "AI is becoming more accessible to developers through open-source libraries.\"\"\",\n",
    "    \n",
    "    'doc3.txt': \"\"\"Python is a popular programming language for data science.\n",
    "It is widely used in scientific computing and web development.\n",
    "Python has a rich ecosystem of libraries like NumPy, Pandas, and scikit-learn.\n",
    "Its simple syntax makes it ideal for beginners and experts alike.\"\"\",\n",
    "}\n",
    "\n",
    "# Write files\n",
    "for filename, content in texts.items():\n",
    "    filepath = os.path.join(corpus_dir, filename)\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        f.write(content)\n",
    "\n",
    "print(\"CUSTOM CORPUS CREATED\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"üìÅ Directory: {corpus_dir}\")\n",
    "print(f\"üìÑ Files created: {len(texts)}\")\n",
    "for filename in texts.keys():\n",
    "    print(f\"   ‚Ä¢ {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e30a1003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING CUSTOM CORPUS\n",
      "=======================================================\n",
      "\n",
      "üìö Corpus loaded successfully!\n",
      "\n",
      "üìÅ Files: ['doc1.txt', 'doc2.txt', 'doc3.txt']\n",
      "\n",
      "üí° Now you can use all standard corpus methods:\n",
      "   ‚Ä¢ my_corpus.raw()    - Get raw text\n",
      "   ‚Ä¢ my_corpus.words()  - Get word tokens  \n",
      "   ‚Ä¢ my_corpus.sents()  - Get sentences\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the custom corpus with PlaintextCorpusReader\n",
    "# Parameters:\n",
    "#   - root: directory containing files\n",
    "#   - fileids: regex pattern to match files\n",
    "\n",
    "my_corpus = PlaintextCorpusReader(\n",
    "    corpus_dir,           # Directory path\n",
    "    r'.*\\.txt'           # Match all .txt files\n",
    ")\n",
    "\n",
    "print(\"LOADING CUSTOM CORPUS\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"\"\"\n",
    "üìö Corpus loaded successfully!\n",
    "\n",
    "üìÅ Files: {my_corpus.fileids()}\n",
    "\n",
    "üí° Now you can use all standard corpus methods:\n",
    "   ‚Ä¢ my_corpus.raw()    - Get raw text\n",
    "   ‚Ä¢ my_corpus.words()  - Get word tokens  \n",
    "   ‚Ä¢ my_corpus.sents()  - Get sentences\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18c21eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING CUSTOM CORPUS\n",
      "=======================================================\n",
      "\n",
      "üìä Corpus-wide statistics:\n",
      "   Total words:     139\n",
      "   Total sentences: 12\n",
      "   Unique words:    90\n",
      "\n",
      "üìÑ Per-file access:\n",
      "   doc1.txt: 44 words, 4 sentences\n",
      "   doc2.txt: 45 words, 4 sentences\n",
      "   doc3.txt: 50 words, 4 sentences\n",
      "\n",
      "üìù Words from doc1.txt:\n",
      "   ['Natural', 'language', 'processing', 'is', 'a', 'field', 'of', 'computer', 'science', '.', 'It', 'deals', 'with', 'the', 'interaction']...\n",
      "\n",
      "üìñ First sentence of doc2.txt:\n",
      "   Machine learning is transforming how we build software systems .\n"
     ]
    }
   ],
   "source": [
    "# Access methods work the same as built-in corpora!\n",
    "print(\"USING CUSTOM CORPUS\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Corpus-wide statistics\n",
    "print(f\"\\nüìä Corpus-wide statistics:\")\n",
    "print(f\"   Total words:     {len(my_corpus.words()):,}\")\n",
    "print(f\"   Total sentences: {len(my_corpus.sents())}\")\n",
    "print(f\"   Unique words:    {len(set(w.lower() for w in my_corpus.words() if w.isalpha()))}\")\n",
    "\n",
    "# Per-file access\n",
    "print(f\"\\nüìÑ Per-file access:\")\n",
    "for fileid in my_corpus.fileids():\n",
    "    words = len(my_corpus.words(fileid))\n",
    "    sents = len(my_corpus.sents(fileid))\n",
    "    print(f\"   {fileid}: {words} words, {sents} sentences\")\n",
    "\n",
    "# Get specific content\n",
    "print(f\"\\nüìù Words from doc1.txt:\")\n",
    "print(f\"   {list(my_corpus.words('doc1.txt'))[:15]}...\")\n",
    "\n",
    "print(f\"\\nüìñ First sentence of doc2.txt:\")\n",
    "print(f\"   {' '.join(my_corpus.sents('doc2.txt')[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b2f033",
   "metadata": {},
   "source": [
    "## 15.6 Categorized Corpus\n",
    "\n",
    "For machine learning tasks, you often need documents organized by category. NLTK's `CategorizedPlaintextCorpusReader` handles this!\n",
    "\n",
    "### Two Ways to Define Categories\n",
    "\n",
    "1. **Directory-based**: Each subdirectory is a category\n",
    "   ```\n",
    "   corpus/\n",
    "   ‚îú‚îÄ‚îÄ sports/\n",
    "   ‚îÇ   ‚îú‚îÄ‚îÄ article1.txt\n",
    "   ‚îÇ   ‚îî‚îÄ‚îÄ article2.txt\n",
    "   ‚îî‚îÄ‚îÄ politics/\n",
    "       ‚îú‚îÄ‚îÄ article3.txt\n",
    "       ‚îî‚îÄ‚îÄ article4.txt\n",
    "   ```\n",
    "\n",
    "2. **File-based**: Categories stored in a separate file\n",
    "\n",
    "### Use Cases\n",
    "\n",
    "- **Document classification** - Train classifiers on labeled documents\n",
    "- **Topic modeling** - Compare vocabulary across categories\n",
    "- **Genre analysis** - Study writing styles by category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5755ef9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CATEGORIZED CORPUS STRUCTURE\n",
      "=======================================================\n",
      "\n",
      "üìÅ ./categorized_corpus/\n",
      "   ‚îú‚îÄ‚îÄ üìÇ tech/\n",
      "   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ software.txt\n",
      "   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ hardware.txt\n",
      "   ‚îî‚îÄ‚îÄ üìÇ science/\n",
      "       ‚îú‚îÄ‚îÄ üìÑ biology.txt\n",
      "       ‚îî‚îÄ‚îÄ üìÑ physics.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a categorized corpus using directory structure\n",
    "from nltk.corpus import CategorizedPlaintextCorpusReader\n",
    "\n",
    "# Create directory structure: category/file.txt\n",
    "cat_corpus_dir = './categorized_corpus'\n",
    "\n",
    "categories_data = {\n",
    "    'tech': {\n",
    "        'software.txt': \"Software development requires programming skills and creativity. Modern applications use agile methodologies and continuous integration.\",\n",
    "        'hardware.txt': \"Computer hardware includes processors, memory, and storage devices. GPUs have become essential for machine learning workloads.\",\n",
    "    },\n",
    "    'science': {\n",
    "        'biology.txt': \"Biology studies living organisms and their interactions with the environment. DNA sequencing has revolutionized genetic research.\",\n",
    "        'physics.txt': \"Physics explains the fundamental laws of the universe through mathematics. Quantum mechanics describes behavior at atomic scales.\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# Create directories and files\n",
    "for category, files in categories_data.items():\n",
    "    cat_path = os.path.join(cat_corpus_dir, category)\n",
    "    os.makedirs(cat_path, exist_ok=True)\n",
    "    \n",
    "    for filename, content in files.items():\n",
    "        with open(os.path.join(cat_path, filename), 'w') as f:\n",
    "            f.write(content)\n",
    "\n",
    "print(\"CATEGORIZED CORPUS STRUCTURE\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"\"\"\n",
    "üìÅ {cat_corpus_dir}/\n",
    "   ‚îú‚îÄ‚îÄ üìÇ tech/\n",
    "   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ software.txt\n",
    "   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ hardware.txt\n",
    "   ‚îî‚îÄ‚îÄ üìÇ science/\n",
    "       ‚îú‚îÄ‚îÄ üìÑ biology.txt\n",
    "       ‚îî‚îÄ‚îÄ üìÑ physics.txt\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6622ac44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING CATEGORIZED CORPUS\n",
      "=======================================================\n",
      "\n",
      "üìë Categories found: ['science', 'tech']\n",
      "\n",
      "üìÅ All files:\n",
      "   ['science/biology.txt', 'science/physics.txt', 'tech/hardware.txt', 'tech/software.txt']\n",
      "\n",
      "üí° The cat_pattern regex extracts categories:\n",
      "   'tech/software.txt' ‚Üí category = 'tech'\n",
      "   'science/biology.txt' ‚Üí category = 'science'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load categorized corpus\n",
    "# cat_pattern extracts category from the path using regex groups\n",
    "\n",
    "cat_corpus = CategorizedPlaintextCorpusReader(\n",
    "    cat_corpus_dir,\n",
    "    r'.*/.*\\.txt',            # Match files in subdirectories\n",
    "    cat_pattern=r'(\\w+)/.*'   # First directory name = category\n",
    ")\n",
    "\n",
    "print(\"LOADING CATEGORIZED CORPUS\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"\"\"\n",
    "üìë Categories found: {cat_corpus.categories()}\n",
    "\n",
    "üìÅ All files:\n",
    "   {cat_corpus.fileids()}\n",
    "   \n",
    "üí° The cat_pattern regex extracts categories:\n",
    "   'tech/software.txt' ‚Üí category = 'tech'\n",
    "   'science/biology.txt' ‚Üí category = 'science'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9128f578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCESSING BY CATEGORY\n",
      "=======================================================\n",
      "\n",
      "üìÇ Category: SCIENCE\n",
      "   Files: ['science/biology.txt', 'science/physics.txt']\n",
      "   Word count: 37\n",
      "   Sample words: ['Biology', 'studies', 'living', 'organisms', 'and', 'their', 'interactions', 'with', 'the', 'environment']...\n",
      "\n",
      "üìÇ Category: TECH\n",
      "   Files: ['tech/hardware.txt', 'tech/software.txt']\n",
      "   Word count: 37\n",
      "   Sample words: ['Computer', 'hardware', 'includes', 'processors', ',', 'memory', ',', 'and', 'storage', 'devices']...\n",
      "\n",
      "\n",
      "üí° This is the same pattern used in Brown corpus:\n",
      "   brown.words(categories='news')\n",
      "\n",
      "   Works exactly the same for custom corpora!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Access content by category - just like the Brown corpus!\n",
    "print(\"ACCESSING BY CATEGORY\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "for category in cat_corpus.categories():\n",
    "    files = cat_corpus.fileids(categories=category)\n",
    "    words = cat_corpus.words(categories=category)\n",
    "    \n",
    "    print(f\"\\nüìÇ Category: {category.upper()}\")\n",
    "    print(f\"   Files: {files}\")\n",
    "    print(f\"   Word count: {len(list(words))}\")\n",
    "    print(f\"   Sample words: {list(words)[:10]}...\")\n",
    "\n",
    "print(f\"\"\"\n",
    "\n",
    "üí° This is the same pattern used in Brown corpus:\n",
    "   brown.words(categories='news')\n",
    "   \n",
    "   Works exactly the same for custom corpora!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163b316c",
   "metadata": {},
   "source": [
    "## 15.7 Corpus Statistics and Analysis\n",
    "\n",
    "Understanding your corpus is crucial before analysis. Key statistics include:\n",
    "\n",
    "### Basic Metrics\n",
    "\n",
    "| Metric | Description | Why It Matters |\n",
    "|--------|-------------|----------------|\n",
    "| **Word count** | Total words | Corpus size |\n",
    "| **Vocabulary size** | Unique words | Lexical richness |\n",
    "| **Avg word length** | Mean characters per word | Text complexity |\n",
    "| **Avg sentence length** | Mean words per sentence | Writing style |\n",
    "| **Lexical diversity** | Unique/Total ratio | Vocabulary variety |\n",
    "\n",
    "### Corpus Comparison\n",
    "\n",
    "Comparing statistics across corpora reveals:\n",
    "- Writing style differences (formal vs informal)\n",
    "- Vocabulary complexity (academic vs casual)\n",
    "- Document structure (short tweets vs long articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53fd6361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ corpus_statistics() function defined!\n"
     ]
    }
   ],
   "source": [
    "def corpus_statistics(corpus, name=\"Corpus\"):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive statistics for a corpus.\n",
    "    \n",
    "    Args:\n",
    "        corpus: NLTK corpus object\n",
    "        name: Display name for the corpus\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of statistics\n",
    "    \"\"\"\n",
    "    # Basic counts\n",
    "    words = list(corpus.words())\n",
    "    alpha_words = [w for w in words if w.isalpha()]\n",
    "    \n",
    "    stats = {\n",
    "        'name': name,\n",
    "        'files': len(corpus.fileids()),\n",
    "        'total_tokens': len(words),\n",
    "        'total_words': len(alpha_words),\n",
    "        'unique_words': len(set(w.lower() for w in alpha_words)),\n",
    "        'sentences': len(corpus.sents()),\n",
    "        'characters': len(corpus.raw()),\n",
    "    }\n",
    "    \n",
    "    # Derived statistics\n",
    "    if stats['total_words'] > 0:\n",
    "        stats['avg_word_length'] = sum(len(w) for w in alpha_words) / len(alpha_words)\n",
    "    else:\n",
    "        stats['avg_word_length'] = 0\n",
    "        \n",
    "    if stats['sentences'] > 0:\n",
    "        stats['avg_sent_length'] = stats['total_words'] / stats['sentences']\n",
    "    else:\n",
    "        stats['avg_sent_length'] = 0\n",
    "        \n",
    "    if stats['total_words'] > 0:\n",
    "        stats['lexical_diversity'] = stats['unique_words'] / stats['total_words']\n",
    "    else:\n",
    "        stats['lexical_diversity'] = 0\n",
    "    \n",
    "    return stats\n",
    "\n",
    "print(\"‚úÖ corpus_statistics() function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea874301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORPUS STATISTICS COMPARISON\n",
      "===========================================================================\n",
      "\n",
      "Metric                   Gutenberg         Brown             Inaugural         \n",
      "---------------------------------------------------------------------------\n",
      "Files                    18                500               60                \n",
      "Total Words              2,135,400         981,716           141,230           \n",
      "Unique Words             41,487            40,234            9,354             \n",
      "Sentences                98,552            57,340            5,395             \n",
      "Avg Word Length          4.18              4.68              4.71              \n",
      "Avg Sentence Length      21.7              17.1              26.2              \n",
      "Lexical Diversity        0.0194            0.0410            0.0662            \n",
      "\n",
      "\n",
      "üí° Observations:\n",
      "   ‚Ä¢ Gutenberg has highest lexical diversity (literary prose)\n",
      "   ‚Ä¢ Inaugural speeches have longest sentences (formal rhetoric)\n",
      "   ‚Ä¢ Brown is balanced across genres (by design)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compare statistics across different NLTK corpora\n",
    "corpora_to_analyze = [\n",
    "    (gutenberg, 'Gutenberg'),\n",
    "    (brown, 'Brown'),\n",
    "    (inaugural, 'Inaugural'),\n",
    "]\n",
    "\n",
    "print(\"CORPUS STATISTICS COMPARISON\")\n",
    "print(\"=\" * 75)\n",
    "\n",
    "all_stats = []\n",
    "for corpus, name in corpora_to_analyze:\n",
    "    stats = corpus_statistics(corpus, name)\n",
    "    all_stats.append(stats)\n",
    "\n",
    "# Display comparison table\n",
    "print(f\"\\n{'Metric':<25}\", end='')\n",
    "for s in all_stats:\n",
    "    print(f\"{s['name']:<18}\", end='')\n",
    "print()\n",
    "print(\"-\" * 75)\n",
    "\n",
    "metrics = [\n",
    "    ('Files', 'files', ','),\n",
    "    ('Total Words', 'total_words', ','),\n",
    "    ('Unique Words', 'unique_words', ','),\n",
    "    ('Sentences', 'sentences', ','),\n",
    "    ('Avg Word Length', 'avg_word_length', '.2f'),\n",
    "    ('Avg Sentence Length', 'avg_sent_length', '.1f'),\n",
    "    ('Lexical Diversity', 'lexical_diversity', '.4f'),\n",
    "]\n",
    "\n",
    "for label, key, fmt in metrics:\n",
    "    print(f\"{label:<25}\", end='')\n",
    "    for s in all_stats:\n",
    "        value = s[key]\n",
    "        if fmt == ',':\n",
    "            print(f\"{value:<18,}\", end='')\n",
    "        else:\n",
    "            print(f\"{value:<18{fmt}}\", end='')\n",
    "    print()\n",
    "\n",
    "print(f\"\"\"\n",
    "\n",
    "üí° Observations:\n",
    "   ‚Ä¢ Gutenberg has highest lexical diversity (literary prose)\n",
    "   ‚Ä¢ Inaugural speeches have longest sentences (formal rhetoric)\n",
    "   ‚Ä¢ Brown is balanced across genres (by design)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042e2b4d",
   "metadata": {},
   "source": [
    "## 15.8 Complete Corpus Manager Class\n",
    "\n",
    "Let's build a reusable class that wraps corpus management functionality.\n",
    "\n",
    "### Features\n",
    "\n",
    "| Method | Description |\n",
    "|--------|-------------|\n",
    "| `add_document()` | Add new documents to corpus |\n",
    "| `get_statistics()` | Get corpus statistics |\n",
    "| `search()` | Search for terms across corpus |\n",
    "| `get_concordance()` | Show word in context |\n",
    "| `vocabulary()` | Get word frequency dictionary |\n",
    "\n",
    "This class provides a convenient interface for working with custom corpora in real projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2faeed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ CorpusManager class defined!\n"
     ]
    }
   ],
   "source": [
    "class CorpusManager:\n",
    "    \"\"\"\n",
    "    Utility class for managing and analyzing text corpora.\n",
    "    \n",
    "    Provides convenient methods for:\n",
    "    - Loading and extending corpora\n",
    "    - Calculating statistics\n",
    "    - Searching and concordance\n",
    "    - Vocabulary analysis\n",
    "    \n",
    "    Example:\n",
    "        manager = CorpusManager('./my_corpus')\n",
    "        manager.add_document('new.txt', 'New document text')\n",
    "        stats = manager.get_statistics()\n",
    "        results = manager.search('python')\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, corpus_path, pattern=r'.*\\.txt'):\n",
    "        \"\"\"\n",
    "        Initialize corpus manager.\n",
    "        \n",
    "        Args:\n",
    "            corpus_path: Directory containing corpus files\n",
    "            pattern: Regex pattern to match files\n",
    "        \"\"\"\n",
    "        self.path = corpus_path\n",
    "        self.pattern = pattern\n",
    "        self._reload_corpus()\n",
    "        print(f\"CorpusManager initialized: {len(self.corpus.fileids())} files loaded\")\n",
    "    \n",
    "    def _reload_corpus(self):\n",
    "        \"\"\"Reload corpus from disk (call after adding files)\"\"\"\n",
    "        self.corpus = PlaintextCorpusReader(self.path, self.pattern)\n",
    "    \n",
    "    def add_document(self, filename, content):\n",
    "        \"\"\"\n",
    "        Add a new document to the corpus.\n",
    "        \n",
    "        Args:\n",
    "            filename: Name for the new file\n",
    "            content: Text content to write\n",
    "        \"\"\"\n",
    "        filepath = os.path.join(self.path, filename)\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            f.write(content)\n",
    "        self._reload_corpus()  # Reload to include new file\n",
    "        print(f\"Added: {filename}\")\n",
    "    \n",
    "    def get_statistics(self):\n",
    "        \"\"\"Get comprehensive corpus statistics.\"\"\"\n",
    "        return corpus_statistics(self.corpus, self.path)\n",
    "    \n",
    "    def search(self, term):\n",
    "        \"\"\"\n",
    "        Search for a term across all documents.\n",
    "        \n",
    "        Args:\n",
    "            term: Search term (case-insensitive)\n",
    "        \n",
    "        Returns:\n",
    "            List of (filename, count) tuples, sorted by count\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        for fileid in self.corpus.fileids():\n",
    "            text = self.corpus.raw(fileid).lower()\n",
    "            count = text.count(term.lower())\n",
    "            if count > 0:\n",
    "                results.append((fileid, count))\n",
    "        return sorted(results, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    def get_concordance(self, word, width=40, lines=10):\n",
    "        \"\"\"\n",
    "        Show a word in context (concordance view).\n",
    "        \n",
    "        Args:\n",
    "            word: Word to look up\n",
    "            width: Context width in characters\n",
    "            lines: Max number of examples\n",
    "        \"\"\"\n",
    "        from nltk import Text\n",
    "        text = Text(self.corpus.words())\n",
    "        text.concordance(word, width=width, lines=lines)\n",
    "    \n",
    "    def vocabulary(self, min_freq=1):\n",
    "        \"\"\"\n",
    "        Get vocabulary with frequency filter.\n",
    "        \n",
    "        Args:\n",
    "            min_freq: Minimum frequency threshold\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary {word: count}\n",
    "        \"\"\"\n",
    "        from collections import Counter\n",
    "        words = [w.lower() for w in self.corpus.words() if w.isalpha()]\n",
    "        freq = Counter(words)\n",
    "        return {w: c for w, c in freq.items() if c >= min_freq}\n",
    "    \n",
    "    def top_words(self, n=20, exclude_stopwords=False):\n",
    "        \"\"\"\n",
    "        Get the most frequent words.\n",
    "        \n",
    "        Args:\n",
    "            n: Number of words to return\n",
    "            exclude_stopwords: Whether to filter out common words\n",
    "        \"\"\"\n",
    "        from nltk.corpus import stopwords\n",
    "        vocab = self.vocabulary()\n",
    "        \n",
    "        if exclude_stopwords:\n",
    "            stop_words = set(stopwords.words('english'))\n",
    "            vocab = {w: c for w, c in vocab.items() if w not in stop_words}\n",
    "        \n",
    "        return sorted(vocab.items(), key=lambda x: x[1], reverse=True)[:n]\n",
    "\n",
    "print(\"‚úÖ CorpusManager class defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cee08a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORPUS MANAGER DEMONSTRATION\n",
      "=======================================================\n",
      "CorpusManager initialized: 3 files loaded\n",
      "\n",
      "üìù Adding a new document...\n",
      "Added: doc4.txt\n",
      "\n",
      "üìÅ Current files: ['doc1.txt', 'doc2.txt', 'doc3.txt', 'doc4.txt']\n",
      "\n",
      "üîç Search for 'Python':\n",
      "   doc3.txt: 2 occurrence(s)\n",
      "   doc4.txt: 1 occurrence(s)\n",
      "\n",
      "üìä Corpus Statistics:\n",
      "   Total words: 148\n",
      "   Unique words: 105\n",
      "   Lexical diversity: 0.7095\n",
      "\n",
      "üî§ Top 10 words (excluding stopwords):\n",
      "   data: 5\n",
      "   learning: 4\n",
      "   language: 3\n",
      "   science: 3\n",
      "   analysis: 3\n",
      "   python: 3\n",
      "   natural: 2\n",
      "   machine: 2\n",
      "   deep: 2\n",
      "   libraries: 2\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate the CorpusManager class\n",
    "print(\"CORPUS MANAGER DEMONSTRATION\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "manager = CorpusManager('./my_corpus')\n",
    "\n",
    "# Add a new document dynamically\n",
    "print(\"\\nüìù Adding a new document...\")\n",
    "manager.add_document('doc4.txt', \"\"\"\n",
    "Data analysis is essential for business intelligence.\n",
    "Visualization helps communicate insights effectively.\n",
    "Python and R are popular tools for data analysis.\n",
    "Statistical methods underpin modern data science.\n",
    "\"\"\")\n",
    "\n",
    "print(f\"\\nüìÅ Current files: {manager.corpus.fileids()}\")\n",
    "\n",
    "# Search across corpus\n",
    "print(f\"\\nüîç Search for 'Python':\")\n",
    "results = manager.search('python')\n",
    "for filename, count in results:\n",
    "    print(f\"   {filename}: {count} occurrence(s)\")\n",
    "\n",
    "# Get statistics\n",
    "print(f\"\\nüìä Corpus Statistics:\")\n",
    "stats = manager.get_statistics()\n",
    "print(f\"   Total words: {stats['total_words']:,}\")\n",
    "print(f\"   Unique words: {stats['unique_words']}\")\n",
    "print(f\"   Lexical diversity: {stats['lexical_diversity']:.4f}\")\n",
    "\n",
    "# Top words\n",
    "print(f\"\\nüî§ Top 10 words (excluding stopwords):\")\n",
    "for word, count in manager.top_words(10, exclude_stopwords=True):\n",
    "    print(f\"   {word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "39949571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLEANUP\n",
      "=======================================================\n",
      "‚úÖ Cleaned up temporary corpus directories\n",
      "\n",
      "üí° In real projects, you would keep your corpus!\n",
      "   These temporary files were just for demonstration.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cleanup temporary directories\n",
    "import shutil\n",
    "\n",
    "print(\"CLEANUP\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "shutil.rmtree('./my_corpus', ignore_errors=True)\n",
    "shutil.rmtree('./categorized_corpus', ignore_errors=True)\n",
    "\n",
    "print(\"‚úÖ Cleaned up temporary corpus directories\")\n",
    "print(\"\"\"\n",
    "üí° In real projects, you would keep your corpus!\n",
    "   These temporary files were just for demonstration.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa05e9f",
   "metadata": {},
   "source": [
    "## Summary & Quick Reference\n",
    "\n",
    "### NLTK Corpus Readers\n",
    "\n",
    "| Reader | Use Case | Example |\n",
    "|--------|----------|---------|\n",
    "| `PlaintextCorpusReader` | Plain text files | Blog posts, articles |\n",
    "| `CategorizedPlaintextCorpusReader` | Categorized documents | Labeled training data |\n",
    "| `TaggedCorpusReader` | POS-tagged text | Custom tagged data |\n",
    "| `BracketParseCorpusReader` | Parsed tree structures | Syntax trees |\n",
    "| `XMLCorpusReader` | XML documents | Structured data |\n",
    "\n",
    "### Common Corpus Methods\n",
    "\n",
    "```python\n",
    "# Loading a corpus\n",
    "from nltk.corpus import corpus_name\n",
    "\n",
    "# Access methods\n",
    "corpus.fileids()                    # List all files\n",
    "corpus.fileids(categories='cat')    # Files in category\n",
    "corpus.categories()                 # List categories\n",
    "corpus.categories(fileid)           # Categories for file\n",
    "\n",
    "# Content access\n",
    "corpus.raw()                        # Raw text string\n",
    "corpus.raw(fileid)                  # Raw text of specific file\n",
    "corpus.words()                      # List of words\n",
    "corpus.words(categories='cat')      # Words in category\n",
    "corpus.sents()                      # List of sentences\n",
    "corpus.tagged_words()               # Words with POS tags\n",
    "```\n",
    "\n",
    "### Creating Custom Corpora\n",
    "\n",
    "```python\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "\n",
    "# Basic corpus\n",
    "corpus = PlaintextCorpusReader('./directory/', r'.*\\.txt')\n",
    "\n",
    "# Categorized corpus  \n",
    "from nltk.corpus import CategorizedPlaintextCorpusReader\n",
    "corpus = CategorizedPlaintextCorpusReader(\n",
    "    './directory/',\n",
    "    r'.*/.*\\.txt',\n",
    "    cat_pattern=r'(\\w+)/.*'  # Category from directory name\n",
    ")\n",
    "```\n",
    "\n",
    "### Key Statistics to Track\n",
    "\n",
    "| Metric | Formula | Interpretation |\n",
    "|--------|---------|----------------|\n",
    "| **Vocabulary Size** | Unique words | Lexical richness |\n",
    "| **Lexical Diversity** | Unique / Total | 0.1 (repetitive) to 1.0 (varied) |\n",
    "| **Avg Sentence Length** | Words / Sentences | Complexity indicator |\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Organize by category** when building classification datasets\n",
    "2. **Use consistent naming** for files\n",
    "3. **Document your corpus** - include README with sources\n",
    "4. **Version control** your corpus alongside code\n",
    "5. **Calculate statistics** before analysis to understand your data\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Build a corpus from your own data (web scraping, APIs)\n",
    "- Compare vocabulary across categories\n",
    "- Use your corpus for classification or language modeling\n",
    "- Section 16: Advanced Topics for parsing and optimization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
