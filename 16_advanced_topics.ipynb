{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7654be2",
   "metadata": {},
   "source": [
    "# NLTK Complete Guide - Section 16: Advanced Topics\n",
    "\n",
    "This notebook covers:\n",
    "- Parsing and Grammar\n",
    "- Context-Free Grammar (CFG)\n",
    "- Dependency Parsing\n",
    "- Information Extraction\n",
    "- Regular Expression Patterns\n",
    "- Performance Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa586d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('maxent_ne_chunker', quiet=True)\n",
    "nltk.download('words', quiet=True)\n",
    "\n",
    "from nltk import CFG, ChartParser, RecursiveDescentParser\n",
    "from nltk import pos_tag, word_tokenize, ne_chunk\n",
    "from nltk.chunk import RegexpParser\n",
    "from nltk.tree import Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c679bb",
   "metadata": {},
   "source": [
    "## 16.1 Context-Free Grammar (CFG)\n",
    "\n",
    "CFG defines rules for syntactic structure of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761a9cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple grammar\n",
    "grammar = CFG.fromstring(\"\"\"\n",
    "    S -> NP VP\n",
    "    NP -> Det N | Det Adj N | 'I'\n",
    "    VP -> V NP | V\n",
    "    Det -> 'the' | 'a'\n",
    "    N -> 'dog' | 'cat' | 'ball' | 'park'\n",
    "    Adj -> 'big' | 'small' | 'happy'\n",
    "    V -> 'chased' | 'saw' | 'ran'\n",
    "\"\"\")\n",
    "\n",
    "print(\"Grammar productions:\")\n",
    "for production in grammar.productions():\n",
    "    print(f\"  {production}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf8d454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse a sentence\n",
    "parser = ChartParser(grammar)\n",
    "\n",
    "sentence = \"the big dog chased a cat\".split()\n",
    "\n",
    "print(f\"Sentence: {' '.join(sentence)}\\n\")\n",
    "print(\"Parse trees:\")\n",
    "\n",
    "for tree in parser.parse(sentence):\n",
    "    print(tree)\n",
    "    tree.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec63066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More complex grammar with recursion\n",
    "complex_grammar = CFG.fromstring(\"\"\"\n",
    "    S -> NP VP\n",
    "    NP -> Det N | Det N PP | 'I' | N\n",
    "    VP -> V | V NP | V NP PP\n",
    "    PP -> P NP\n",
    "    Det -> 'the' | 'a' | 'my'\n",
    "    N -> 'dog' | 'cat' | 'park' | 'telescope' | 'man' | 'hill'\n",
    "    V -> 'saw' | 'walked' | 'chased'\n",
    "    P -> 'in' | 'on' | 'with' | 'by'\n",
    "\"\"\")\n",
    "\n",
    "parser = ChartParser(complex_grammar)\n",
    "\n",
    "# Ambiguous sentence\n",
    "sentence = \"I saw the man with the telescope\".lower().split()\n",
    "\n",
    "print(f\"Sentence: {' '.join(sentence)}\")\n",
    "print(\"\\nPossible interpretations:\")\n",
    "\n",
    "for i, tree in enumerate(parser.parse(sentence), 1):\n",
    "    print(f\"\\nInterpretation {i}:\")\n",
    "    tree.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a072593",
   "metadata": {},
   "source": [
    "## 16.2 Probabilistic CFG (PCFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85519212",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import PCFG, ViterbiParser\n",
    "\n",
    "# Grammar with probabilities\n",
    "pcfg = PCFG.fromstring(\"\"\"\n",
    "    S -> NP VP [1.0]\n",
    "    NP -> Det N [0.5] | Det Adj N [0.3] | 'I' [0.2]\n",
    "    VP -> V NP [0.7] | V [0.3]\n",
    "    Det -> 'the' [0.6] | 'a' [0.4]\n",
    "    N -> 'dog' [0.4] | 'cat' [0.3] | 'ball' [0.3]\n",
    "    Adj -> 'big' [0.5] | 'small' [0.5]\n",
    "    V -> 'chased' [0.5] | 'saw' [0.5]\n",
    "\"\"\")\n",
    "\n",
    "print(\"PCFG Productions:\")\n",
    "for prod in pcfg.productions()[:8]:\n",
    "    print(f\"  {prod}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ba1197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse with Viterbi (finds most probable parse)\n",
    "viterbi_parser = ViterbiParser(pcfg)\n",
    "\n",
    "sentence = \"the dog chased a cat\".split()\n",
    "\n",
    "print(f\"Sentence: {' '.join(sentence)}\\n\")\n",
    "\n",
    "for tree in viterbi_parser.parse(sentence):\n",
    "    print(f\"Probability: {tree.prob():.6f}\")\n",
    "    tree.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76ec060",
   "metadata": {},
   "source": [
    "## 16.3 Regular Expression for Information Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ddbe79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract patterns using regex\n",
    "text = \"\"\"Contact us at support@example.com or sales@company.org.\n",
    "Call 123-456-7890 or (555) 123-4567 for assistance.\n",
    "Visit https://www.example.com or http://test.org for more info.\n",
    "Prices: $19.99, $150, $1,299.00\"\"\"\n",
    "\n",
    "# Email pattern\n",
    "email_pattern = r'[\\w.-]+@[\\w.-]+\\.\\w+'\n",
    "emails = re.findall(email_pattern, text)\n",
    "print(f\"Emails: {emails}\")\n",
    "\n",
    "# Phone pattern\n",
    "phone_pattern = r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}'\n",
    "phones = re.findall(phone_pattern, text)\n",
    "print(f\"Phones: {phones}\")\n",
    "\n",
    "# URL pattern\n",
    "url_pattern = r'https?://[\\w./]+'\n",
    "urls = re.findall(url_pattern, text)\n",
    "print(f\"URLs: {urls}\")\n",
    "\n",
    "# Price pattern\n",
    "price_pattern = r'\\$[\\d,]+\\.?\\d*'\n",
    "prices = re.findall(price_pattern, text)\n",
    "print(f\"Prices: {prices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a00c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatternExtractor:\n",
    "    \"\"\"Extract various patterns from text\"\"\"\n",
    "    \n",
    "    patterns = {\n",
    "        'email': r'[\\w.-]+@[\\w.-]+\\.\\w+',\n",
    "        'phone': r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}',\n",
    "        'url': r'https?://[\\w./-]+',\n",
    "        'price': r'\\$[\\d,]+\\.?\\d*',\n",
    "        'date': r'\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}',\n",
    "        'time': r'\\d{1,2}:\\d{2}(?:\\s?[AP]M)?',\n",
    "        'hashtag': r'#\\w+',\n",
    "        'mention': r'@\\w+',\n",
    "    }\n",
    "    \n",
    "    @classmethod\n",
    "    def extract(cls, text, pattern_name):\n",
    "        \"\"\"Extract specific pattern\"\"\"\n",
    "        if pattern_name not in cls.patterns:\n",
    "            raise ValueError(f\"Unknown pattern: {pattern_name}\")\n",
    "        return re.findall(cls.patterns[pattern_name], text, re.IGNORECASE)\n",
    "    \n",
    "    @classmethod\n",
    "    def extract_all(cls, text):\n",
    "        \"\"\"Extract all patterns\"\"\"\n",
    "        results = {}\n",
    "        for name, pattern in cls.patterns.items():\n",
    "            matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "            if matches:\n",
    "                results[name] = matches\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10dce2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\"\"\n",
    "Meeting scheduled for 01/15/2024 at 2:30 PM.\n",
    "Contact john@email.com or call (555) 123-4567.\n",
    "Check out our website: https://www.example.com\n",
    "Follow us @company #innovation #tech\n",
    "Special offer: $99.99!\n",
    "\"\"\"\n",
    "\n",
    "results = PatternExtractor.extract_all(sample_text)\n",
    "\n",
    "print(\"Extracted Information:\")\n",
    "print(\"=\" * 40)\n",
    "for pattern_type, matches in results.items():\n",
    "    print(f\"{pattern_type}: {matches}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6120061f",
   "metadata": {},
   "source": [
    "## 16.4 Relation Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b21cfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_relations(text):\n",
    "    \"\"\"Extract subject-relation-object triples\"\"\"\n",
    "    # POS tag and chunk\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged = pos_tag(tokens)\n",
    "    \n",
    "    # Grammar for relation extraction\n",
    "    grammar = r\"\"\"\n",
    "        NP: {<DT>?<JJ>*<NN.*>+}\n",
    "        VP: {<VB.*><RB>?}\n",
    "        RELATION: {<NP><VP><NP>}\n",
    "    \"\"\"\n",
    "    \n",
    "    parser = RegexpParser(grammar)\n",
    "    tree = parser.parse(tagged)\n",
    "    \n",
    "    relations = []\n",
    "    \n",
    "    for subtree in tree.subtrees():\n",
    "        if subtree.label() == 'RELATION':\n",
    "            parts = []\n",
    "            for child in subtree:\n",
    "                if isinstance(child, Tree):\n",
    "                    parts.append(' '.join(w for w, t in child.leaves()))\n",
    "            if len(parts) >= 2:\n",
    "                relations.append(tuple(parts))\n",
    "    \n",
    "    return relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1eae8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"The company acquired the startup.\",\n",
    "    \"John founded a technology company.\",\n",
    "    \"The scientists discovered a new species.\",\n",
    "]\n",
    "\n",
    "print(\"Relation Extraction\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for sent in sentences:\n",
    "    relations = extract_relations(sent)\n",
    "    print(f\"\\n{sent}\")\n",
    "    print(f\"Relations: {relations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0239bd",
   "metadata": {},
   "source": [
    "## 16.5 Text Normalization Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504e6401",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "class TextNormalizer:\n",
    "    \"\"\"Comprehensive text normalization pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self, language='english'):\n",
    "        self.stop_words = set(stopwords.words(language))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "        # Common contractions\n",
    "        self.contractions = {\n",
    "            \"won't\": \"will not\", \"can't\": \"cannot\",\n",
    "            \"n't\": \" not\", \"'re\": \" are\",\n",
    "            \"'s\": \" is\", \"'d\": \" would\",\n",
    "            \"'ll\": \" will\", \"'ve\": \" have\",\n",
    "            \"'m\": \" am\",\n",
    "        }\n",
    "    \n",
    "    def expand_contractions(self, text):\n",
    "        \"\"\"Expand contractions\"\"\"\n",
    "        for contraction, expansion in self.contractions.items():\n",
    "            text = text.replace(contraction, expansion)\n",
    "        return text\n",
    "    \n",
    "    def remove_accents(self, text):\n",
    "        \"\"\"Remove accented characters\"\"\"\n",
    "        nfkd = unicodedata.normalize('NFKD', text)\n",
    "        return ''.join(c for c in nfkd if not unicodedata.combining(c))\n",
    "    \n",
    "    def normalize(self, text, \n",
    "                  lowercase=True,\n",
    "                  remove_punctuation=True,\n",
    "                  remove_numbers=False,\n",
    "                  remove_stopwords=True,\n",
    "                  lemmatize=True):\n",
    "        \"\"\"Full normalization pipeline\"\"\"\n",
    "        \n",
    "        # Expand contractions\n",
    "        text = self.expand_contractions(text)\n",
    "        \n",
    "        # Remove accents\n",
    "        text = self.remove_accents(text)\n",
    "        \n",
    "        # Lowercase\n",
    "        if lowercase:\n",
    "            text = text.lower()\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # Filter tokens\n",
    "        filtered = []\n",
    "        for token in tokens:\n",
    "            # Remove punctuation\n",
    "            if remove_punctuation and not token.isalnum():\n",
    "                continue\n",
    "            \n",
    "            # Remove numbers\n",
    "            if remove_numbers and token.isdigit():\n",
    "                continue\n",
    "            \n",
    "            # Remove stopwords\n",
    "            if remove_stopwords and token.lower() in self.stop_words:\n",
    "                continue\n",
    "            \n",
    "            # Lemmatize\n",
    "            if lemmatize:\n",
    "                token = self.lemmatizer.lemmatize(token)\n",
    "            \n",
    "            filtered.append(token)\n",
    "        \n",
    "        return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57635eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = TextNormalizer()\n",
    "\n",
    "texts = [\n",
    "    \"I can't believe it's already 2024! The caf√© was amazing.\",\n",
    "    \"They're running 5 miles every day. She's been training hard.\",\n",
    "    \"The dogs were happily playing with their toys in the gardens.\",\n",
    "]\n",
    "\n",
    "print(\"Text Normalization\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for text in texts:\n",
    "    normalized = normalizer.normalize(text)\n",
    "    print(f\"\\nOriginal: {text}\")\n",
    "    print(f\"Normalized: {normalized}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90516f1",
   "metadata": {},
   "source": [
    "## 16.6 Performance Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52db9c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from functools import lru_cache\n",
    "\n",
    "# Caching for repeated operations\n",
    "@lru_cache(maxsize=10000)\n",
    "def cached_lemmatize(word):\n",
    "    \"\"\"Cached lemmatization\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return lemmatizer.lemmatize(word)\n",
    "\n",
    "# Test performance\n",
    "words = [\"running\", \"dogs\", \"happily\", \"better\"] * 1000\n",
    "\n",
    "# Without cache\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "start = time.time()\n",
    "result1 = [lemmatizer.lemmatize(w) for w in words]\n",
    "time1 = time.time() - start\n",
    "\n",
    "# With cache (first run)\n",
    "cached_lemmatize.cache_clear()\n",
    "start = time.time()\n",
    "result2 = [cached_lemmatize(w) for w in words]\n",
    "time2 = time.time() - start\n",
    "\n",
    "# With cache (second run - cached)\n",
    "start = time.time()\n",
    "result3 = [cached_lemmatize(w) for w in words]\n",
    "time3 = time.time() - start\n",
    "\n",
    "print(\"Performance Comparison\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Without cache:     {time1:.4f}s\")\n",
    "print(f\"With cache (1st):  {time2:.4f}s\")\n",
    "print(f\"With cache (2nd):  {time3:.4f}s\")\n",
    "print(f\"\\nSpeedup: {time1/time3:.1f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cab110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch processing for efficiency\n",
    "from nltk import pos_tag_sents\n",
    "\n",
    "sentences = [\n",
    "    word_tokenize(\"The quick brown fox jumps.\"),\n",
    "    word_tokenize(\"Natural language processing is fascinating.\"),\n",
    "    word_tokenize(\"Machine learning transforms industries.\"),\n",
    "] * 100\n",
    "\n",
    "# Individual processing\n",
    "start = time.time()\n",
    "result1 = [pos_tag(sent) for sent in sentences]\n",
    "time1 = time.time() - start\n",
    "\n",
    "# Batch processing\n",
    "start = time.time()\n",
    "result2 = pos_tag_sents(sentences)\n",
    "time2 = time.time() - start\n",
    "\n",
    "print(\"Batch vs Individual Processing\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Individual: {time1:.4f}s\")\n",
    "print(f\"Batch:      {time2:.4f}s\")\n",
    "print(f\"Speedup:    {time1/time2:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd920bff",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Parsing\n",
    "- `CFG.fromstring()` - Define grammar\n",
    "- `ChartParser` - General parsing\n",
    "- `RecursiveDescentParser` - Top-down parsing\n",
    "- `ViterbiParser` - Probabilistic parsing\n",
    "\n",
    "### Information Extraction\n",
    "- Regular expressions for patterns\n",
    "- Chunking for phrases\n",
    "- NER for named entities\n",
    "- Relation extraction for triples\n",
    "\n",
    "### Optimization Tips\n",
    "- Use `lru_cache` for repeated operations\n",
    "- Use batch functions (`pos_tag_sents`)\n",
    "- Precompile regex patterns\n",
    "- Limit vocabulary size for features"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
