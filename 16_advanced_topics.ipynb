{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7654be2",
   "metadata": {},
   "source": [
    "# NLTK Complete Guide - Section 16: Advanced Topics\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook covers advanced NLP concepts that go beyond basic text processing. These techniques are essential for building sophisticated NLP applications.\n",
    "\n",
    "### Topics Covered\n",
    "\n",
    "| Topic | Description | Use Cases |\n",
    "|-------|-------------|-----------|\n",
    "| **Context-Free Grammar** | Formal rules for sentence structure | Parsing, grammar checking |\n",
    "| **Probabilistic CFG** | Grammar with rule probabilities | Disambiguation, language generation |\n",
    "| **Information Extraction** | Extract structured data from text | Emails, phones, dates from text |\n",
    "| **Relation Extraction** | Find relationships between entities | Knowledge graphs, QA systems |\n",
    "| **Text Normalization** | Standardize text preprocessing | Pipeline building, data cleaning |\n",
    "| **Performance Optimization** | Speed up NLP operations | Production systems, large datasets |\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "This section assumes familiarity with:\n",
    "- Tokenization and POS tagging\n",
    "- Basic Python classes and functions\n",
    "- Regular expressions basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa586d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('maxent_ne_chunker', quiet=True)\n",
    "nltk.download('words', quiet=True)\n",
    "\n",
    "from nltk import CFG, ChartParser, RecursiveDescentParser\n",
    "from nltk import pos_tag, word_tokenize, ne_chunk\n",
    "from nltk.chunk import RegexpParser\n",
    "from nltk.tree import Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb9cc75",
   "metadata": {},
   "source": [
    "### Understanding the Imports\n",
    "\n",
    "| Import | Purpose |\n",
    "|--------|---------|\n",
    "| `CFG` | Define Context-Free Grammar rules |\n",
    "| `ChartParser` | Efficient parsing algorithm |\n",
    "| `RecursiveDescentParser` | Top-down parsing (simpler) |\n",
    "| `RegexpParser` | Pattern-based chunking |\n",
    "| `Tree` | Tree data structure for parse trees |\n",
    "| `ne_chunk` | Named Entity Recognition chunking |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c679bb",
   "metadata": {},
   "source": [
    "## 16.1 Context-Free Grammar (CFG)\n",
    "\n",
    "### What is a Grammar?\n",
    "\n",
    "A **grammar** defines the rules for constructing valid sentences in a language. In NLP, we use **Context-Free Grammar (CFG)** to describe sentence structure.\n",
    "\n",
    "### CFG Components\n",
    "\n",
    "| Component | Symbol | Example | Description |\n",
    "|-----------|--------|---------|-------------|\n",
    "| **Non-terminal** | S, NP, VP | S, NP | Abstract categories |\n",
    "| **Terminal** | lowercase | 'dog', 'the' | Actual words |\n",
    "| **Production Rule** | ‚Üí | S ‚Üí NP VP | How to expand non-terminals |\n",
    "| **Start Symbol** | S | S | Where parsing begins |\n",
    "\n",
    "### Grammar Notation\n",
    "\n",
    "```\n",
    "S  ‚Üí NP VP      # A sentence is a noun phrase + verb phrase\n",
    "NP ‚Üí Det N      # A noun phrase is a determiner + noun\n",
    "VP ‚Üí V NP       # A verb phrase is a verb + noun phrase\n",
    "Det ‚Üí 'the'     # Terminal symbols are actual words\n",
    "N ‚Üí 'dog'\n",
    "V ‚Üí 'chased'\n",
    "```\n",
    "\n",
    "### Why CFG Matters\n",
    "\n",
    "- **Parsing**: Understand sentence structure\n",
    "- **Grammar checking**: Validate sentence correctness\n",
    "- **Language generation**: Generate valid sentences\n",
    "- **Machine translation**: Transfer structure between languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761a9cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple Context-Free Grammar\n",
    "# CFG.fromstring() parses a multi-line grammar specification\n",
    "\n",
    "grammar = CFG.fromstring(\"\"\"\n",
    "    S -> NP VP\n",
    "    NP -> Det N | Det Adj N | 'I'\n",
    "    VP -> V NP | V\n",
    "    Det -> 'the' | 'a'\n",
    "    N -> 'dog' | 'cat' | 'ball' | 'park'\n",
    "    Adj -> 'big' | 'small' | 'happy'\n",
    "    V -> 'chased' | 'saw' | 'ran'\n",
    "\"\"\")\n",
    "\n",
    "print(\"CONTEXT-FREE GRAMMAR\")\n",
    "print(\"=\" * 55)\n",
    "print(\"\"\"\n",
    "This grammar defines simple English sentences like:\n",
    "  \"the dog chased a cat\"\n",
    "  \"I saw the big ball\"\n",
    "\n",
    "Grammar Components:\n",
    "  S   = Sentence (start symbol)\n",
    "  NP  = Noun Phrase\n",
    "  VP  = Verb Phrase  \n",
    "  Det = Determiner (the, a)\n",
    "  N   = Noun\n",
    "  Adj = Adjective\n",
    "  V   = Verb\n",
    "\"\"\")\n",
    "\n",
    "print(\"Production Rules:\")\n",
    "print(\"-\" * 55)\n",
    "for production in grammar.productions():\n",
    "    print(f\"  {production}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf8d454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse a sentence using the grammar\n",
    "# ChartParser efficiently finds all valid parse trees\n",
    "\n",
    "parser = ChartParser(grammar)\n",
    "\n",
    "sentence = \"the big dog chased a cat\".split()\n",
    "\n",
    "print(\"PARSING A SENTENCE\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"\\nSentence: \\\"{' '.join(sentence)}\\\"\")\n",
    "print(f\"Tokens:   {sentence}\")\n",
    "print(\"\\n\" + \"-\" * 55)\n",
    "print(\"Parse Tree(s):\")\n",
    "\n",
    "for i, tree in enumerate(parser.parse(sentence), 1):\n",
    "    print(f\"\\nInterpretation {i}:\")\n",
    "    print(tree)\n",
    "    print(\"\\nVisualization:\")\n",
    "    tree.pretty_print()\n",
    "\n",
    "print(\"\"\"\n",
    "üí° Reading the Parse Tree:\n",
    "   ‚Ä¢ S at root = valid sentence\n",
    "   ‚Ä¢ Each level shows phrase structure\n",
    "   ‚Ä¢ Leaves are the actual words\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec63066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrating AMBIGUITY in grammar\n",
    "# Some sentences have multiple valid interpretations!\n",
    "\n",
    "complex_grammar = CFG.fromstring(\"\"\"\n",
    "    S -> NP VP\n",
    "    NP -> Det N | Det N PP | 'I' | N\n",
    "    VP -> V | V NP | V NP PP\n",
    "    PP -> P NP\n",
    "    Det -> 'the' | 'a' | 'my'\n",
    "    N -> 'dog' | 'cat' | 'park' | 'telescope' | 'man' | 'hill'\n",
    "    V -> 'saw' | 'walked' | 'chased'\n",
    "    P -> 'in' | 'on' | 'with' | 'by'\n",
    "\"\"\")\n",
    "\n",
    "parser = ChartParser(complex_grammar)\n",
    "\n",
    "# This sentence is AMBIGUOUS!\n",
    "sentence = \"I saw the man with the telescope\".lower().split()\n",
    "\n",
    "print(\"SYNTACTIC AMBIGUITY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nSentence: \\\"{' '.join(sentence)}\\\"\")\n",
    "print(\"\"\"\n",
    "This sentence has TWO valid interpretations:\n",
    "\n",
    "1. I used a telescope to see the man\n",
    "   \"with the telescope\" modifies the VERB (how I saw)\n",
    "\n",
    "2. I saw a man who had a telescope  \n",
    "   \"with the telescope\" modifies the NOUN (which man)\n",
    "\"\"\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for i, tree in enumerate(parser.parse(sentence), 1):\n",
    "    print(f\"\\nüîç Interpretation {i}:\")\n",
    "    tree.pretty_print()\n",
    "    \n",
    "    # Explain the interpretation\n",
    "    if i == 1:\n",
    "        print(\"   ‚Üí PP 'with telescope' attached to VP (I used the telescope)\")\n",
    "    else:\n",
    "        print(\"   ‚Üí PP 'with telescope' attached to NP (the man has telescope)\")\n",
    "\n",
    "print(\"\"\"\n",
    "üí° Ambiguity is a major challenge in NLP!\n",
    "   Context and world knowledge help humans disambiguate,\n",
    "   but machines need additional techniques (PCFGs, statistics).\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a072593",
   "metadata": {},
   "source": [
    "## 16.2 Probabilistic CFG (PCFG)\n",
    "\n",
    "### The Problem with Plain CFG\n",
    "\n",
    "Regular CFG can find ALL valid parses, but:\n",
    "- Multiple parses may exist (ambiguity)\n",
    "- All parses are treated equally\n",
    "- No way to prefer more likely interpretations\n",
    "\n",
    "### Solution: Add Probabilities\n",
    "\n",
    "**Probabilistic CFG (PCFG)** adds probability to each rule:\n",
    "- Rules for the same non-terminal must sum to 1.0\n",
    "- Parse probability = product of all rule probabilities\n",
    "- Choose the most probable parse\n",
    "\n",
    "```\n",
    "# Regular CFG               # PCFG with probabilities\n",
    "NP -> Det N                 NP -> Det N [0.6]\n",
    "NP -> Det Adj N             NP -> Det Adj N [0.4]\n",
    "```\n",
    "\n",
    "### Why PCFG?\n",
    "\n",
    "| Benefit | Description |\n",
    "|---------|-------------|\n",
    "| **Disambiguation** | Prefer common constructions |\n",
    "| **Ranking** | Sort parses by probability |\n",
    "| **Generation** | Generate natural-sounding text |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85519212",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import PCFG, ViterbiParser\n",
    "\n",
    "# Define a Probabilistic CFG\n",
    "# Each rule has a probability in brackets [p]\n",
    "# Rules for same LHS must sum to 1.0\n",
    "\n",
    "pcfg = PCFG.fromstring(\"\"\"\n",
    "    S -> NP VP [1.0]\n",
    "    NP -> Det N [0.5] | Det Adj N [0.3] | 'I' [0.2]\n",
    "    VP -> V NP [0.7] | V [0.3]\n",
    "    Det -> 'the' [0.6] | 'a' [0.4]\n",
    "    N -> 'dog' [0.4] | 'cat' [0.3] | 'ball' [0.3]\n",
    "    Adj -> 'big' [0.5] | 'small' [0.5]\n",
    "    V -> 'chased' [0.5] | 'saw' [0.5]\n",
    "\"\"\")\n",
    "\n",
    "print(\"PROBABILISTIC CFG\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "Each rule now has an associated probability:\n",
    "  ‚Ä¢ NP ‚Üí Det N [0.5] means 50% of NPs follow this pattern\n",
    "  ‚Ä¢ Det ‚Üí 'the' [0.6] means 'the' is more common than 'a'\n",
    "\"\"\")\n",
    "print(\"Sample Productions:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for prod in pcfg.productions()[:10]:\n",
    "    print(f\"  {prod}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ba1197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ViterbiParser finds the MOST PROBABLE parse tree\n",
    "# (Uses dynamic programming, like in speech recognition)\n",
    "\n",
    "viterbi_parser = ViterbiParser(pcfg)\n",
    "\n",
    "sentence = \"the dog chased a cat\".split()\n",
    "\n",
    "print(\"VITERBI PARSING (Finding Most Probable Parse)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nSentence: \\\"{' '.join(sentence)}\\\"\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for tree in viterbi_parser.parse(sentence):\n",
    "    prob = tree.prob()\n",
    "    print(f\"\\nüìä Parse Probability: {prob:.8f}\")\n",
    "    print(f\"\\nHow it's calculated:\")\n",
    "    print(f\"  P(S‚ÜíNP VP)   √ó P(NP‚ÜíDet N)  √ó P(Det‚Üí'the') √ó ...\")\n",
    "    print(f\"  = 1.0        √ó 0.5          √ó 0.6          √ó ...\")\n",
    "    print(f\"  = {prob:.8f}\")\n",
    "    print(f\"\\nParse Tree:\")\n",
    "    tree.pretty_print()\n",
    "\n",
    "print(\"\"\"\n",
    "üí° The Viterbi algorithm efficiently finds the most probable\n",
    "   parse without exploring all possibilities.\n",
    "   \n",
    "   This is essential for:\n",
    "   ‚Ä¢ Disambiguating sentences\n",
    "   ‚Ä¢ Speech recognition (choosing best interpretation)\n",
    "   ‚Ä¢ Machine translation (selecting best structure)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76ec060",
   "metadata": {},
   "source": [
    "## 16.3 Information Extraction with Regular Expressions\n",
    "\n",
    "**Information Extraction (IE)** finds structured data in unstructured text. The simplest approach uses **regular expressions** to match patterns.\n",
    "\n",
    "### Common Extraction Targets\n",
    "\n",
    "| Target | Pattern Example | Description |\n",
    "|--------|-----------------|-------------|\n",
    "| **Email** | `\\w+@\\w+\\.\\w+` | user@domain.com |\n",
    "| **Phone** | `\\d{3}-\\d{3}-\\d{4}` | 123-456-7890 |\n",
    "| **URL** | `https?://[\\w./]+` | http://example.com |\n",
    "| **Date** | `\\d{1,2}/\\d{1,2}/\\d{4}` | 12/25/2024 |\n",
    "| **Price** | `\\$[\\d,]+\\.?\\d*` | $19.99 |\n",
    "| **Hashtag** | `#\\w+` | #python |\n",
    "\n",
    "### Why Regex for IE?\n",
    "\n",
    "‚úÖ **Fast** - Compiled patterns are efficient  \n",
    "‚úÖ **Precise** - Exact pattern matching  \n",
    "‚úÖ **Portable** - Works across languages  \n",
    "‚ùå **Limited** - Can't handle complex semantics  \n",
    "‚ùå **Brittle** - Small format changes break patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ddbe79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract structured information using regular expressions\n",
    "\n",
    "text = \"\"\"\n",
    "Contact us at support@example.com or sales@company.org.\n",
    "Call 123-456-7890 or (555) 123-4567 for assistance.\n",
    "Visit https://www.example.com or http://test.org for more info.\n",
    "Prices: $19.99, $150, $1,299.00 - special offer ends 12/31/2024!\n",
    "\"\"\"\n",
    "\n",
    "print(\"INFORMATION EXTRACTION WITH REGEX\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nInput Text:\\n{text}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Define patterns with explanations\n",
    "patterns = {\n",
    "    'Emails': (r'[\\w.-]+@[\\w.-]+\\.\\w+', \n",
    "               'username @ domain . extension'),\n",
    "    'Phones': (r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}', \n",
    "               'optional ( + 3 digits + optional ) + separator + 3 digits + separator + 4 digits'),\n",
    "    'URLs': (r'https?://[\\w./]+', \n",
    "             'http or https :// + domain'),\n",
    "    'Prices': (r'\\$[\\d,]+\\.?\\d*', \n",
    "               '$ + digits with optional commas and decimals'),\n",
    "    'Dates': (r'\\d{1,2}/\\d{1,2}/\\d{2,4}', \n",
    "              'month/day/year format'),\n",
    "}\n",
    "\n",
    "print(\"\\nExtracted Information:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for name, (pattern, explanation) in patterns.items():\n",
    "    matches = re.findall(pattern, text)\n",
    "    print(f\"\\nüìå {name}:\")\n",
    "    print(f\"   Pattern: {pattern}\")\n",
    "    print(f\"   Matches: {matches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a00c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatternExtractor:\n",
    "    \"\"\"\n",
    "    Reusable pattern extractor for common information types.\n",
    "    \n",
    "    This class provides a centralized way to extract various\n",
    "    types of structured information from text.\n",
    "    \n",
    "    Example:\n",
    "        results = PatternExtractor.extract_all(text)\n",
    "        emails = PatternExtractor.extract(text, 'email')\n",
    "    \"\"\"\n",
    "    \n",
    "    # Pre-defined patterns for common information types\n",
    "    patterns = {\n",
    "        'email': r'[\\w.-]+@[\\w.-]+\\.\\w+',\n",
    "        'phone': r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}',\n",
    "        'url': r'https?://[\\w./-]+',\n",
    "        'price': r'\\$[\\d,]+\\.?\\d*',\n",
    "        'date': r'\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}',\n",
    "        'time': r'\\d{1,2}:\\d{2}(?:\\s?[AP]M)?',\n",
    "        'hashtag': r'#\\w+',\n",
    "        'mention': r'@\\w+',\n",
    "        'ip_address': r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}',\n",
    "        'percentage': r'\\d+\\.?\\d*%',\n",
    "    }\n",
    "    \n",
    "    @classmethod\n",
    "    def extract(cls, text, pattern_name):\n",
    "        \"\"\"\n",
    "        Extract specific pattern from text.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text\n",
    "            pattern_name: Name of pattern (email, phone, etc.)\n",
    "        \n",
    "        Returns:\n",
    "            List of matches\n",
    "        \"\"\"\n",
    "        if pattern_name not in cls.patterns:\n",
    "            raise ValueError(f\"Unknown pattern: {pattern_name}. \"\n",
    "                           f\"Available: {list(cls.patterns.keys())}\")\n",
    "        return re.findall(cls.patterns[pattern_name], text, re.IGNORECASE)\n",
    "    \n",
    "    @classmethod\n",
    "    def extract_all(cls, text):\n",
    "        \"\"\"\n",
    "        Extract ALL known patterns from text.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary {pattern_name: [matches]}\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        for name, pattern in cls.patterns.items():\n",
    "            matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "            if matches:\n",
    "                results[name] = matches\n",
    "        return results\n",
    "    \n",
    "    @classmethod\n",
    "    def add_pattern(cls, name, pattern):\n",
    "        \"\"\"Add a custom pattern to the extractor.\"\"\"\n",
    "        cls.patterns[name] = pattern\n",
    "\n",
    "print(\"‚úÖ PatternExtractor class defined!\")\n",
    "print(f\"\\nAvailable patterns: {list(PatternExtractor.patterns.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10dce2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the PatternExtractor class\n",
    "sample_text = \"\"\"\n",
    "üìß Meeting scheduled for 01/15/2024 at 2:30 PM.\n",
    "Contact john.doe@email.com or call (555) 123-4567.\n",
    "Check out our website: https://www.example.com/products\n",
    "Follow us @company #innovation #tech #AI\n",
    "Special offer: $99.99 (50% off!) Server IP: 192.168.1.100\n",
    "\"\"\"\n",
    "\n",
    "print(\"PATTERN EXTRACTOR DEMO\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nInput Text:{sample_text}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Extract all patterns at once\n",
    "results = PatternExtractor.extract_all(sample_text)\n",
    "\n",
    "print(\"\\nüìä Extracted Information:\")\n",
    "for pattern_type, matches in results.items():\n",
    "    emoji_map = {\n",
    "        'email': 'üìß', 'phone': 'üìû', 'url': 'üîó',\n",
    "        'date': 'üìÖ', 'time': '‚è∞', 'price': 'üí∞',\n",
    "        'hashtag': '#Ô∏è‚É£', 'mention': '@', 'percentage': '%',\n",
    "        'ip_address': 'üñ•Ô∏è'\n",
    "    }\n",
    "    emoji = emoji_map.get(pattern_type, '‚Ä¢')\n",
    "    print(f\"\\n{emoji} {pattern_type.upper()}:\")\n",
    "    for match in matches:\n",
    "        print(f\"   {match}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6120061f",
   "metadata": {},
   "source": [
    "## 16.4 Relation Extraction\n",
    "\n",
    "**Relation Extraction** identifies relationships between entities in text.\n",
    "\n",
    "### Examples\n",
    "\n",
    "| Text | Subject | Relation | Object |\n",
    "|------|---------|----------|--------|\n",
    "| \"Apple acquired Beats\" | Apple | acquired | Beats |\n",
    "| \"Einstein was born in Germany\" | Einstein | born_in | Germany |\n",
    "| \"Python was created by Guido\" | Python | created_by | Guido |\n",
    "\n",
    "### Approaches\n",
    "\n",
    "| Method | Description | Complexity |\n",
    "|--------|-------------|------------|\n",
    "| **Pattern-based** | Regex/rules for specific relations | Simple |\n",
    "| **Chunking** | NP-VP-NP sequences | Medium |\n",
    "| **Dependency parsing** | Analyze grammatical relations | Complex |\n",
    "| **Deep Learning** | Neural models (BERT, etc.) | Advanced |\n",
    "\n",
    "### Simple Approach: Chunking\n",
    "\n",
    "We can use chunking grammar to find:\n",
    "1. Noun Phrases (potential entities)\n",
    "2. Verb Phrases (potential relations)\n",
    "3. Extract NP-VP-NP patterns as triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b21cfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_relations(text):\n",
    "    \"\"\"\n",
    "    Extract subject-relation-object triples from text.\n",
    "    \n",
    "    Uses chunking to identify:\n",
    "    1. Noun Phrases (NP) - potential subjects/objects\n",
    "    2. Verb Phrases (VP) - potential relations\n",
    "    3. NP-VP-NP patterns as relation triples\n",
    "    \n",
    "    Args:\n",
    "        text: Input sentence\n",
    "    \n",
    "    Returns:\n",
    "        List of relation tuples\n",
    "    \"\"\"\n",
    "    # Step 1: Tokenize and POS tag\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged = pos_tag(tokens)\n",
    "    \n",
    "    # Step 2: Define chunking grammar\n",
    "    # NP: Determiner? + Adjectives* + Nouns+\n",
    "    # VP: Verb + optional Adverb\n",
    "    grammar = r\"\"\"\n",
    "        NP: {<DT>?<JJ>*<NN.*>+}\n",
    "        VP: {<VB.*><RB>?}\n",
    "        RELATION: {<NP><VP><NP>}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 3: Parse with chunk grammar\n",
    "    parser = RegexpParser(grammar)\n",
    "    tree = parser.parse(tagged)\n",
    "    \n",
    "    # Step 4: Extract NP-VP-NP patterns\n",
    "    relations = []\n",
    "    \n",
    "    for subtree in tree.subtrees():\n",
    "        if subtree.label() == 'RELATION':\n",
    "            parts = []\n",
    "            for child in subtree:\n",
    "                if isinstance(child, Tree):\n",
    "                    # Combine words in the chunk\n",
    "                    phrase = ' '.join(word for word, tag in child.leaves())\n",
    "                    parts.append(phrase)\n",
    "            if len(parts) >= 2:\n",
    "                relations.append(tuple(parts))\n",
    "    \n",
    "    return relations\n",
    "\n",
    "print(\"‚úÖ extract_relations() function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1eae8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate relation extraction\n",
    "sentences = [\n",
    "    \"The company acquired the startup.\",\n",
    "    \"John founded a technology company.\",\n",
    "    \"The scientists discovered a new species.\",\n",
    "    \"Apple released the iPhone.\",\n",
    "    \"The team won the championship.\",\n",
    "]\n",
    "\n",
    "print(\"RELATION EXTRACTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for sent in sentences:\n",
    "    relations = extract_relations(sent)\n",
    "    \n",
    "    print(f\"\\nüìù Sentence: \\\"{sent}\\\"\")\n",
    "    \n",
    "    if relations:\n",
    "        for rel in relations:\n",
    "            if len(rel) >= 3:\n",
    "                subj, verb, obj = rel[0], rel[1], rel[2]\n",
    "                print(f\"   üìå Subject: {subj}\")\n",
    "                print(f\"      Relation: {verb}\")\n",
    "                print(f\"      Object: {obj}\")\n",
    "            elif len(rel) == 2:\n",
    "                print(f\"   üìå Partial: {rel}\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è No clear NP-VP-NP pattern found\")\n",
    "\n",
    "print(\"\"\"\n",
    "\n",
    "üí° Limitations of Simple Relation Extraction:\n",
    "   ‚Ä¢ Only captures basic Subject-Verb-Object patterns\n",
    "   ‚Ä¢ Misses passive voice (\"The startup was acquired\")\n",
    "   ‚Ä¢ Doesn't resolve pronouns (\"It was founded by...\")\n",
    "   ‚Ä¢ For production, consider dependency parsing or ML models\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0239bd",
   "metadata": {},
   "source": [
    "## 16.5 Text Normalization Pipeline\n",
    "\n",
    "**Text normalization** standardizes text before analysis. A good pipeline handles multiple transformations consistently.\n",
    "\n",
    "### Common Normalization Steps\n",
    "\n",
    "| Step | Description | Example |\n",
    "|------|-------------|---------|\n",
    "| **Lowercase** | Convert to lowercase | \"Hello\" ‚Üí \"hello\" |\n",
    "| **Expand contractions** | \"don't\" ‚Üí \"do not\" | Standardize forms |\n",
    "| **Remove accents** | \"caf√©\" ‚Üí \"cafe\" | Handle special chars |\n",
    "| **Remove punctuation** | \"Hello!\" ‚Üí \"Hello\" | Clean tokens |\n",
    "| **Remove stopwords** | Filter common words | Remove \"the\", \"is\" |\n",
    "| **Lemmatization** | Reduce to base form | \"running\" ‚Üí \"run\" |\n",
    "\n",
    "### Why Normalization Matters\n",
    "\n",
    "- **Consistency**: Same word, same representation\n",
    "- **Reduce vocabulary**: \"Running\", \"runs\", \"ran\" ‚Üí \"run\"\n",
    "- **Better matching**: \"caf√©\" matches \"cafe\"\n",
    "- **Cleaner features**: Focus on meaningful content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504e6401",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "class TextNormalizer:\n",
    "    \"\"\"\n",
    "    Comprehensive text normalization pipeline.\n",
    "    \n",
    "    Combines multiple normalization techniques into a single,\n",
    "    configurable pipeline for preprocessing text.\n",
    "    \n",
    "    Features:\n",
    "    - Contraction expansion\n",
    "    - Accent removal\n",
    "    - Lowercasing\n",
    "    - Punctuation removal\n",
    "    - Stopword filtering\n",
    "    - Lemmatization\n",
    "    \n",
    "    Example:\n",
    "        normalizer = TextNormalizer()\n",
    "        tokens = normalizer.normalize(\"I can't believe it's 2024!\")\n",
    "        # ['believe', '2024']\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, language='english'):\n",
    "        \"\"\"Initialize with language-specific resources.\"\"\"\n",
    "        self.stop_words = set(stopwords.words(language))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "        # Common English contractions\n",
    "        self.contractions = {\n",
    "            \"won't\": \"will not\", \"can't\": \"cannot\",\n",
    "            \"couldn't\": \"could not\", \"shouldn't\": \"should not\",\n",
    "            \"wouldn't\": \"would not\", \"don't\": \"do not\",\n",
    "            \"doesn't\": \"does not\", \"didn't\": \"did not\",\n",
    "            \"haven't\": \"have not\", \"hasn't\": \"has not\",\n",
    "            \"hadn't\": \"had not\", \"isn't\": \"is not\",\n",
    "            \"aren't\": \"are not\", \"wasn't\": \"was not\",\n",
    "            \"weren't\": \"were not\", \"let's\": \"let us\",\n",
    "            \"n't\": \" not\", \"'re\": \" are\",\n",
    "            \"'s\": \" is\", \"'d\": \" would\",\n",
    "            \"'ll\": \" will\", \"'ve\": \" have\",\n",
    "            \"'m\": \" am\",\n",
    "        }\n",
    "        print(\"TextNormalizer initialized\")\n",
    "    \n",
    "    def expand_contractions(self, text):\n",
    "        \"\"\"Expand contractions to full forms.\"\"\"\n",
    "        for contraction, expansion in self.contractions.items():\n",
    "            text = text.replace(contraction, expansion)\n",
    "        return text\n",
    "    \n",
    "    def remove_accents(self, text):\n",
    "        \"\"\"Remove accent marks from characters.\"\"\"\n",
    "        # Normalize to decomposed form, then remove combining characters\n",
    "        nfkd = unicodedata.normalize('NFKD', text)\n",
    "        return ''.join(c for c in nfkd if not unicodedata.combining(c))\n",
    "    \n",
    "    def normalize(self, text, \n",
    "                  lowercase=True,\n",
    "                  remove_punctuation=True,\n",
    "                  remove_numbers=False,\n",
    "                  remove_stopwords=True,\n",
    "                  lemmatize=True,\n",
    "                  expand_contractions=True):\n",
    "        \"\"\"\n",
    "        Full normalization pipeline.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text\n",
    "            lowercase: Convert to lowercase\n",
    "            remove_punctuation: Remove punctuation marks\n",
    "            remove_numbers: Remove numeric tokens\n",
    "            remove_stopwords: Filter out common words\n",
    "            lemmatize: Reduce words to base form\n",
    "            expand_contractions: Expand contractions\n",
    "        \n",
    "        Returns:\n",
    "            List of normalized tokens\n",
    "        \"\"\"\n",
    "        # Step 1: Expand contractions\n",
    "        if expand_contractions:\n",
    "            text = self.expand_contractions(text)\n",
    "        \n",
    "        # Step 2: Remove accents\n",
    "        text = self.remove_accents(text)\n",
    "        \n",
    "        # Step 3: Lowercase\n",
    "        if lowercase:\n",
    "            text = text.lower()\n",
    "        \n",
    "        # Step 4: Tokenize\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # Step 5: Filter tokens\n",
    "        filtered = []\n",
    "        for token in tokens:\n",
    "            # Remove punctuation\n",
    "            if remove_punctuation and not token.isalnum():\n",
    "                continue\n",
    "            \n",
    "            # Remove numbers\n",
    "            if remove_numbers and token.isdigit():\n",
    "                continue\n",
    "            \n",
    "            # Remove stopwords\n",
    "            if remove_stopwords and token.lower() in self.stop_words:\n",
    "                continue\n",
    "            \n",
    "            # Lemmatize\n",
    "            if lemmatize and token.isalpha():\n",
    "                token = self.lemmatizer.lemmatize(token)\n",
    "            \n",
    "            filtered.append(token)\n",
    "        \n",
    "        return filtered\n",
    "    \n",
    "    def normalize_minimal(self, text):\n",
    "        \"\"\"Light normalization: lowercase and tokenize only.\"\"\"\n",
    "        return self.normalize(text, \n",
    "                             remove_punctuation=False,\n",
    "                             remove_stopwords=False,\n",
    "                             lemmatize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57635eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the TextNormalizer\n",
    "normalizer = TextNormalizer()\n",
    "\n",
    "texts = [\n",
    "    \"I can't believe it's already 2024! The caf√© was amazing.\",\n",
    "    \"They're running 5 miles every day. She's been training hard.\",\n",
    "    \"The dogs were happily playing with their toys in the gardens.\",\n",
    "]\n",
    "\n",
    "print(\"TEXT NORMALIZATION EXAMPLES\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "for text in texts:\n",
    "    normalized = normalizer.normalize(text)\n",
    "    \n",
    "    print(f\"\\nüìù Original:\")\n",
    "    print(f\"   \\\"{text}\\\"\")\n",
    "    print(f\"\\n‚ú® Normalized:\")\n",
    "    print(f\"   {normalized}\")\n",
    "    print(f\"\\n   Reduction: {len(text.split())} words ‚Üí {len(normalized)} tokens\")\n",
    "    print(\"-\" * 65)\n",
    "\n",
    "# Show step-by-step normalization\n",
    "print(\"\\nüìã STEP-BY-STEP NORMALIZATION:\")\n",
    "print(\"-\" * 65)\n",
    "sample = \"I can't believe it's AMAZING!\"\n",
    "print(f\"Original: \\\"{sample}\\\"\")\n",
    "print(f\"1. Expand contractions: \\\"{normalizer.expand_contractions(sample)}\\\"\")\n",
    "print(f\"2. Remove accents: \\\"{normalizer.remove_accents(sample)}\\\"\")\n",
    "print(f\"3. Full normalize: {normalizer.normalize(sample)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90516f1",
   "metadata": {},
   "source": [
    "## 16.6 Performance Optimization\n",
    "\n",
    "When processing large text collections, performance matters. Here are key optimization techniques.\n",
    "\n",
    "### Optimization Strategies\n",
    "\n",
    "| Technique | Description | Use Case |\n",
    "|-----------|-------------|----------|\n",
    "| **Caching** | Store repeated computations | Lemmatization, lookups |\n",
    "| **Batch processing** | Process multiple items together | POS tagging sentences |\n",
    "| **Compile patterns** | Pre-compile regex | Repeated matching |\n",
    "| **Lazy loading** | Load resources only when needed | Large corpora |\n",
    "| **Generators** | Process items one at a time | Memory efficiency |\n",
    "\n",
    "### The `lru_cache` Decorator\n",
    "\n",
    "Python's `functools.lru_cache` memoizes function results:\n",
    "- First call: Computes and stores result\n",
    "- Subsequent calls: Returns cached result\n",
    "- Great for pure functions (same input ‚Üí same output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52db9c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from functools import lru_cache\n",
    "\n",
    "# OPTIMIZATION 1: Caching with lru_cache\n",
    "# Perfect for repeated operations like lemmatization\n",
    "\n",
    "@lru_cache(maxsize=10000)\n",
    "def cached_lemmatize(word):\n",
    "    \"\"\"\n",
    "    Cached lemmatization - stores results for repeated words.\n",
    "    \n",
    "    In real text, many words repeat (the, is, a, etc.)\n",
    "    Caching avoids redundant computation.\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return lemmatizer.lemmatize(word)\n",
    "\n",
    "# Test performance: lemmatize 4000 words (with repetition)\n",
    "words = [\"running\", \"dogs\", \"happily\", \"better\", \"cats\", \"walked\", \"quickly\", \"good\"] * 500\n",
    "\n",
    "print(\"CACHING PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"Processing {len(words):,} words (repeated vocabulary)\\n\")\n",
    "\n",
    "# Without cache\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "start = time.time()\n",
    "result1 = [lemmatizer.lemmatize(w) for w in words]\n",
    "time_uncached = time.time() - start\n",
    "\n",
    "# With cache (first run - building cache)\n",
    "cached_lemmatize.cache_clear()  # Clear any existing cache\n",
    "start = time.time()\n",
    "result2 = [cached_lemmatize(w) for w in words]\n",
    "time_first_cached = time.time() - start\n",
    "\n",
    "# With cache (second run - using cache)\n",
    "start = time.time()\n",
    "result3 = [cached_lemmatize(w) for w in words]\n",
    "time_cached = time.time() - start\n",
    "\n",
    "print(f\"{'Method':<25} {'Time':>10} {'Speedup':>10}\")\n",
    "print(\"-\" * 55)\n",
    "print(f\"{'Without cache:':<25} {time_uncached:>10.4f}s {'-':>10}\")\n",
    "print(f\"{'With cache (1st run):':<25} {time_first_cached:>10.4f}s {time_uncached/time_first_cached:>10.1f}x\")\n",
    "print(f\"{'With cache (2nd run):':<25} {time_cached:>10.4f}s {time_uncached/time_cached:>10.1f}x\")\n",
    "\n",
    "# Cache statistics\n",
    "info = cached_lemmatize.cache_info()\n",
    "print(f\"\\nüìä Cache Statistics:\")\n",
    "print(f\"   Hits: {info.hits:,} (returned from cache)\")\n",
    "print(f\"   Misses: {info.misses:,} (computed fresh)\")\n",
    "print(f\"   Hit ratio: {info.hits/(info.hits+info.misses)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cab110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIMIZATION 2: Batch Processing\n",
    "# Many NLTK functions have batch versions that are more efficient\n",
    "\n",
    "from nltk import pos_tag_sents\n",
    "\n",
    "# Create test data: 300 sentences\n",
    "sentences = [\n",
    "    word_tokenize(\"The quick brown fox jumps over the lazy dog.\"),\n",
    "    word_tokenize(\"Natural language processing is a fascinating field.\"),\n",
    "    word_tokenize(\"Machine learning transforms industries worldwide.\"),\n",
    "] * 100\n",
    "\n",
    "print(\"BATCH PROCESSING COMPARISON\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"Processing {len(sentences)} sentences\\n\")\n",
    "\n",
    "# Individual processing (one at a time)\n",
    "start = time.time()\n",
    "result1 = [pos_tag(sent) for sent in sentences]\n",
    "time_individual = time.time() - start\n",
    "\n",
    "# Batch processing (all at once)\n",
    "start = time.time()\n",
    "result2 = pos_tag_sents(sentences)\n",
    "time_batch = time.time() - start\n",
    "\n",
    "print(f\"{'Method':<25} {'Time':>10} {'Speedup':>10}\")\n",
    "print(\"-\" * 55)\n",
    "print(f\"{'Individual pos_tag():':<25} {time_individual:>10.4f}s {'-':>10}\")\n",
    "print(f\"{'Batch pos_tag_sents():':<25} {time_batch:>10.4f}s {time_individual/time_batch:>10.1f}x\")\n",
    "\n",
    "print(f\"\"\"\n",
    "üí° Why is batch processing faster?\n",
    "   ‚Ä¢ Single model load (not repeated per sentence)\n",
    "   ‚Ä¢ Optimized memory allocation\n",
    "   ‚Ä¢ Better CPU cache utilization\n",
    "   \n",
    "üìå NLTK batch functions:\n",
    "   ‚Ä¢ pos_tag_sents(sentences) - Tag multiple sentences\n",
    "   ‚Ä¢ ne_chunk_sents(tagged) - NER on multiple sentences\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd920bff",
   "metadata": {},
   "source": [
    "## Summary & Quick Reference\n",
    "\n",
    "### Parsing\n",
    "\n",
    "| Tool | Description | Use Case |\n",
    "|------|-------------|----------|\n",
    "| `CFG.fromstring()` | Define grammar from string | Create grammars |\n",
    "| `ChartParser` | General-purpose parser | Find all parses |\n",
    "| `ViterbiParser` | Find most probable parse | PCFG disambiguation |\n",
    "| `RecursiveDescentParser` | Simple top-down parser | Teaching/debugging |\n",
    "\n",
    "### Grammar Syntax\n",
    "\n",
    "```python\n",
    "from nltk import CFG, PCFG\n",
    "\n",
    "# Context-Free Grammar\n",
    "grammar = CFG.fromstring(\"\"\"\n",
    "    S -> NP VP\n",
    "    NP -> Det N\n",
    "    Det -> 'the' | 'a'\n",
    "    N -> 'dog' | 'cat'\n",
    "\"\"\")\n",
    "\n",
    "# Probabilistic CFG (probabilities in brackets)\n",
    "pcfg = PCFG.fromstring(\"\"\"\n",
    "    S -> NP VP [1.0]\n",
    "    NP -> Det N [0.6] | N [0.4]\n",
    "\"\"\")\n",
    "```\n",
    "\n",
    "### Information Extraction\n",
    "\n",
    "```python\n",
    "import re\n",
    "\n",
    "# Common patterns\n",
    "email = r'[\\w.-]+@[\\w.-]+\\.\\w+'\n",
    "phone = r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}'\n",
    "url = r'https?://[\\w./]+'\n",
    "\n",
    "# Extract\n",
    "matches = re.findall(pattern, text)\n",
    "```\n",
    "\n",
    "### Text Normalization Checklist\n",
    "\n",
    "1. ‚òê Expand contractions (\"don't\" ‚Üí \"do not\")\n",
    "2. ‚òê Remove accents (\"caf√©\" ‚Üí \"cafe\")\n",
    "3. ‚òê Lowercase\n",
    "4. ‚òê Tokenize\n",
    "5. ‚òê Remove punctuation\n",
    "6. ‚òê Remove stopwords\n",
    "7. ‚òê Lemmatize\n",
    "\n",
    "### Performance Tips\n",
    "\n",
    "| Technique | Code | Speedup |\n",
    "|-----------|------|---------|\n",
    "| **Cache results** | `@lru_cache(maxsize=N)` | 10-100x for repeated calls |\n",
    "| **Batch processing** | `pos_tag_sents(list)` | 2-5x |\n",
    "| **Compile regex** | `re.compile(pattern)` | 2-3x for repeated use |\n",
    "| **Generator expressions** | `(x for x in items)` | Memory efficient |\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **CFG** formalizes sentence structure - essential for parsing\n",
    "2. **PCFG** adds probabilities for disambiguation\n",
    "3. **Regex** is fast for structured patterns (emails, phones)\n",
    "4. **Normalization** is crucial for consistent preprocessing\n",
    "5. **Optimization** matters at scale - use caching and batching\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Build a complete NLP pipeline combining these techniques\n",
    "- Explore dependency parsing for deeper analysis\n",
    "- Try neural approaches (transformers, BERT) for production\n",
    "- Section 17: Real-World Projects for practical applications"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
