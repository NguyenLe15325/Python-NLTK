{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdd1cb9d",
   "metadata": {},
   "source": [
    "# NLTK Complete Guide - Section 17: Real-World Projects\n",
    "\n",
    "This notebook covers practical NLP projects:\n",
    "1. Text Summarization\n",
    "2. Keyword Extraction\n",
    "3. Spam Classifier\n",
    "4. Question Answering\n",
    "5. Chatbot Foundation\n",
    "6. Document Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18c6d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import heapq\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, ne_chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944b4d74",
   "metadata": {},
   "source": [
    "## Project 1: Extractive Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6912619f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSummarizer:\n",
    "    \"\"\"Extractive text summarization using sentence scoring\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        \"\"\"Clean and tokenize text\"\"\"\n",
    "        # Remove special characters\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text\n",
    "    \n",
    "    def score_sentences(self, text):\n",
    "        \"\"\"Score sentences based on word frequency\"\"\"\n",
    "        text = self.preprocess(text)\n",
    "        sentences = sent_tokenize(text)\n",
    "        \n",
    "        # Calculate word frequencies\n",
    "        words = word_tokenize(text.lower())\n",
    "        words = [w for w in words if w.isalpha() and w not in self.stop_words]\n",
    "        word_freq = Counter(words)\n",
    "        \n",
    "        # Normalize frequencies\n",
    "        max_freq = max(word_freq.values()) if word_freq else 1\n",
    "        word_freq = {w: f/max_freq for w, f in word_freq.items()}\n",
    "        \n",
    "        # Score sentences\n",
    "        sentence_scores = {}\n",
    "        for sent in sentences:\n",
    "            words = word_tokenize(sent.lower())\n",
    "            score = sum(word_freq.get(w, 0) for w in words if w.isalpha())\n",
    "            # Normalize by sentence length to avoid bias toward long sentences\n",
    "            word_count = len([w for w in words if w.isalpha()])\n",
    "            if word_count > 0:\n",
    "                sentence_scores[sent] = score / word_count\n",
    "        \n",
    "        return sentence_scores\n",
    "    \n",
    "    def summarize(self, text, num_sentences=3):\n",
    "        \"\"\"Generate summary with top N sentences\"\"\"\n",
    "        scores = self.score_sentences(text)\n",
    "        \n",
    "        # Get top sentences while maintaining order\n",
    "        top_sentences = heapq.nlargest(num_sentences, scores, key=scores.get)\n",
    "        \n",
    "        # Reorder by appearance in original text\n",
    "        sentences = sent_tokenize(text)\n",
    "        summary_sentences = [s for s in sentences if s in top_sentences]\n",
    "        \n",
    "        return ' '.join(summary_sentences)\n",
    "    \n",
    "    def summarize_ratio(self, text, ratio=0.3):\n",
    "        \"\"\"Summarize to a ratio of original length\"\"\"\n",
    "        sentences = sent_tokenize(text)\n",
    "        num_sentences = max(1, int(len(sentences) * ratio))\n",
    "        return self.summarize(text, num_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60069a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the summarizer\n",
    "article = \"\"\"\n",
    "Artificial intelligence has transformed numerous industries in recent years. \n",
    "Machine learning algorithms can now process vast amounts of data to identify patterns \n",
    "that humans might miss. Natural language processing enables computers to understand \n",
    "and generate human language. Deep learning models have achieved remarkable results in \n",
    "image recognition, speech synthesis, and game playing. Companies are investing heavily \n",
    "in AI research and development. The technology has applications in healthcare, finance, \n",
    "transportation, and entertainment. However, AI also raises ethical concerns about privacy, \n",
    "job displacement, and algorithmic bias. Researchers are working to address these challenges \n",
    "while continuing to push the boundaries of what's possible. The future of AI promises \n",
    "even more exciting developments as computing power increases and algorithms improve.\n",
    "\"\"\"\n",
    "\n",
    "summarizer = TextSummarizer()\n",
    "\n",
    "print(\"ORIGINAL TEXT:\")\n",
    "print(\"=\" * 60)\n",
    "print(article.strip())\n",
    "print(f\"\\n({len(sent_tokenize(article))} sentences)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SUMMARY (3 sentences):\")\n",
    "print(\"=\" * 60)\n",
    "summary = summarizer.summarize(article, num_sentences=3)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52152f66",
   "metadata": {},
   "source": [
    "## Project 2: Keyword Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd6de70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeywordExtractor:\n",
    "    \"\"\"Extract keywords using TF-IDF-like scoring and POS filtering\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    def extract_candidates(self, text):\n",
    "        \"\"\"Extract candidate keywords (nouns and noun phrases)\"\"\"\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        tagged = pos_tag(tokens)\n",
    "        \n",
    "        # Filter for nouns and adjectives\n",
    "        candidates = []\n",
    "        for word, tag in tagged:\n",
    "            if tag.startswith(('NN', 'JJ')) and word.isalpha():\n",
    "                if word not in self.stop_words and len(word) > 2:\n",
    "                    lemma = self.lemmatizer.lemmatize(word)\n",
    "                    candidates.append(lemma)\n",
    "        \n",
    "        return candidates\n",
    "    \n",
    "    def extract_phrases(self, text):\n",
    "        \"\"\"Extract noun phrases as potential keywords\"\"\"\n",
    "        from nltk.chunk import RegexpParser\n",
    "        from nltk.tree import Tree\n",
    "        \n",
    "        grammar = r\"NP: {<JJ>*<NN.*>+}\"\n",
    "        parser = RegexpParser(grammar)\n",
    "        \n",
    "        sentences = sent_tokenize(text)\n",
    "        phrases = []\n",
    "        \n",
    "        for sent in sentences:\n",
    "            tokens = word_tokenize(sent)\n",
    "            tagged = pos_tag(tokens)\n",
    "            tree = parser.parse(tagged)\n",
    "            \n",
    "            for subtree in tree:\n",
    "                if isinstance(subtree, Tree) and subtree.label() == 'NP':\n",
    "                    phrase = ' '.join(w.lower() for w, t in subtree.leaves())\n",
    "                    if len(phrase.split()) > 1:  # Multi-word phrases\n",
    "                        phrases.append(phrase)\n",
    "        \n",
    "        return phrases\n",
    "    \n",
    "    def extract_keywords(self, text, top_n=10, include_phrases=True):\n",
    "        \"\"\"Extract top keywords\"\"\"\n",
    "        # Single word keywords\n",
    "        candidates = self.extract_candidates(text)\n",
    "        word_freq = Counter(candidates)\n",
    "        \n",
    "        # Score by frequency and position\n",
    "        keywords = word_freq.most_common(top_n)\n",
    "        \n",
    "        result = {'single_words': keywords}\n",
    "        \n",
    "        if include_phrases:\n",
    "            phrases = self.extract_phrases(text)\n",
    "            phrase_freq = Counter(phrases)\n",
    "            result['phrases'] = phrase_freq.most_common(top_n // 2)\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d682d84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = KeywordExtractor()\n",
    "\n",
    "text = \"\"\"\n",
    "Machine learning is a subset of artificial intelligence that enables computers \n",
    "to learn from data. Deep learning uses neural networks with multiple layers. \n",
    "Natural language processing helps machines understand human language. \n",
    "Computer vision allows systems to interpret visual information. \n",
    "Reinforcement learning trains agents through rewards and penalties.\n",
    "Data science combines machine learning with statistical analysis.\n",
    "Feature engineering is crucial for machine learning model performance.\n",
    "\"\"\"\n",
    "\n",
    "keywords = extractor.extract_keywords(text, top_n=10)\n",
    "\n",
    "print(\"KEYWORD EXTRACTION\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "print(\"\\nTop Single Words:\")\n",
    "for word, freq in keywords['single_words']:\n",
    "    print(f\"  {word}: {freq}\")\n",
    "\n",
    "print(\"\\nTop Phrases:\")\n",
    "for phrase, freq in keywords['phrases']:\n",
    "    print(f\"  {phrase}: {freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515d46ec",
   "metadata": {},
   "source": [
    "## Project 3: Spam Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a2fe76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "\n",
    "class SpamClassifier:\n",
    "    \"\"\"Simple spam detection using Naive Bayes\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.classifier = None\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "        # Spam indicators\n",
    "        self.spam_words = {\n",
    "            'free', 'winner', 'cash', 'prize', 'urgent', 'congratulations',\n",
    "            'click', 'subscribe', 'offer', 'limited', 'act', 'now',\n",
    "            'money', 'credit', 'loan', 'discount', 'deal', 'buy'\n",
    "        }\n",
    "    \n",
    "    def extract_features(self, text):\n",
    "        \"\"\"Extract features from text\"\"\"\n",
    "        text_lower = text.lower()\n",
    "        words = set(word_tokenize(text_lower))\n",
    "        \n",
    "        features = {\n",
    "            # Word presence\n",
    "            'has_free': 'free' in words,\n",
    "            'has_winner': 'winner' in words,\n",
    "            'has_click': 'click' in words,\n",
    "            'has_urgent': 'urgent' in words,\n",
    "            'has_money': 'money' in words,\n",
    "            \n",
    "            # Patterns\n",
    "            'has_url': bool(re.search(r'https?://', text_lower)),\n",
    "            'has_phone': bool(re.search(r'\\d{3}[-.\\s]?\\d{3}[-.\\s]?\\d{4}', text)),\n",
    "            'has_email': bool(re.search(r'@\\w+\\.\\w+', text)),\n",
    "            'has_caps': bool(re.search(r'[A-Z]{3,}', text)),\n",
    "            'has_exclaim': '!' in text,\n",
    "            'exclaim_count': text.count('!') > 2,\n",
    "            'has_dollar': '$' in text,\n",
    "            \n",
    "            # Statistics\n",
    "            'spam_word_count': len(words & self.spam_words),\n",
    "            'short_message': len(words) < 20,\n",
    "        }\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def train(self, training_data):\n",
    "        \"\"\"Train classifier with labeled data: [(text, label), ...]\"\"\"\n",
    "        featuresets = [\n",
    "            (self.extract_features(text), label)\n",
    "            for text, label in training_data\n",
    "        ]\n",
    "        self.classifier = NaiveBayesClassifier.train(featuresets)\n",
    "    \n",
    "    def classify(self, text):\n",
    "        \"\"\"Classify text as spam or ham\"\"\"\n",
    "        features = self.extract_features(text)\n",
    "        return self.classifier.classify(features)\n",
    "    \n",
    "    def probability(self, text):\n",
    "        \"\"\"Get spam probability\"\"\"\n",
    "        features = self.extract_features(text)\n",
    "        prob_dist = self.classifier.prob_classify(features)\n",
    "        return {\n",
    "            'spam': prob_dist.prob('spam'),\n",
    "            'ham': prob_dist.prob('ham')\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8ffbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "training_data = [\n",
    "    # Spam examples\n",
    "    (\"CONGRATULATIONS! You've won $1000! Click here NOW!\", \"spam\"),\n",
    "    (\"FREE MONEY! Act now for your cash prize!\", \"spam\"),\n",
    "    (\"Urgent: Your account needs verification. Click link.\", \"spam\"),\n",
    "    (\"Winner! Claim your free gift card today!\", \"spam\"),\n",
    "    (\"Limited offer! Buy now and save 90%!\", \"spam\"),\n",
    "    (\"You're selected for exclusive deal! Call 555-1234\", \"spam\"),\n",
    "    (\"Get rich quick! Make $5000 daily from home!\", \"spam\"),\n",
    "    (\"DISCOUNT!!! Subscribe now for FREE samples!!!\", \"spam\"),\n",
    "    \n",
    "    # Ham examples\n",
    "    (\"Hey, are we still meeting for lunch tomorrow?\", \"ham\"),\n",
    "    (\"The project deadline has been extended to Friday.\", \"ham\"),\n",
    "    (\"Thanks for sending the report. I'll review it today.\", \"ham\"),\n",
    "    (\"Can you pick up groceries on your way home?\", \"ham\"),\n",
    "    (\"Meeting rescheduled to 3pm in conference room B.\", \"ham\"),\n",
    "    (\"Happy birthday! Hope you have a great day.\", \"ham\"),\n",
    "    (\"I attached the documents you requested.\", \"ham\"),\n",
    "    (\"Let me know when you're free to discuss the proposal.\", \"ham\"),\n",
    "]\n",
    "\n",
    "# Train classifier\n",
    "spam_classifier = SpamClassifier()\n",
    "spam_classifier.train(training_data)\n",
    "\n",
    "print(\"Spam Classifier trained!\")\n",
    "print(\"\\nMost informative features:\")\n",
    "spam_classifier.classifier.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dc1960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test messages\n",
    "test_messages = [\n",
    "    \"FREE iPhone! Click now to claim your prize!\",\n",
    "    \"Don't forget about the team meeting at 2pm.\",\n",
    "    \"URGENT! Your account will be suspended! Act NOW!\",\n",
    "    \"Can you send me the updated spreadsheet?\",\n",
    "    \"Winner selected! Claim $500 gift card here!\",\n",
    "]\n",
    "\n",
    "print(\"SPAM DETECTION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for msg in test_messages:\n",
    "    result = spam_classifier.classify(msg)\n",
    "    probs = spam_classifier.probability(msg)\n",
    "    \n",
    "    emoji = \"ðŸš«\" if result == \"spam\" else \"âœ…\"\n",
    "    \n",
    "    print(f\"\\n{emoji} {result.upper()} ({probs['spam']:.1%} spam)\")\n",
    "    print(f\"   \\\"{msg[:50]}...\\\"\" if len(msg) > 50 else f\"   \\\"{msg}\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fa0752",
   "metadata": {},
   "source": [
    "## Project 4: Simple Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f30f32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleQA:\n",
    "    \"\"\"Simple extractive question answering\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        \"\"\"Tokenize and filter\"\"\"\n",
    "        words = word_tokenize(text.lower())\n",
    "        return [w for w in words if w.isalpha() and w not in self.stop_words]\n",
    "    \n",
    "    def similarity(self, sent1, sent2):\n",
    "        \"\"\"Calculate word overlap similarity\"\"\"\n",
    "        words1 = set(self.preprocess(sent1))\n",
    "        words2 = set(self.preprocess(sent2))\n",
    "        \n",
    "        if not words1 or not words2:\n",
    "            return 0\n",
    "        \n",
    "        intersection = words1 & words2\n",
    "        union = words1 | words2\n",
    "        \n",
    "        return len(intersection) / len(union)\n",
    "    \n",
    "    def find_answer(self, context, question, top_n=1):\n",
    "        \"\"\"Find best matching sentence(s) for the question\"\"\"\n",
    "        sentences = sent_tokenize(context)\n",
    "        \n",
    "        # Score each sentence\n",
    "        scored = []\n",
    "        for sent in sentences:\n",
    "            score = self.similarity(sent, question)\n",
    "            scored.append((sent, score))\n",
    "        \n",
    "        # Sort by score\n",
    "        scored.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        if top_n == 1:\n",
    "            return scored[0][0] if scored[0][1] > 0 else \"No answer found.\"\n",
    "        \n",
    "        return [s for s, score in scored[:top_n] if score > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca401e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = SimpleQA()\n",
    "\n",
    "context = \"\"\"\n",
    "Python was created by Guido van Rossum and first released in 1991.\n",
    "It is a high-level, interpreted programming language known for its simplicity.\n",
    "Python emphasizes code readability and uses significant indentation.\n",
    "The language supports multiple programming paradigms, including procedural,\n",
    "object-oriented, and functional programming. Python has a large standard library\n",
    "and is widely used in web development, data science, and artificial intelligence.\n",
    "The name Python was inspired by Monty Python's Flying Circus.\n",
    "\"\"\"\n",
    "\n",
    "questions = [\n",
    "    \"Who created Python?\",\n",
    "    \"When was Python first released?\",\n",
    "    \"What is Python used for?\",\n",
    "    \"Where does the name Python come from?\",\n",
    "]\n",
    "\n",
    "print(\"QUESTION ANSWERING\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Context: {context.strip()[:100]}...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for q in questions:\n",
    "    answer = qa.find_answer(context, q)\n",
    "    print(f\"\\nQ: {q}\")\n",
    "    print(f\"A: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40d46a3",
   "metadata": {},
   "source": [
    "## Project 5: Simple Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa71c48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class SimpleChatbot:\n",
    "    \"\"\"Pattern-matching chatbot with NLTK\"\"\"\n",
    "    \n",
    "    def __init__(self, name=\"Bot\"):\n",
    "        self.name = name\n",
    "        self.context = {}\n",
    "        \n",
    "        # Response patterns\n",
    "        self.patterns = {\n",
    "            r'hello|hi|hey': [\n",
    "                \"Hello! How can I help you today?\",\n",
    "                \"Hi there! What can I do for you?\",\n",
    "                \"Hey! Nice to meet you!\"\n",
    "            ],\n",
    "            r'how are you': [\n",
    "                \"I'm doing great, thanks for asking!\",\n",
    "                \"I'm fine! How about you?\",\n",
    "                \"All good here!\"\n",
    "            ],\n",
    "            r'what is your name|who are you': [\n",
    "                f\"I'm {name}, your friendly chatbot!\",\n",
    "                f\"My name is {name}. Nice to meet you!\"\n",
    "            ],\n",
    "            r'bye|goodbye|quit|exit': [\n",
    "                \"Goodbye! Have a great day!\",\n",
    "                \"See you later!\",\n",
    "                \"Bye! Take care!\"\n",
    "            ],\n",
    "            r'thank': [\n",
    "                \"You're welcome!\",\n",
    "                \"Happy to help!\",\n",
    "                \"No problem!\"\n",
    "            ],\n",
    "            r'weather': [\n",
    "                \"I can't check the weather, but I hope it's nice!\",\n",
    "                \"Sorry, I don't have access to weather data.\"\n",
    "            ],\n",
    "            r'help': [\n",
    "                \"I can chat with you! Try saying hello or asking me questions.\",\n",
    "                \"I'm here to help! Just start a conversation.\"\n",
    "            ],\n",
    "        }\n",
    "        \n",
    "        self.default_responses = [\n",
    "            \"I'm not sure I understand. Could you rephrase that?\",\n",
    "            \"Interesting! Tell me more.\",\n",
    "            \"I see. What else would you like to talk about?\",\n",
    "            \"Could you explain that differently?\"\n",
    "        ]\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        \"\"\"Clean input text\"\"\"\n",
    "        return text.lower().strip()\n",
    "    \n",
    "    def match_pattern(self, text):\n",
    "        \"\"\"Find matching pattern\"\"\"\n",
    "        for pattern, responses in self.patterns.items():\n",
    "            if re.search(pattern, text):\n",
    "                return random.choice(responses)\n",
    "        return None\n",
    "    \n",
    "    def respond(self, user_input):\n",
    "        \"\"\"Generate response to user input\"\"\"\n",
    "        text = self.preprocess(user_input)\n",
    "        \n",
    "        # Check for pattern match\n",
    "        response = self.match_pattern(text)\n",
    "        \n",
    "        if response:\n",
    "            return response\n",
    "        \n",
    "        # Default response\n",
    "        return random.choice(self.default_responses)\n",
    "    \n",
    "    def chat(self):\n",
    "        \"\"\"Interactive chat loop\"\"\"\n",
    "        print(f\"{self.name}: Hello! I'm {self.name}. Type 'quit' to exit.\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        while True:\n",
    "            user_input = input(\"You: \").strip()\n",
    "            \n",
    "            if not user_input:\n",
    "                continue\n",
    "            \n",
    "            if re.search(r'bye|quit|exit', user_input.lower()):\n",
    "                print(f\"{self.name}: Goodbye!\")\n",
    "                break\n",
    "            \n",
    "            response = self.respond(user_input)\n",
    "            print(f\"{self.name}: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23033e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo the chatbot (non-interactive)\n",
    "bot = SimpleChatbot(\"NLTK-Bot\")\n",
    "\n",
    "demo_inputs = [\n",
    "    \"Hello!\",\n",
    "    \"What is your name?\",\n",
    "    \"How are you doing?\",\n",
    "    \"What's the weather like?\",\n",
    "    \"Tell me a joke\",\n",
    "    \"Thank you!\",\n",
    "    \"Goodbye!\"\n",
    "]\n",
    "\n",
    "print(\"CHATBOT DEMO\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for user_input in demo_inputs:\n",
    "    response = bot.respond(user_input)\n",
    "    print(f\"You: {user_input}\")\n",
    "    print(f\"Bot: {response}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b391dfb",
   "metadata": {},
   "source": [
    "## Project 6: Document Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9ad907",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class DocumentSimilarity:\n",
    "    \"\"\"Calculate document similarity using TF-IDF and cosine similarity\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.documents = []\n",
    "        self.vocabulary = set()\n",
    "        self.idf = {}\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        \"\"\"Tokenize and normalize text\"\"\"\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        return [\n",
    "            self.lemmatizer.lemmatize(w)\n",
    "            for w in tokens\n",
    "            if w.isalpha() and w not in self.stop_words\n",
    "        ]\n",
    "    \n",
    "    def fit(self, documents):\n",
    "        \"\"\"Fit on a collection of documents\"\"\"\n",
    "        self.documents = [self.preprocess(doc) for doc in documents]\n",
    "        \n",
    "        # Build vocabulary and calculate IDF\n",
    "        doc_count = len(self.documents)\n",
    "        word_doc_count = defaultdict(int)\n",
    "        \n",
    "        for doc in self.documents:\n",
    "            for word in set(doc):\n",
    "                word_doc_count[word] += 1\n",
    "                self.vocabulary.add(word)\n",
    "        \n",
    "        # Calculate IDF\n",
    "        for word, count in word_doc_count.items():\n",
    "            self.idf[word] = math.log(doc_count / count)\n",
    "    \n",
    "    def tf_idf(self, document):\n",
    "        \"\"\"Calculate TF-IDF vector for a document\"\"\"\n",
    "        if isinstance(document, str):\n",
    "            document = self.preprocess(document)\n",
    "        \n",
    "        word_count = Counter(document)\n",
    "        max_count = max(word_count.values()) if word_count else 1\n",
    "        \n",
    "        vector = {}\n",
    "        for word in self.vocabulary:\n",
    "            tf = word_count.get(word, 0) / max_count\n",
    "            idf = self.idf.get(word, 0)\n",
    "            vector[word] = tf * idf\n",
    "        \n",
    "        return vector\n",
    "    \n",
    "    def cosine_similarity(self, vec1, vec2):\n",
    "        \"\"\"Calculate cosine similarity between two vectors\"\"\"\n",
    "        dot_product = sum(vec1.get(k, 0) * vec2.get(k, 0) for k in self.vocabulary)\n",
    "        \n",
    "        mag1 = math.sqrt(sum(v ** 2 for v in vec1.values()))\n",
    "        mag2 = math.sqrt(sum(v ** 2 for v in vec2.values()))\n",
    "        \n",
    "        if mag1 == 0 or mag2 == 0:\n",
    "            return 0\n",
    "        \n",
    "        return dot_product / (mag1 * mag2)\n",
    "    \n",
    "    def similarity_matrix(self, documents):\n",
    "        \"\"\"Calculate pairwise similarity matrix\"\"\"\n",
    "        self.fit(documents)\n",
    "        vectors = [self.tf_idf(doc) for doc in self.documents]\n",
    "        \n",
    "        n = len(documents)\n",
    "        matrix = [[0] * n for _ in range(n)]\n",
    "        \n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                matrix[i][j] = self.cosine_similarity(vectors[i], vectors[j])\n",
    "        \n",
    "        return matrix\n",
    "    \n",
    "    def find_similar(self, query, documents, top_n=3):\n",
    "        \"\"\"Find most similar documents to a query\"\"\"\n",
    "        self.fit(documents)\n",
    "        query_vector = self.tf_idf(query)\n",
    "        \n",
    "        similarities = []\n",
    "        for i, doc in enumerate(self.documents):\n",
    "            doc_vector = self.tf_idf(doc)\n",
    "            sim = self.cosine_similarity(query_vector, doc_vector)\n",
    "            similarities.append((i, sim))\n",
    "        \n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        return similarities[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfcbeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test document similarity\n",
    "documents = [\n",
    "    \"Machine learning is a subset of artificial intelligence.\",\n",
    "    \"Deep learning uses neural networks with multiple layers.\",\n",
    "    \"Natural language processing helps computers understand text.\",\n",
    "    \"Python is widely used for machine learning and AI.\",\n",
    "    \"Data science combines statistics with programming.\",\n",
    "]\n",
    "\n",
    "sim_calc = DocumentSimilarity()\n",
    "matrix = sim_calc.similarity_matrix(documents)\n",
    "\n",
    "print(\"DOCUMENT SIMILARITY MATRIX\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Print matrix\n",
    "print(\"\\n     \", end=\"\")\n",
    "for i in range(len(documents)):\n",
    "    print(f\"Doc{i+1:>6}\", end=\"\")\n",
    "print()\n",
    "\n",
    "for i, row in enumerate(matrix):\n",
    "    print(f\"Doc{i+1}\", end=\"\")\n",
    "    for val in row:\n",
    "        print(f\"{val:>7.2f}\", end=\"\")\n",
    "    print()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Documents:\")\n",
    "for i, doc in enumerate(documents):\n",
    "    print(f\"Doc{i+1}: {doc[:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f3dce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find similar documents to a query\n",
    "query = \"How does artificial intelligence work?\"\n",
    "\n",
    "similar = sim_calc.find_similar(query, documents, top_n=3)\n",
    "\n",
    "print(f\"\\nQuery: '{query}'\")\n",
    "print(\"\\nMost similar documents:\")\n",
    "for idx, score in similar:\n",
    "    print(f\"  {score:.2f}: {documents[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ac71e6",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Projects Covered\n",
    "\n",
    "| Project | Key Techniques |\n",
    "|---------|----------------|\n",
    "| Text Summarization | Sentence scoring, word frequency |\n",
    "| Keyword Extraction | TF-IDF, POS filtering, noun phrases |\n",
    "| Spam Classifier | Naive Bayes, feature extraction |\n",
    "| Question Answering | Sentence similarity, extractive QA |\n",
    "| Chatbot | Pattern matching, regex |\n",
    "| Document Similarity | TF-IDF, cosine similarity |\n",
    "\n",
    "### Key Takeaways\n",
    "- Combine multiple NLTK tools for real applications\n",
    "- Preprocessing is crucial for all NLP tasks\n",
    "- Feature engineering impacts model performance\n",
    "- Start simple, then add complexity as needed"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
