{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdd1cb9d",
   "metadata": {},
   "source": [
    "# NLTK Complete Guide - Section 17: Real-World Projects\n",
    "\n",
    "This notebook covers practical NLP projects:\n",
    "1. Text Summarization\n",
    "2. Keyword Extraction\n",
    "3. Spam Classifier\n",
    "4. Question Answering\n",
    "5. Chatbot Foundation\n",
    "6. Document Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d18c6d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import heapq\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, ne_chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944b4d74",
   "metadata": {},
   "source": [
    "## Project 1: Extractive Text Summarization\n",
    "\n",
    "### Pipeline Architecture\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                        TEXT SUMMARIZATION PIPELINE                       â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   Raw Text   â”‚â”€â”€â”€â–¶â”‚  Preprocess  â”‚â”€â”€â”€â–¶â”‚   Sentence   â”‚â”€â”€â”€â–¶â”‚    Word      â”‚\n",
    "â”‚              â”‚    â”‚  (normalize) â”‚    â”‚  Tokenize    â”‚    â”‚  Tokenize    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                                                   â”‚\n",
    "                                                                   â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   Summary    â”‚â—€â”€â”€â”€â”‚  Top-N by    â”‚â—€â”€â”€â”€â”‚   Score      â”‚â—€â”€â”€â”€â”‚    Word      â”‚\n",
    "â”‚   Output     â”‚    â”‚  Original    â”‚    â”‚  Sentences   â”‚    â”‚  Frequency   â”‚\n",
    "â”‚              â”‚    â”‚  Order       â”‚    â”‚              â”‚    â”‚  (filtered)  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### How It Works\n",
    "\n",
    "| Step | Process | Purpose |\n",
    "|------|---------|---------|\n",
    "| 1 | **Preprocess** | Normalize whitespace, clean text |\n",
    "| 2 | **Sentence Tokenize** | Split into individual sentences |\n",
    "| 3 | **Word Tokenize** | Split into words for frequency analysis |\n",
    "| 4 | **Filter Stopwords** | Remove common words (\"the\", \"is\", \"a\") |\n",
    "| 5 | **Word Frequency** | Count important words, normalize by max |\n",
    "| 6 | **Score Sentences** | Sum word scores Ã· sentence length |\n",
    "| 7 | **Select Top-N** | Pick highest-scoring sentences |\n",
    "| 8 | **Preserve Order** | Return sentences in original order |\n",
    "\n",
    "### Key Insight\n",
    "This is **extractive** summarization - we select existing sentences, not generate new ones. The assumption: sentences with frequent important words are more central to the document's meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6912619f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSummarizer:\n",
    "    \"\"\"Extractive text summarization using sentence scoring\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        \"\"\"Clean and tokenize text\"\"\"\n",
    "        # Remove extra whitespace and normalize\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "    \n",
    "    def score_sentences(self, text):\n",
    "        \"\"\"Score sentences based on word frequency\"\"\"\n",
    "        text = self.preprocess(text)\n",
    "        sentences = sent_tokenize(text)\n",
    "        \n",
    "        # Calculate word frequencies\n",
    "        words = word_tokenize(text.lower())\n",
    "        words = [w for w in words if w.isalpha() and w not in self.stop_words]\n",
    "        word_freq = Counter(words)\n",
    "        \n",
    "        # Normalize frequencies\n",
    "        max_freq = max(word_freq.values()) if word_freq else 1\n",
    "        word_freq = {w: f/max_freq for w, f in word_freq.items()}\n",
    "        \n",
    "        # Score sentences\n",
    "        sentence_scores = {}\n",
    "        for sent in sentences:\n",
    "            words = word_tokenize(sent.lower())\n",
    "            score = sum(word_freq.get(w, 0) for w in words if w.isalpha())\n",
    "            # Normalize by sentence length to avoid bias toward long sentences\n",
    "            word_count = len([w for w in words if w.isalpha()])\n",
    "            if word_count > 0:\n",
    "                sentence_scores[sent] = score / word_count\n",
    "        \n",
    "        return sentence_scores\n",
    "    \n",
    "    def summarize(self, text, num_sentences=3):\n",
    "        \"\"\"Generate summary with top N sentences\"\"\"\n",
    "        # Preprocess text FIRST, then use same preprocessed version throughout\n",
    "        text = self.preprocess(text)\n",
    "        sentences = sent_tokenize(text)\n",
    "        \n",
    "        scores = self.score_sentences(text)\n",
    "        \n",
    "        # Get top sentences while maintaining order\n",
    "        top_sentences = heapq.nlargest(num_sentences, scores, key=scores.get)\n",
    "        \n",
    "        # Reorder by appearance in original text\n",
    "        summary_sentences = [s for s in sentences if s in top_sentences]\n",
    "        \n",
    "        return ' '.join(summary_sentences)\n",
    "    \n",
    "    def summarize_ratio(self, text, ratio=0.3):\n",
    "        \"\"\"Summarize to a ratio of original length\"\"\"\n",
    "        text = self.preprocess(text)\n",
    "        sentences = sent_tokenize(text)\n",
    "        num_sentences = max(1, int(len(sentences) * ratio))\n",
    "        return self.summarize(text, num_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d60069a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL TEXT:\n",
      "============================================================\n",
      "Artificial intelligence has transformed numerous industries in recent years. \n",
      "Machine learning algorithms can now process vast amounts of data to identify patterns \n",
      "that humans might miss. Natural language processing enables computers to understand \n",
      "and generate human language. Deep learning models have achieved remarkable results in \n",
      "image recognition, speech synthesis, and game playing. Companies are investing heavily \n",
      "in AI research and development. The technology has applications in healthcare, finance, \n",
      "transportation, and entertainment. However, AI also raises ethical concerns about privacy, \n",
      "job displacement, and algorithmic bias. Researchers are working to address these challenges \n",
      "while continuing to push the boundaries of what's possible. The future of AI promises \n",
      "even more exciting developments as computing power increases and algorithms improve.\n",
      "\n",
      "(9 sentences)\n",
      "\n",
      "============================================================\n",
      "SUMMARY (3 sentences):\n",
      "============================================================\n",
      "Natural language processing enables computers to understand and generate human language. Companies are investing heavily in AI research and development. However, AI also raises ethical concerns about privacy, job displacement, and algorithmic bias.\n"
     ]
    }
   ],
   "source": [
    "# Test the summarizer\n",
    "article = \"\"\"\n",
    "Artificial intelligence has transformed numerous industries in recent years. \n",
    "Machine learning algorithms can now process vast amounts of data to identify patterns \n",
    "that humans might miss. Natural language processing enables computers to understand \n",
    "and generate human language. Deep learning models have achieved remarkable results in \n",
    "image recognition, speech synthesis, and game playing. Companies are investing heavily \n",
    "in AI research and development. The technology has applications in healthcare, finance, \n",
    "transportation, and entertainment. However, AI also raises ethical concerns about privacy, \n",
    "job displacement, and algorithmic bias. Researchers are working to address these challenges \n",
    "while continuing to push the boundaries of what's possible. The future of AI promises \n",
    "even more exciting developments as computing power increases and algorithms improve.\n",
    "\"\"\"\n",
    "\n",
    "summarizer = TextSummarizer()\n",
    "\n",
    "print(\"ORIGINAL TEXT:\")\n",
    "print(\"=\" * 60)\n",
    "print(article.strip())\n",
    "print(f\"\\n({len(sent_tokenize(article))} sentences)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SUMMARY (3 sentences):\")\n",
    "print(\"=\" * 60)\n",
    "summary = summarizer.summarize(article, num_sentences=3)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52152f66",
   "metadata": {},
   "source": [
    "## Project 2: Keyword Extraction\n",
    "\n",
    "### Pipeline Architecture\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                       KEYWORD EXTRACTION PIPELINE                        â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                              â”‚    Raw Text     â”‚\n",
    "                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                       â”‚\n",
    "                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                         â–¼                           â–¼\n",
    "              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "              â”‚  SINGLE WORDS   â”‚         â”‚  NOUN PHRASES   â”‚\n",
    "              â”‚    Pipeline     â”‚         â”‚    Pipeline     â”‚\n",
    "              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                       â”‚                           â”‚\n",
    "                       â–¼                           â–¼\n",
    "              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "              â”‚ Word Tokenize   â”‚         â”‚ Sent Tokenize   â”‚\n",
    "              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                       â”‚                           â”‚\n",
    "                       â–¼                           â–¼\n",
    "              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "              â”‚   POS Tagging   â”‚         â”‚   POS Tagging   â”‚\n",
    "              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                       â”‚                           â”‚\n",
    "                       â–¼                           â–¼\n",
    "              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "              â”‚ Filter: NN, JJ  â”‚         â”‚ Chunk: NP Grammarâ”‚\n",
    "              â”‚ (Nouns, Adj)    â”‚         â”‚ {<JJ>*<NN.*>+}  â”‚\n",
    "              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                       â”‚                           â”‚\n",
    "                       â–¼                           â–¼\n",
    "              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "              â”‚   Lemmatize     â”‚         â”‚ Extract Phrases â”‚\n",
    "              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                       â”‚                           â”‚\n",
    "                       â–¼                           â–¼\n",
    "              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "              â”‚ Count Frequency â”‚         â”‚ Count Frequency â”‚\n",
    "              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                       â”‚                           â”‚\n",
    "                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                     â–¼\n",
    "                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                          â”‚   Top Keywords  â”‚\n",
    "                          â”‚ + Top Phrases   â”‚\n",
    "                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### How It Works\n",
    "\n",
    "| Step | Process | Purpose |\n",
    "|------|---------|---------|\n",
    "| 1 | **POS Tagging** | Identify word types (noun, verb, adj) |\n",
    "| 2 | **Filter by POS** | Keep only nouns (NN*) and adjectives (JJ) |\n",
    "| 3 | **Lemmatize** | Normalize words (\"learning\" â†’ \"learn\") |\n",
    "| 4 | **Chunking** | Extract multi-word phrases using grammar |\n",
    "| 5 | **Frequency Count** | Rank by occurrence count |\n",
    "\n",
    "### Why POS Filtering?\n",
    "Keywords are typically **nouns** (concepts, entities) and **adjectives** (descriptors). Verbs, prepositions, and articles rarely make good keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edd6de70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeywordExtractor:\n",
    "    \"\"\"Extract keywords using TF-IDF-like scoring and POS filtering\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    def extract_candidates(self, text):\n",
    "        \"\"\"Extract candidate keywords (nouns and noun phrases)\"\"\"\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        tagged = pos_tag(tokens)\n",
    "        \n",
    "        # Filter for nouns and adjectives\n",
    "        candidates = []\n",
    "        for word, tag in tagged:\n",
    "            if tag.startswith(('NN', 'JJ')) and word.isalpha():\n",
    "                if word not in self.stop_words and len(word) > 2:\n",
    "                    lemma = self.lemmatizer.lemmatize(word)\n",
    "                    candidates.append(lemma)\n",
    "        \n",
    "        return candidates\n",
    "    \n",
    "    def extract_phrases(self, text):\n",
    "        \"\"\"Extract noun phrases as potential keywords\"\"\"\n",
    "        from nltk.chunk import RegexpParser\n",
    "        from nltk.tree import Tree\n",
    "        \n",
    "        grammar = r\"NP: {<JJ>*<NN.*>+}\"\n",
    "        parser = RegexpParser(grammar)\n",
    "        \n",
    "        sentences = sent_tokenize(text)\n",
    "        phrases = []\n",
    "        \n",
    "        for sent in sentences:\n",
    "            tokens = word_tokenize(sent)\n",
    "            tagged = pos_tag(tokens)\n",
    "            tree = parser.parse(tagged)\n",
    "            \n",
    "            for subtree in tree:\n",
    "                if isinstance(subtree, Tree) and subtree.label() == 'NP':\n",
    "                    phrase = ' '.join(w.lower() for w, t in subtree.leaves())\n",
    "                    if len(phrase.split()) > 1:  # Multi-word phrases\n",
    "                        phrases.append(phrase)\n",
    "        \n",
    "        return phrases\n",
    "    \n",
    "    def extract_keywords(self, text, top_n=10, include_phrases=True):\n",
    "        \"\"\"Extract top keywords\"\"\"\n",
    "        # Single word keywords\n",
    "        candidates = self.extract_candidates(text)\n",
    "        word_freq = Counter(candidates)\n",
    "        \n",
    "        # Score by frequency and position\n",
    "        keywords = word_freq.most_common(top_n)\n",
    "        \n",
    "        result = {'single_words': keywords}\n",
    "        \n",
    "        if include_phrases:\n",
    "            phrases = self.extract_phrases(text)\n",
    "            phrase_freq = Counter(phrases)\n",
    "            result['phrases'] = phrase_freq.most_common(top_n // 2)\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d682d84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEYWORD EXTRACTION\n",
      "========================================\n",
      "\n",
      "Top Single Words:\n",
      "  machine: 4\n",
      "  learning: 2\n",
      "  computer: 2\n",
      "  data: 2\n",
      "  language: 2\n",
      "  subset: 1\n",
      "  artificial: 1\n",
      "  intelligence: 1\n",
      "  deep: 1\n",
      "  neural: 1\n",
      "\n",
      "Top Phrases:\n",
      "  machine learning: 1\n",
      "  artificial intelligence: 1\n",
      "  deep learning: 1\n",
      "  neural networks: 1\n",
      "  multiple layers: 1\n"
     ]
    }
   ],
   "source": [
    "extractor = KeywordExtractor()\n",
    "\n",
    "text = \"\"\"\n",
    "Machine learning is a subset of artificial intelligence that enables computers \n",
    "to learn from data. Deep learning uses neural networks with multiple layers. \n",
    "Natural language processing helps machines understand human language. \n",
    "Computer vision allows systems to interpret visual information. \n",
    "Reinforcement learning trains agents through rewards and penalties.\n",
    "Data science combines machine learning with statistical analysis.\n",
    "Feature engineering is crucial for machine learning model performance.\n",
    "\"\"\"\n",
    "\n",
    "keywords = extractor.extract_keywords(text, top_n=10)\n",
    "\n",
    "print(\"KEYWORD EXTRACTION\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "print(\"\\nTop Single Words:\")\n",
    "for word, freq in keywords['single_words']:\n",
    "    print(f\"  {word}: {freq}\")\n",
    "\n",
    "print(\"\\nTop Phrases:\")\n",
    "for phrase, freq in keywords['phrases']:\n",
    "    print(f\"  {phrase}: {freq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515d46ec",
   "metadata": {},
   "source": [
    "## Project 3: Spam Classifier\n",
    "\n",
    "### Pipeline Architecture\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                         SPAM CLASSIFIER PIPELINE                         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "                           TRAINING PHASE\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Labeled     â”‚â”€â”€â”€â–¶â”‚   Feature    â”‚â”€â”€â”€â–¶â”‚   Feature    â”‚â”€â”€â”€â–¶â”‚    Train     â”‚\n",
    "â”‚  Messages    â”‚    â”‚  Extraction  â”‚    â”‚   Vectors    â”‚    â”‚ Naive Bayes  â”‚\n",
    "â”‚ (spam/ham)   â”‚    â”‚              â”‚    â”‚              â”‚    â”‚   Classifier â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                                                   â”‚\n",
    "                                                                   â–¼\n",
    "                                                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                                                            â”‚   Trained    â”‚\n",
    "                                                            â”‚    Model     â”‚\n",
    "                                                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                                                   â”‚\n",
    "                          PREDICTION PHASE                         â”‚\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚\n",
    "â”‚    New       â”‚â”€â”€â”€â–¶â”‚   Feature    â”‚â”€â”€â”€â–¶â”‚   Classify   â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "â”‚   Message    â”‚    â”‚  Extraction  â”‚    â”‚  (spam/ham)  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                           â”‚                   â”‚\n",
    "                           â–¼                   â–¼\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                    â”‚   Features   â”‚    â”‚  Probability â”‚\n",
    "                    â”‚  â€¢ has_free  â”‚    â”‚   spam: 85%  â”‚\n",
    "                    â”‚  â€¢ has_caps  â”‚    â”‚   ham:  15%  â”‚\n",
    "                    â”‚  â€¢ has_url   â”‚    â”‚              â”‚\n",
    "                    â”‚  â€¢ ...       â”‚    â”‚              â”‚\n",
    "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Feature Engineering (The Key!)\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                     FEATURE EXTRACTION                          â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  Input: \"FREE iPhone! Click NOW to claim your prize!!!\"        â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  Word Features:           Pattern Features:                     â”‚\n",
    "â”‚  â€¢ has_free: True         â€¢ has_url: False                      â”‚\n",
    "â”‚  â€¢ has_winner: False      â€¢ has_caps: True (FREE, NOW)          â”‚\n",
    "â”‚  â€¢ has_click: True        â€¢ has_exclaim: True                   â”‚\n",
    "â”‚  â€¢ has_urgent: False      â€¢ exclaim_count: True (>2)            â”‚\n",
    "â”‚  â€¢ has_money: False       â€¢ has_dollar: False                   â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  Statistics:                                                    â”‚\n",
    "â”‚  â€¢ spam_word_count: 3 (free, click, prize)                     â”‚\n",
    "â”‚  â€¢ short_message: True                                          â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### How Naive Bayes Works\n",
    "\n",
    "$$P(\\text{spam}|\\text{features}) = \\frac{P(\\text{features}|\\text{spam}) \\cdot P(\\text{spam})}{P(\\text{features})}$$\n",
    "\n",
    "The classifier learns:\n",
    "- How often each feature appears in spam vs ham\n",
    "- Multiplies probabilities assuming feature independence (naive assumption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9a2fe76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "\n",
    "class SpamClassifier:\n",
    "    \"\"\"Simple spam detection using Naive Bayes\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.classifier = None\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "        # Spam indicators\n",
    "        self.spam_words = {\n",
    "            'free', 'winner', 'cash', 'prize', 'urgent', 'congratulations',\n",
    "            'click', 'subscribe', 'offer', 'limited', 'act', 'now',\n",
    "            'money', 'credit', 'loan', 'discount', 'deal', 'buy'\n",
    "        }\n",
    "    \n",
    "    def extract_features(self, text):\n",
    "        \"\"\"Extract features from text\"\"\"\n",
    "        text_lower = text.lower()\n",
    "        words = set(word_tokenize(text_lower))\n",
    "        \n",
    "        features = {\n",
    "            # Word presence\n",
    "            'has_free': 'free' in words,\n",
    "            'has_winner': 'winner' in words,\n",
    "            'has_click': 'click' in words,\n",
    "            'has_urgent': 'urgent' in words,\n",
    "            'has_money': 'money' in words,\n",
    "            \n",
    "            # Patterns\n",
    "            'has_url': bool(re.search(r'https?://', text_lower)),\n",
    "            'has_phone': bool(re.search(r'\\d{3}[-.\\s]?\\d{3}[-.\\s]?\\d{4}', text)),\n",
    "            'has_email': bool(re.search(r'@\\w+\\.\\w+', text)),\n",
    "            'has_caps': bool(re.search(r'[A-Z]{3,}', text)),\n",
    "            'has_exclaim': '!' in text,\n",
    "            'exclaim_count': text.count('!') > 2,\n",
    "            'has_dollar': '$' in text,\n",
    "            \n",
    "            # Statistics\n",
    "            'spam_word_count': len(words & self.spam_words),\n",
    "            'short_message': len(words) < 20,\n",
    "        }\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def train(self, training_data):\n",
    "        \"\"\"Train classifier with labeled data: [(text, label), ...]\"\"\"\n",
    "        featuresets = [\n",
    "            (self.extract_features(text), label)\n",
    "            for text, label in training_data\n",
    "        ]\n",
    "        self.classifier = NaiveBayesClassifier.train(featuresets)\n",
    "    \n",
    "    def classify(self, text):\n",
    "        \"\"\"Classify text as spam or ham\"\"\"\n",
    "        features = self.extract_features(text)\n",
    "        return self.classifier.classify(features)\n",
    "    \n",
    "    def probability(self, text):\n",
    "        \"\"\"Get spam probability\"\"\"\n",
    "        features = self.extract_features(text)\n",
    "        prob_dist = self.classifier.prob_classify(features)\n",
    "        return {\n",
    "            'spam': prob_dist.prob('spam'),\n",
    "            'ham': prob_dist.prob('ham')\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e8ffbde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam Classifier trained!\n",
      "\n",
      "Most informative features:\n",
      "Most Informative Features\n",
      "             has_exclaim = False             ham : spam   =      5.0 : 1.0\n",
      "             has_exclaim = True             spam : ham    =      5.0 : 1.0\n",
      "         spam_word_count = 0                 ham : spam   =      5.0 : 1.0\n",
      "                has_free = True             spam : ham    =      2.3 : 1.0\n",
      "                has_caps = False             ham : spam   =      1.5 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Training data\n",
    "training_data = [\n",
    "    # Spam examples\n",
    "    (\"CONGRATULATIONS! You've won $1000! Click here NOW!\", \"spam\"),\n",
    "    (\"FREE MONEY! Act now for your cash prize!\", \"spam\"),\n",
    "    (\"Urgent: Your account needs verification. Click link.\", \"spam\"),\n",
    "    (\"Winner! Claim your free gift card today!\", \"spam\"),\n",
    "    (\"Limited offer! Buy now and save 90%!\", \"spam\"),\n",
    "    (\"You're selected for exclusive deal! Call 555-1234\", \"spam\"),\n",
    "    (\"Get rich quick! Make $5000 daily from home!\", \"spam\"),\n",
    "    (\"DISCOUNT!!! Subscribe now for FREE samples!!!\", \"spam\"),\n",
    "    \n",
    "    # Ham examples\n",
    "    (\"Hey, are we still meeting for lunch tomorrow?\", \"ham\"),\n",
    "    (\"The project deadline has been extended to Friday.\", \"ham\"),\n",
    "    (\"Thanks for sending the report. I'll review it today.\", \"ham\"),\n",
    "    (\"Can you pick up groceries on your way home?\", \"ham\"),\n",
    "    (\"Meeting rescheduled to 3pm in conference room B.\", \"ham\"),\n",
    "    (\"Happy birthday! Hope you have a great day.\", \"ham\"),\n",
    "    (\"I attached the documents you requested.\", \"ham\"),\n",
    "    (\"Let me know when you're free to discuss the proposal.\", \"ham\"),\n",
    "]\n",
    "\n",
    "# Train classifier\n",
    "spam_classifier = SpamClassifier()\n",
    "spam_classifier.train(training_data)\n",
    "\n",
    "print(\"Spam Classifier trained!\")\n",
    "print(\"\\nMost informative features:\")\n",
    "spam_classifier.classifier.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41dc1960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPAM DETECTION RESULTS\n",
      "============================================================\n",
      "\n",
      "ğŸš« SPAM (99.9% spam)\n",
      "   \"FREE iPhone! Click now to claim your prize!\"\n",
      "\n",
      "âœ… HAM (0.6% spam)\n",
      "   \"Don't forget about the team meeting at 2pm.\"\n",
      "\n",
      "ğŸš« SPAM (99.8% spam)\n",
      "   \"URGENT! Your account will be suspended! Act NOW!\"\n",
      "\n",
      "âœ… HAM (0.6% spam)\n",
      "   \"Can you send me the updated spreadsheet?\"\n",
      "\n",
      "ğŸš« SPAM (94.2% spam)\n",
      "   \"Winner selected! Claim $500 gift card here!\"\n"
     ]
    }
   ],
   "source": [
    "# Test messages\n",
    "test_messages = [\n",
    "    \"FREE iPhone! Click now to claim your prize!\",\n",
    "    \"Don't forget about the team meeting at 2pm.\",\n",
    "    \"URGENT! Your account will be suspended! Act NOW!\",\n",
    "    \"Can you send me the updated spreadsheet?\",\n",
    "    \"Winner selected! Claim $500 gift card here!\",\n",
    "]\n",
    "\n",
    "print(\"SPAM DETECTION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for msg in test_messages:\n",
    "    result = spam_classifier.classify(msg)\n",
    "    probs = spam_classifier.probability(msg)\n",
    "    \n",
    "    emoji = \"ğŸš«\" if result == \"spam\" else \"âœ…\"\n",
    "    \n",
    "    print(f\"\\n{emoji} {result.upper()} ({probs['spam']:.1%} spam)\")\n",
    "    print(f\"   \\\"{msg[:50]}...\\\"\" if len(msg) > 50 else f\"   \\\"{msg}\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fa0752",
   "metadata": {},
   "source": [
    "## Project 4: Simple Question Answering\n",
    "\n",
    "### Pipeline Architecture\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                      QUESTION ANSWERING PIPELINE                         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   Context    â”‚                              â”‚   Question   â”‚\n",
    "â”‚   Document   â”‚                              â”‚              â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "       â”‚                                             â”‚\n",
    "       â–¼                                             â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   Sentence   â”‚                              â”‚  Preprocess  â”‚\n",
    "â”‚   Tokenize   â”‚                              â”‚   Question   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "       â”‚                                             â”‚\n",
    "       â–¼                                             â”‚\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                                                              â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚\n",
    "â”‚  â”‚ Sentence 1 â”‚  â”‚ Sentence 2 â”‚  â”‚ Sentence N â”‚   ...       â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜             â”‚\n",
    "â”‚        â”‚               â”‚               â”‚                     â”‚\n",
    "â”‚        â–¼               â–¼               â–¼                     â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚\n",
    "â”‚  â”‚ Preprocess â”‚  â”‚ Preprocess â”‚  â”‚ Preprocess â”‚             â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜             â”‚\n",
    "â”‚        â”‚               â”‚               â”‚                     â”‚\n",
    "â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚\n",
    "â”‚                        â–¼                                     â”‚\n",
    "â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚              â”‚    Calculate    â”‚         Question Words      â”‚\n",
    "â”‚              â”‚   Similarity    â”‚                             â”‚\n",
    "â”‚              â”‚   (Jaccard)     â”‚                             â”‚\n",
    "â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                             â”‚\n",
    "â”‚                       â”‚                                      â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                        â–¼\n",
    "              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "              â”‚  Rank Sentences â”‚\n",
    "              â”‚   by Similarity â”‚\n",
    "              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                       â”‚\n",
    "                       â–¼\n",
    "              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "              â”‚   Best Match    â”‚\n",
    "              â”‚    = Answer     â”‚\n",
    "              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Jaccard Similarity\n",
    "\n",
    "$$\\text{Similarity}(Q, S) = \\frac{|Q \\cap S|}{|Q \\cup S|}$$\n",
    "\n",
    "Where:\n",
    "- $Q$ = Set of words in question (after preprocessing)\n",
    "- $S$ = Set of words in sentence (after preprocessing)\n",
    "- $|Q \\cap S|$ = Words in common\n",
    "- $|Q \\cup S|$ = Total unique words\n",
    "\n",
    "### Example\n",
    "\n",
    "```\n",
    "Question: \"Who created Python?\"\n",
    "Words: {who, created, python}\n",
    "\n",
    "Sentence: \"Python was created by Guido van Rossum...\"\n",
    "Words: {python, created, guido, van, rossum...}\n",
    "\n",
    "Intersection: {python, created} = 2\n",
    "Union: {who, created, python, guido, van, rossum...} = 6+\n",
    "\n",
    "Similarity = 2/6 â‰ˆ 0.33  â† Highest score wins!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f30f32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleQA:\n",
    "    \"\"\"Simple extractive question answering\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        \"\"\"Tokenize and filter\"\"\"\n",
    "        words = word_tokenize(text.lower())\n",
    "        return [w for w in words if w.isalpha() and w not in self.stop_words]\n",
    "    \n",
    "    def similarity(self, sent1, sent2):\n",
    "        \"\"\"Calculate word overlap similarity\"\"\"\n",
    "        words1 = set(self.preprocess(sent1))\n",
    "        words2 = set(self.preprocess(sent2))\n",
    "        \n",
    "        if not words1 or not words2:\n",
    "            return 0\n",
    "        \n",
    "        intersection = words1 & words2\n",
    "        union = words1 | words2\n",
    "        \n",
    "        return len(intersection) / len(union)\n",
    "    \n",
    "    def find_answer(self, context, question, top_n=1):\n",
    "        \"\"\"Find best matching sentence(s) for the question\"\"\"\n",
    "        sentences = sent_tokenize(context)\n",
    "        \n",
    "        # Score each sentence\n",
    "        scored = []\n",
    "        for sent in sentences:\n",
    "            score = self.similarity(sent, question)\n",
    "            scored.append((sent, score))\n",
    "        \n",
    "        # Sort by score\n",
    "        scored.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        if top_n == 1:\n",
    "            return scored[0][0] if scored[0][1] > 0 else \"No answer found.\"\n",
    "        \n",
    "        return [s for s, score in scored[:top_n] if score > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca401e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION ANSWERING\n",
      "============================================================\n",
      "Context: Python was created by Guido van Rossum and first released in 1991.\n",
      "It is a high-level, interpreted p...\n",
      "============================================================\n",
      "\n",
      "Q: Who invented Python?\n",
      "A: The name Python was inspired by Monty Python's Flying Circus.\n",
      "\n",
      "Q: When was Python first released?\n",
      "A: \n",
      "Python was created by Guido van Rossum and first released in 1991.\n",
      "\n",
      "Q: What is Python used for?\n",
      "A: Python has a large standard library\n",
      "and is widely used in web development, data science, and artificial intelligence.\n",
      "\n",
      "Q: Where does the name Python come from?\n",
      "A: The name Python was inspired by Monty Python's Flying Circus.\n"
     ]
    }
   ],
   "source": [
    "qa = SimpleQA()\n",
    "\n",
    "context = \"\"\"\n",
    "Python was created by Guido van Rossum and first released in 1991.\n",
    "It is a high-level, interpreted programming language known for its simplicity.\n",
    "Python emphasizes code readability and uses significant indentation.\n",
    "The language supports multiple programming paradigms, including procedural,\n",
    "object-oriented, and functional programming. Python has a large standard library\n",
    "and is widely used in web development, data science, and artificial intelligence.\n",
    "The name Python was inspired by Monty Python's Flying Circus.\n",
    "\"\"\"\n",
    "\n",
    "questions = [\n",
    "    \"Who created Python?\",\n",
    "    \"When was Python first released?\",\n",
    "    \"What is Python used for?\",\n",
    "    \"Where does the name Python come from?\",\n",
    "]\n",
    "\n",
    "print(\"QUESTION ANSWERING\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Context: {context.strip()[:100]}...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for q in questions:\n",
    "    answer = qa.find_answer(context, q)\n",
    "    print(f\"\\nQ: {q}\")\n",
    "    print(f\"A: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40d46a3",
   "metadata": {},
   "source": [
    "## Project 5: Simple Chatbot\n",
    "\n",
    "### Pipeline Architecture\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                          CHATBOT PIPELINE                                â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                         â”‚   User Input    â”‚\n",
    "                         â”‚  \"Hello there!\" â”‚\n",
    "                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                  â”‚\n",
    "                                  â–¼\n",
    "                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                         â”‚   Preprocess    â”‚\n",
    "                         â”‚  (lowercase)    â”‚\n",
    "                         â”‚ \"hello there!\"  â”‚\n",
    "                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                  â”‚\n",
    "                                  â–¼\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                    â”‚     PATTERN MATCHING        â”‚\n",
    "                    â”‚                             â”‚\n",
    "                    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
    "                    â”‚  â”‚ r'hello|hi|hey'      â”œâ”€â”€â”¼â”€â”€â–¶ Match! \n",
    "                    â”‚  â”‚ r'how are you'       â”‚  â”‚\n",
    "                    â”‚  â”‚ r'what is your name' â”‚  â”‚\n",
    "                    â”‚  â”‚ r'bye|goodbye'       â”‚  â”‚\n",
    "                    â”‚  â”‚ r'thank'             â”‚  â”‚\n",
    "                    â”‚  â”‚ r'weather'           â”‚  â”‚\n",
    "                    â”‚  â”‚ r'help'              â”‚  â”‚\n",
    "                    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
    "                    â”‚                             â”‚\n",
    "                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                   â”‚\n",
    "                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                    â”‚                             â”‚\n",
    "              Match Found?                   No Match\n",
    "                    â”‚                             â”‚\n",
    "                    â–¼                             â–¼\n",
    "          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "          â”‚ Random Response â”‚           â”‚ Default Responseâ”‚\n",
    "          â”‚  from Pattern   â”‚           â”‚   (fallback)    â”‚\n",
    "          â”‚    List         â”‚           â”‚                 â”‚\n",
    "          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                   â”‚                             â”‚\n",
    "                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                  â”‚\n",
    "                                  â–¼\n",
    "                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                         â”‚   Bot Response  â”‚\n",
    "                         â”‚  \"Hello! How    â”‚\n",
    "                         â”‚   can I help?\"  â”‚\n",
    "                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Pattern â†’ Response Mapping\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  PATTERN (Regex)          â”‚  POSSIBLE RESPONSES                 â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  r'hello|hi|hey'          â”‚  â€¢ \"Hello! How can I help you?\"    â”‚\n",
    "â”‚                           â”‚  â€¢ \"Hi there! What can I do?\"       â”‚\n",
    "â”‚                           â”‚  â€¢ \"Hey! Nice to meet you!\"         â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  r'how are you'           â”‚  â€¢ \"I'm doing great, thanks!\"      â”‚\n",
    "â”‚                           â”‚  â€¢ \"I'm fine! How about you?\"      â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  r'bye|goodbye|quit'      â”‚  â€¢ \"Goodbye! Have a great day!\"    â”‚\n",
    "â”‚                           â”‚  â€¢ \"See you later!\"                 â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Limitations & Extensions\n",
    "\n",
    "| This Chatbot | More Advanced Chatbots |\n",
    "|--------------|------------------------|\n",
    "| Pattern matching only | Intent classification (ML) |\n",
    "| No context memory | Dialogue state tracking |\n",
    "| Fixed responses | Response generation (LLM) |\n",
    "| No entity extraction | Named entity recognition |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa71c48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class SimpleChatbot:\n",
    "    \"\"\"Pattern-matching chatbot with NLTK\"\"\"\n",
    "    \n",
    "    def __init__(self, name=\"Bot\"):\n",
    "        self.name = name\n",
    "        self.context = {}\n",
    "        \n",
    "        # Response patterns\n",
    "        self.patterns = {\n",
    "            r'hello|hi|hey': [\n",
    "                \"Hello! How can I help you today?\",\n",
    "                \"Hi there! What can I do for you?\",\n",
    "                \"Hey! Nice to meet you!\"\n",
    "            ],\n",
    "            r'how are you': [\n",
    "                \"I'm doing great, thanks for asking!\",\n",
    "                \"I'm fine! How about you?\",\n",
    "                \"All good here!\"\n",
    "            ],\n",
    "            r'what is your name|who are you': [\n",
    "                f\"I'm {name}, your friendly chatbot!\",\n",
    "                f\"My name is {name}. Nice to meet you!\"\n",
    "            ],\n",
    "            r'bye|goodbye|quit|exit': [\n",
    "                \"Goodbye! Have a great day!\",\n",
    "                \"See you later!\",\n",
    "                \"Bye! Take care!\"\n",
    "            ],\n",
    "            r'thank': [\n",
    "                \"You're welcome!\",\n",
    "                \"Happy to help!\",\n",
    "                \"No problem!\"\n",
    "            ],\n",
    "            r'weather': [\n",
    "                \"I can't check the weather, but I hope it's nice!\",\n",
    "                \"Sorry, I don't have access to weather data.\"\n",
    "            ],\n",
    "            r'help': [\n",
    "                \"I can chat with you! Try saying hello or asking me questions.\",\n",
    "                \"I'm here to help! Just start a conversation.\"\n",
    "            ],\n",
    "        }\n",
    "        \n",
    "        self.default_responses = [\n",
    "            \"I'm not sure I understand. Could you rephrase that?\",\n",
    "            \"Interesting! Tell me more.\",\n",
    "            \"I see. What else would you like to talk about?\",\n",
    "            \"Could you explain that differently?\"\n",
    "        ]\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        \"\"\"Clean input text\"\"\"\n",
    "        return text.lower().strip()\n",
    "    \n",
    "    def match_pattern(self, text):\n",
    "        \"\"\"Find matching pattern\"\"\"\n",
    "        for pattern, responses in self.patterns.items():\n",
    "            if re.search(pattern, text):\n",
    "                return random.choice(responses)\n",
    "        return None\n",
    "    \n",
    "    def respond(self, user_input):\n",
    "        \"\"\"Generate response to user input\"\"\"\n",
    "        text = self.preprocess(user_input)\n",
    "        \n",
    "        # Check for pattern match\n",
    "        response = self.match_pattern(text)\n",
    "        \n",
    "        if response:\n",
    "            return response\n",
    "        \n",
    "        # Default response\n",
    "        return random.choice(self.default_responses)\n",
    "    \n",
    "    def chat(self):\n",
    "        \"\"\"Interactive chat loop\"\"\"\n",
    "        print(f\"{self.name}: Hello! I'm {self.name}. Type 'quit' to exit.\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        while True:\n",
    "            user_input = input(\"You: \").strip()\n",
    "            \n",
    "            if not user_input:\n",
    "                continue\n",
    "            \n",
    "            if re.search(r'bye|quit|exit', user_input.lower()):\n",
    "                print(f\"{self.name}: Goodbye!\")\n",
    "                break\n",
    "            \n",
    "            response = self.respond(user_input)\n",
    "            print(f\"{self.name}: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b23033e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHATBOT DEMO\n",
      "==================================================\n",
      "You: Hello!\n",
      "Bot: Hi there! What can I do for you?\n",
      "\n",
      "You: What is your name?\n",
      "Bot: My name is NLTK-Bot. Nice to meet you!\n",
      "\n",
      "You: How are you doing?\n",
      "Bot: All good here!\n",
      "\n",
      "You: What's the weather like?\n",
      "Bot: I can't check the weather, but I hope it's nice!\n",
      "\n",
      "You: Tell me a joke\n",
      "Bot: I'm not sure I understand. Could you rephrase that?\n",
      "\n",
      "You: Thank you!\n",
      "Bot: Happy to help!\n",
      "\n",
      "You: Goodbye!\n",
      "Bot: See you later!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Demo the chatbot (non-interactive)\n",
    "bot = SimpleChatbot(\"NLTK-Bot\")\n",
    "\n",
    "demo_inputs = [\n",
    "    \"Hello!\",\n",
    "    \"What is your name?\",\n",
    "    \"How are you doing?\",\n",
    "    \"What's the weather like?\",\n",
    "    \"Tell me a joke\",\n",
    "    \"Thank you!\",\n",
    "    \"Goodbye!\"\n",
    "]\n",
    "\n",
    "print(\"CHATBOT DEMO\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for user_input in demo_inputs:\n",
    "    response = bot.respond(user_input)\n",
    "    print(f\"You: {user_input}\")\n",
    "    print(f\"Bot: {response}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b391dfb",
   "metadata": {},
   "source": [
    "## Project 6: Document Similarity\n",
    "\n",
    "### Pipeline Architecture\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                     DOCUMENT SIMILARITY PIPELINE                         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "                              FITTING PHASE\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                                                                          â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”                                     â”‚\n",
    "â”‚  â”‚  Doc 1 â”‚  â”‚  Doc 2 â”‚  â”‚  Doc N â”‚  ...                                â”‚\n",
    "â”‚  â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                                     â”‚\n",
    "â”‚      â”‚           â”‚           â”‚                                           â”‚\n",
    "â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚\n",
    "â”‚                  â–¼                                                       â”‚\n",
    "â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                             â”‚\n",
    "â”‚         â”‚   Preprocess    â”‚  (tokenize, lowercase, lemmatize,           â”‚\n",
    "â”‚         â”‚   All Docs      â”‚   remove stopwords)                         â”‚\n",
    "â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                             â”‚\n",
    "â”‚                  â”‚                                                       â”‚\n",
    "â”‚                  â–¼                                                       â”‚\n",
    "â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                             â”‚\n",
    "â”‚         â”‚ Build Vocabularyâ”‚  {machine, learning, ai, neural, ...}       â”‚\n",
    "â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                             â”‚\n",
    "â”‚                  â”‚                                                       â”‚\n",
    "â”‚                  â–¼                                                       â”‚\n",
    "â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                             â”‚\n",
    "â”‚         â”‚  Calculate IDF  â”‚  IDF(word) = log(N / doc_count(word))       â”‚\n",
    "â”‚         â”‚  for each word  â”‚                                             â”‚\n",
    "â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                             â”‚\n",
    "â”‚                                                                          â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "                           SIMILARITY PHASE\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                                                                          â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”                                     â”‚\n",
    "â”‚  â”‚  Doc A â”‚              â”‚  Doc B â”‚                                     â”‚\n",
    "â”‚  â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                                     â”‚\n",
    "â”‚      â”‚                       â”‚                                           â”‚\n",
    "â”‚      â–¼                       â–¼                                           â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                  â”‚\n",
    "â”‚  â”‚  TF-IDF    â”‚         â”‚  TF-IDF    â”‚                                  â”‚\n",
    "â”‚  â”‚  Vector A  â”‚         â”‚  Vector B  â”‚                                  â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                                  â”‚\n",
    "â”‚        â”‚                      â”‚                                          â”‚\n",
    "â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                          â”‚\n",
    "â”‚                   â–¼                                                      â”‚\n",
    "â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                            â”‚\n",
    "â”‚          â”‚     Cosine      â”‚                                            â”‚\n",
    "â”‚          â”‚   Similarity    â”‚                                            â”‚\n",
    "â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                            â”‚\n",
    "â”‚                   â”‚                                                      â”‚\n",
    "â”‚                   â–¼                                                      â”‚\n",
    "â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                            â”‚\n",
    "â”‚          â”‚  Score: 0.75    â”‚  (0 = unrelated, 1 = identical)            â”‚\n",
    "â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                            â”‚\n",
    "â”‚                                                                          â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### TF-IDF Calculation\n",
    "\n",
    "**TF (Term Frequency)**: How often a word appears in THIS document\n",
    "$$TF(t, d) = \\frac{\\text{count of } t \\text{ in } d}{\\text{max count in } d}$$\n",
    "\n",
    "**IDF (Inverse Document Frequency)**: How rare a word is across ALL documents\n",
    "$$IDF(t) = \\log\\frac{N}{\\text{documents containing } t}$$\n",
    "\n",
    "**TF-IDF**: \n",
    "$$\\text{TF-IDF}(t, d) = TF(t, d) \\times IDF(t)$$\n",
    "\n",
    "### Cosine Similarity\n",
    "\n",
    "$$\\cos(\\theta) = \\frac{\\vec{A} \\cdot \\vec{B}}{||\\vec{A}|| \\times ||\\vec{B}||} = \\frac{\\sum A_i B_i}{\\sqrt{\\sum A_i^2} \\times \\sqrt{\\sum B_i^2}}$$\n",
    "\n",
    "### Example\n",
    "\n",
    "```\n",
    "Doc1: \"machine learning is great\"     â†’ TF-IDF: [0.5, 0.5, 0.1, 0.3, ...]\n",
    "Doc2: \"deep learning uses neural\"     â†’ TF-IDF: [0.0, 0.5, 0.0, 0.0, ...]\n",
    "                                              â†‘\n",
    "                                         \"learning\" in both\n",
    "\n",
    "Cosine Similarity = high where vectors point same direction\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b9ad907",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class DocumentSimilarity:\n",
    "    \"\"\"Calculate document similarity using TF-IDF and cosine similarity\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.documents = []\n",
    "        self.vocabulary = set()\n",
    "        self.idf = {}\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        \"\"\"Tokenize and normalize text\"\"\"\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        return [\n",
    "            self.lemmatizer.lemmatize(w)\n",
    "            for w in tokens\n",
    "            if w.isalpha() and w not in self.stop_words\n",
    "        ]\n",
    "    \n",
    "    def fit(self, documents):\n",
    "        \"\"\"Fit on a collection of documents\"\"\"\n",
    "        self.documents = [self.preprocess(doc) for doc in documents]\n",
    "        \n",
    "        # Build vocabulary and calculate IDF\n",
    "        doc_count = len(self.documents)\n",
    "        word_doc_count = defaultdict(int)\n",
    "        \n",
    "        for doc in self.documents:\n",
    "            for word in set(doc):\n",
    "                word_doc_count[word] += 1\n",
    "                self.vocabulary.add(word)\n",
    "        \n",
    "        # Calculate IDF\n",
    "        for word, count in word_doc_count.items():\n",
    "            self.idf[word] = math.log(doc_count / count)\n",
    "    \n",
    "    def tf_idf(self, document):\n",
    "        \"\"\"Calculate TF-IDF vector for a document\"\"\"\n",
    "        if isinstance(document, str):\n",
    "            document = self.preprocess(document)\n",
    "        \n",
    "        word_count = Counter(document)\n",
    "        max_count = max(word_count.values()) if word_count else 1\n",
    "        \n",
    "        vector = {}\n",
    "        for word in self.vocabulary:\n",
    "            tf = word_count.get(word, 0) / max_count\n",
    "            idf = self.idf.get(word, 0)\n",
    "            vector[word] = tf * idf\n",
    "        \n",
    "        return vector\n",
    "    \n",
    "    def cosine_similarity(self, vec1, vec2):\n",
    "        \"\"\"Calculate cosine similarity between two vectors\"\"\"\n",
    "        dot_product = sum(vec1.get(k, 0) * vec2.get(k, 0) for k in self.vocabulary)\n",
    "        \n",
    "        mag1 = math.sqrt(sum(v ** 2 for v in vec1.values()))\n",
    "        mag2 = math.sqrt(sum(v ** 2 for v in vec2.values()))\n",
    "        \n",
    "        if mag1 == 0 or mag2 == 0:\n",
    "            return 0\n",
    "        \n",
    "        return dot_product / (mag1 * mag2)\n",
    "    \n",
    "    def similarity_matrix(self, documents):\n",
    "        \"\"\"Calculate pairwise similarity matrix\"\"\"\n",
    "        self.fit(documents)\n",
    "        vectors = [self.tf_idf(doc) for doc in self.documents]\n",
    "        \n",
    "        n = len(documents)\n",
    "        matrix = [[0] * n for _ in range(n)]\n",
    "        \n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                matrix[i][j] = self.cosine_similarity(vectors[i], vectors[j])\n",
    "        \n",
    "        return matrix\n",
    "    \n",
    "    def find_similar(self, query, documents, top_n=3):\n",
    "        \"\"\"Find most similar documents to a query\"\"\"\n",
    "        self.fit(documents)\n",
    "        query_vector = self.tf_idf(query)\n",
    "        \n",
    "        similarities = []\n",
    "        for i, doc in enumerate(self.documents):\n",
    "            doc_vector = self.tf_idf(doc)\n",
    "            sim = self.cosine_similarity(query_vector, doc_vector)\n",
    "            similarities.append((i, sim))\n",
    "        \n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        return similarities[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cbfcbeae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOCUMENT SIMILARITY MATRIX\n",
      "==================================================\n",
      "\n",
      "     Doc     1Doc     2Doc     3Doc     4Doc     5\n",
      "Doc1   1.00   0.02   0.00   0.11   0.00\n",
      "Doc2   0.02   1.00   0.00   0.02   0.00\n",
      "Doc3   0.00   0.00   1.00   0.00   0.00\n",
      "Doc4   0.11   0.02   0.00   1.00   0.00\n",
      "Doc5   0.00   0.00   0.00   0.00   1.00\n",
      "\n",
      "==================================================\n",
      "Documents:\n",
      "Doc1: Machine learning is a subset of artificial intelli...\n",
      "Doc2: Deep learning uses neural networks with multiple l...\n",
      "Doc3: Natural language processing helps computers unders...\n",
      "Doc4: Python is widely used for machine learning and AI....\n",
      "Doc5: Data science combines statistics with programming....\n"
     ]
    }
   ],
   "source": [
    "# Test document similarity\n",
    "documents = [\n",
    "    \"Machine learning is a subset of artificial intelligence.\",\n",
    "    \"Deep learning uses neural networks with multiple layers.\",\n",
    "    \"Natural language processing helps computers understand text.\",\n",
    "    \"Python is widely used for machine learning and AI.\",\n",
    "    \"Data science combines statistics with programming.\",\n",
    "]\n",
    "\n",
    "sim_calc = DocumentSimilarity()\n",
    "matrix = sim_calc.similarity_matrix(documents)\n",
    "\n",
    "print(\"DOCUMENT SIMILARITY MATRIX\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Print matrix\n",
    "print(\"\\n     \", end=\"\")\n",
    "for i in range(len(documents)):\n",
    "    print(f\"Doc{i+1:>6}\", end=\"\")\n",
    "print()\n",
    "\n",
    "for i, row in enumerate(matrix):\n",
    "    print(f\"Doc{i+1}\", end=\"\")\n",
    "    for val in row:\n",
    "        print(f\"{val:>7.2f}\", end=\"\")\n",
    "    print()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Documents:\")\n",
    "for i, doc in enumerate(documents):\n",
    "    print(f\"Doc{i+1}: {doc[:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2f3dce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: 'How does artificial intelligence work?'\n",
      "\n",
      "Most similar documents:\n",
      "  0.76: Machine learning is a subset of artificial intelligence.\n",
      "  0.00: Deep learning uses neural networks with multiple layers.\n",
      "  0.00: Natural language processing helps computers understand text.\n"
     ]
    }
   ],
   "source": [
    "# Find similar documents to a query\n",
    "query = \"How does artificial intelligence work?\"\n",
    "\n",
    "similar = sim_calc.find_similar(query, documents, top_n=3)\n",
    "\n",
    "print(f\"\\nQuery: '{query}'\")\n",
    "print(\"\\nMost similar documents:\")\n",
    "for idx, score in similar:\n",
    "    print(f\"  {score:.2f}: {documents[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ac71e6",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Projects Covered\n",
    "\n",
    "| Project | Key Techniques |\n",
    "|---------|----------------|\n",
    "| Text Summarization | Sentence scoring, word frequency |\n",
    "| Keyword Extraction | TF-IDF, POS filtering, noun phrases |\n",
    "| Spam Classifier | Naive Bayes, feature extraction |\n",
    "| Question Answering | Sentence similarity, extractive QA |\n",
    "| Chatbot | Pattern matching, regex |\n",
    "| Document Similarity | TF-IDF, cosine similarity |\n",
    "\n",
    "### Key Takeaways\n",
    "- Combine multiple NLTK tools for real applications\n",
    "- Preprocessing is crucial for all NLP tasks\n",
    "- Feature engineering impacts model performance\n",
    "- Start simple, then add complexity as needed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
